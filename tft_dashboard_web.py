#!/usr/bin/env python3
"""
TFT Monitoring Dashboard - Production Web Interface
Built with Streamlit for easy deployment and professional UI

Usage:
    streamlit run tft_dashboard_web.py

Requirements:
    pip install streamlit plotly requests pandas
"""

import streamlit as st
import requests
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import time
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# PAGE CONFIGURATION
# =============================================================================

st.set_page_config(
    page_title="TFT Monitoring Dashboard",
    page_icon="üîÆ",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'About': "TFT Monitoring Dashboard - Temporal Fusion Transformer for Server Monitoring"
    }
)

# No custom CSS - let Streamlit handle rendering normally

# =============================================================================
# SESSION STATE INITIALIZATION
# =============================================================================

if 'daemon_url' not in st.session_state:
    st.session_state.daemon_url = "http://localhost:8000"

if 'refresh_interval' not in st.session_state:
    st.session_state.refresh_interval = 60

if 'history' not in st.session_state:
    st.session_state.history = []

if 'last_update' not in st.session_state:
    st.session_state.last_update = None

if 'daemon_connected' not in st.session_state:
    st.session_state.daemon_connected = False

if 'demo_mode' not in st.session_state:
    st.session_state.demo_mode = False

if 'demo_running' not in st.session_state:
    st.session_state.demo_running = False

if 'demo_data' not in st.session_state:
    st.session_state.demo_data = None

if 'demo_index' not in st.session_state:
    st.session_state.demo_index = 0

if 'demo_scenario' not in st.session_state:
    st.session_state.demo_scenario = None

if 'demo_start_time' not in st.session_state:
    st.session_state.demo_start_time = None

if 'demo_generator' not in st.session_state:
    st.session_state.demo_generator = None

if 'last_demo_tick' not in st.session_state:
    st.session_state.last_demo_tick = None

# =============================================================================
# DAEMON API CLIENT
# =============================================================================

class DaemonClient:
    """Client for TFT Inference Daemon API."""

    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip('/')

    def check_health(self) -> Dict:
        """Check daemon health status."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=2)
            if response.ok:
                return {'connected': True, 'data': response.json()}
            return {'connected': False, 'error': f"HTTP {response.status_code}"}
        except requests.exceptions.ConnectionError:
            return {'connected': False, 'error': 'Connection refused'}
        except Exception as e:
            return {'connected': False, 'error': str(e)}

    def get_predictions(self) -> Optional[Dict]:
        """Get current predictions from daemon."""
        try:
            response = requests.get(f"{self.base_url}/predictions/current", timeout=5)
            if response.ok:
                return response.json()
            return None
        except Exception as e:
            st.error(f"Error fetching predictions: {e}")
            return None

    def get_alerts(self) -> Optional[Dict]:
        """Get active alerts from daemon."""
        try:
            response = requests.get(f"{self.base_url}/alerts/active", timeout=5)
            if response.ok:
                return response.json()
            return None
        except Exception as e:
            return None

    def feed_data(self, records: List[Dict]) -> bool:
        """Feed data to daemon for demo mode."""
        try:
            response = requests.post(
                f"{self.base_url}/feed/data",
                json={"records": records},
                timeout=5
            )
            if not response.ok:
                st.error(f"Daemon returned {response.status_code}: {response.text[:200]}")
            return response.ok
        except Exception as e:
            st.error(f"Error feeding data: {e}")
            return False

# =============================================================================
# DEMO MODE FUNCTIONS
# =============================================================================

def initialize_demo_generator(scenario: str, fleet_size: int = 90):
    """
    Initialize streaming demo data generator.

    Args:
        scenario: 'stable', 'degrading', or 'critical'
        fleet_size: Number of servers to simulate (max 90 - full fleet)
    """
    from demo_stream_generator import DemoStreamGenerator

    # Create new generator - max 90 servers (full training fleet)
    generator = DemoStreamGenerator(num_servers=min(fleet_size, 90), seed=None)
    generator.set_scenario(scenario)

    st.session_state.demo_generator = generator
    st.session_state.demo_scenario = scenario
    st.session_state.demo_running = True
    st.session_state.demo_start_time = datetime.now()

def stop_demo():
    """Stop the demo mode."""
    st.session_state.demo_running = False
    st.session_state.demo_generator = None
    st.session_state.demo_scenario = None

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def get_health_status(predictions: Dict) -> tuple:
    """
    Determine overall environment health status based on ACTUAL server risk scores.

    Returns: (status_text, status_color, status_emoji)
    """
    if not predictions or 'predictions' not in predictions:
        return "Unknown", "gray", "‚ùì"

    # Calculate actual fleet health based on server risk scores
    server_preds = predictions.get('predictions', {})
    if not server_preds:
        return "Unknown", "gray", "‚ùì"

    # Count servers by risk level
    critical_count = 0
    warning_count = 0
    caution_count = 0
    healthy_count = 0

    for server_name, server_pred in server_preds.items():
        risk = calculate_server_risk_score(server_pred)
        if risk >= 80:  # Critical/Imminent Failure
            critical_count += 1
        elif risk >= 60:  # Danger/Warning
            warning_count += 1
        elif risk >= 50:  # Degrading
            caution_count += 1
        else:  # Healthy/Watch
            healthy_count += 1

    total = len(server_preds)

    # Determine status based on percentage of unhealthy servers
    critical_pct = critical_count / total if total > 0 else 0
    warning_pct = warning_count / total if total > 0 else 0
    unhealthy_pct = (critical_count + warning_count) / total if total > 0 else 0

    if critical_pct > 0.3 or unhealthy_pct > 0.5:  # >30% critical OR >50% unhealthy
        return "Critical", "red", "üî¥"
    elif critical_pct > 0.1 or unhealthy_pct > 0.3:  # >10% critical OR >30% unhealthy
        return "Warning", "orange", "üü†"
    elif unhealthy_pct > 0.1:  # >10% unhealthy (degrading servers)
        return "Caution", "yellow", "üü°"
    else:
        return "Healthy", "green", "üü¢"

@st.cache_data(ttl=300)  # Cache for 5 minutes
def get_server_profile(server_name: str) -> str:
    """Extract server profile from server name."""
    if server_name.startswith('ppml'): return 'ML Compute'
    if server_name.startswith('ppdb'): return 'Database'
    if server_name.startswith('ppweb'): return 'Web API'
    if server_name.startswith('ppcon'): return 'Conductor'
    if server_name.startswith('ppetl'): return 'ETL/Ingest'
    if server_name.startswith('pprisk'): return 'Risk Analytics'
    return 'Generic'

def calculate_server_risk_score(server_pred: Dict) -> float:
    """
    Calculate aggregated risk score for a server (0-100).

    EXECUTIVE-FRIENDLY SCORING:
    - Prioritizes CURRENT state (70% weight) - "What's on fire NOW?"
    - Considers PREDICTIONS (30% weight) - "What will be on fire soon?"
    - Only flags truly critical situations
    """
    current_risk = 0.0  # Based on current metrics
    predicted_risk = 0.0  # Based on 30-min predictions

    profile = get_server_profile(server_pred.get('server_name', ''))

    # =========================================================================
    # CPU RISK ASSESSMENT (LINBORG: calculate % CPU Used from components)
    # =========================================================================
    # Calculate total CPU usage: 100 - idle, or sum(user + sys + iowait)
    cpu_idle = server_pred.get('cpu_idle_pct', {}).get('current', 0) if 'cpu_idle_pct' in server_pred else 0
    cpu_user = server_pred.get('cpu_user_pct', {}).get('current', 0) if 'cpu_user_pct' in server_pred else 0
    cpu_sys = server_pred.get('cpu_sys_pct', {}).get('current', 0) if 'cpu_sys_pct' in server_pred else 0
    cpu_iowait = server_pred.get('cpu_iowait_pct', {}).get('current', 0) if 'cpu_iowait_pct' in server_pred else 0

    # Calculate current CPU usage
    if cpu_idle > 0:
        current_cpu = 100 - cpu_idle
    else:
        current_cpu = cpu_user + cpu_sys + cpu_iowait

    # Calculate predicted CPU (use idle if available, otherwise sum components)
    if 'cpu_idle_pct' in server_pred:
        idle_p90 = server_pred['cpu_idle_pct'].get('p90', [])
        min_idle_p90 = min(idle_p90[:6]) if idle_p90 and len(idle_p90) >= 6 else cpu_idle
        max_cpu_p90 = 100 - min_idle_p90
    else:
        max_cpu_p90 = current_cpu  # Fallback

    # CURRENT CPU RISK (what matters most to executives)
    if current_cpu >= 98:
        current_risk += 60  # CRITICAL - System will hang
    elif current_cpu >= 95:
        current_risk += 40  # Severe degradation
    elif current_cpu >= 90:
        current_risk += 20  # High load

    # PREDICTED CPU RISK (early warning)
    if max_cpu_p90 >= 98:
        predicted_risk += 30  # Will become critical
    elif max_cpu_p90 >= 95:
        predicted_risk += 20  # Will degrade
    elif max_cpu_p90 >= 90:
        predicted_risk += 10  # Will be high

    # =========================================================================
    # I/O WAIT RISK ASSESSMENT (CRITICAL - "System troubleshooting 101")
    # =========================================================================
    if 'cpu_iowait_pct' in server_pred:
        iowait = server_pred['cpu_iowait_pct']
        current_iowait = iowait.get('current', 0)
        p90 = iowait.get('p90', [])
        max_iowait_p90 = max(p90[:6]) if p90 and len(p90) >= 6 else current_iowait

        # I/O wait is a critical indicator - high iowait = disk/storage bottleneck
        # CURRENT
        if current_iowait >= 30:
            current_risk += 50  # CRITICAL - severe I/O bottleneck
        elif current_iowait >= 20:
            current_risk += 30  # High I/O contention
        elif current_iowait >= 10:
            current_risk += 15  # Elevated I/O wait
        elif current_iowait >= 5:
            current_risk += 5   # Noticeable

        # PREDICTED
        if max_iowait_p90 >= 30:
            predicted_risk += 25  # Will have severe bottleneck
        elif max_iowait_p90 >= 20:
            predicted_risk += 15  # Will have high contention
        elif max_iowait_p90 >= 10:
            predicted_risk += 8   # Will be elevated

    # =========================================================================
    # MEMORY RISK ASSESSMENT (LINBORG: mem_used_pct)
    # =========================================================================
    if 'mem_used_pct' in server_pred:
        mem = server_pred['mem_used_pct']
        current_mem = mem.get('current', 0)
        p90 = mem.get('p90', [])
        max_mem_p90 = max(p90[:6]) if p90 and len(p90) >= 6 else current_mem

        # Profile-specific thresholds
        if profile == 'Database':
            # Databases: high memory is normal (page cache), only >95% concerning
            # CURRENT
            if current_mem >= 98:
                current_risk += 40  # Very high
            elif current_mem >= 95:
                current_risk += 15  # Elevated
            # PREDICTED
            if max_mem_p90 >= 98:
                predicted_risk += 20  # Will be very high
        else:
            # Non-DB servers: high memory = OOM risk
            # CURRENT
            if current_mem >= 98:
                current_risk += 60  # CRITICAL - OOM imminent
            elif current_mem >= 95:
                current_risk += 40  # High memory pressure
            elif current_mem >= 90:
                current_risk += 20  # Elevated
            # PREDICTED
            if max_mem_p90 >= 98:
                predicted_risk += 30  # Will OOM
            elif max_mem_p90 >= 95:
                predicted_risk += 20  # Will have pressure

    # =========================================================================
    # LOAD AVERAGE RISK ASSESSMENT (System load indicator)
    # =========================================================================
    if 'load_average' in server_pred:
        load = server_pred['load_average']
        current_load = load.get('current', 0)
        p90 = load.get('p90', [])
        max_load_p90 = max(p90[:6]) if p90 and len(p90) >= 6 else current_load

        # Load average > 8 on typical servers indicates heavy queuing
        # CURRENT
        if current_load > 12:
            current_risk += 25  # Severe load
        elif current_load > 8:
            current_risk += 15  # High load
        elif current_load > 6:
            current_risk += 5   # Elevated
        # PREDICTED
        if max_load_p90 > 12:
            predicted_risk += 12  # Will be severely loaded
        elif max_load_p90 > 8:
            predicted_risk += 8   # Will be high

    # =========================================================================
    # WEIGHTED FINAL SCORE
    # =========================================================================
    # 70% current state (executives care about NOW)
    # 30% predictions (early warning value)
    final_risk = (current_risk * 0.7) + (predicted_risk * 0.3)

    return min(final_risk, 100)

def get_risk_color(risk_score: float) -> str:
    """Get color for risk score."""
    if risk_score >= 70:
        return "#ff4444"  # Red
    elif risk_score >= 40:
        return "#ff9900"  # Orange
    elif risk_score >= 20:
        return "#ffcc00"  # Yellow
    else:
        return "#44ff44"  # Green

# =============================================================================
# SIDEBAR
# =============================================================================

with st.sidebar:
    st.title("‚öôÔ∏è Dashboard Settings")

    # Daemon connection
    st.subheader("Daemon Connection")
    daemon_url = st.text_input(
        "Daemon URL",
        value=st.session_state.daemon_url,
        help="URL of the TFT inference daemon"
    )
    st.session_state.daemon_url = daemon_url

    # Test connection
    client = DaemonClient(daemon_url)
    health = client.check_health()

    if health['connected']:
        st.success("‚úÖ Connected to daemon")
        st.session_state.daemon_connected = True
        health_data = health.get('data', {})
        st.caption(f"Status: {health_data.get('status', 'Unknown')}")

        # Show warmup status if available
        try:
            status_response = requests.get(f"{daemon_url}/status", timeout=2)
            if status_response.ok:
                status_data = status_response.json()
                warmup = status_data.get('warmup', {})
                if not warmup.get('is_warmed_up', True):
                    progress = warmup.get('progress_percent', 0)
                    st.warning(f"‚è≥ {warmup.get('message', 'Model warming up')} ({progress:.0f}%)")
                    st.progress(progress / 100)
                else:
                    st.caption(f"‚úÖ {warmup.get('message', 'Model ready')}")
        except:
            pass
    else:
        st.error(f"‚ùå Not connected: {health.get('error', 'Unknown error')}")
        st.session_state.daemon_connected = False
        st.info("Start daemon with:\n```\npython tft_inference.py --daemon\n```")

    st.divider()

    # Refresh settings
    st.subheader("Refresh Settings")
    refresh_interval = st.slider(
        "Auto-refresh interval (seconds)",
        min_value=5,
        max_value=300,
        value=st.session_state.refresh_interval,
        step=5,
        help="How often to fetch new predictions"
    )
    st.session_state.refresh_interval = refresh_interval

    auto_refresh = st.checkbox("Enable auto-refresh", value=True)

    if st.button("üîÑ Refresh Now", width='stretch'):
        # Force cache clear
        if 'cached_predictions' in st.session_state:
            del st.session_state['cached_predictions']
        if 'cached_alerts' in st.session_state:
            del st.session_state['cached_alerts']
        st.session_state.last_update = None
        st.rerun()

    st.divider()

    # Interactive Demo Control (Scenario Switcher)
    st.subheader("üé¨ Interactive Demo Control")

    @st.fragment
    def render_scenario_controls():
        """Scenario control buttons - isolated to prevent full app rerun"""
        if st.session_state.daemon_connected:
            st.markdown("**Scenario Control:** (Changes metrics generator behavior)")

            col1, col2, col3 = st.columns(3)

            # Metrics generator URL (port 8001)
            generator_url = "http://localhost:8001"

            with col1:
                if st.button("üü¢ Healthy", width='stretch', key="scenario_healthy"):
                    try:
                        response = requests.post(
                            f"{generator_url}/scenario/set",
                            json={"scenario": "healthy"},
                            timeout=2
                        )
                        if response.ok:
                            st.success("‚úÖ Scenario: Healthy - All servers recovering")
                        else:
                            st.error(f"Failed to change scenario: {response.status_code}")
                    except Exception as e:
                        st.error(f"Cannot connect to metrics generator at {generator_url}")

            with col2:
                if st.button("üü° Degrading", width='stretch', key="scenario_degrading"):
                    try:
                        response = requests.post(
                            f"{generator_url}/scenario/set",
                            json={"scenario": "degrading"},
                            timeout=2
                        )
                        if response.ok:
                            result = response.json()
                            st.warning(f"‚ö†Ô∏è Scenario: Degrading - {result.get('affected_servers', 0)} servers affected")
                        else:
                            st.error(f"Failed to change scenario: {response.status_code}")
                    except Exception as e:
                        st.error(f"Cannot connect to metrics generator at {generator_url}")

            with col3:
                if st.button("üî¥ Critical", width='stretch', key="scenario_critical"):
                    try:
                        response = requests.post(
                            f"{generator_url}/scenario/set",
                            json={"scenario": "critical"},
                            timeout=2
                        )
                        if response.ok:
                            result = response.json()
                            st.error(f"üî¥ Scenario: Critical - {result.get('affected_servers', 0)} servers in crisis!")
                        else:
                            st.error(f"Failed to change scenario: {response.status_code}")
                    except Exception as e:
                        st.error(f"Cannot connect to metrics generator at {generator_url}")

            # Show current scenario status
            try:
                scenario_response = requests.get(f"{generator_url}/scenario/status", timeout=1)
                if scenario_response.ok:
                    status = scenario_response.json()
                    st.info(f"üìä Current: {status['scenario'].upper()} | "
                            f"Affected: {status.get('total_affected', 0)} servers | "
                            f"Tick: {status.get('tick_count', 0)}")
            except:
                st.caption("üí° Tip: Metrics generator should be running on port 8001")
        else:
            st.warning("‚ö†Ô∏è Connect to daemon to use scenario control")

    # Render the fragment
    render_scenario_controls()

    st.divider()

    # Demo Mode
    st.subheader("üé¨ Demo Mode (Legacy)")

    demo_enabled = st.checkbox(
        "Enable Demo Mode",
        value=st.session_state.demo_mode,
        help="Generate and stream demo scenarios in real-time"
    )
    st.session_state.demo_mode = demo_enabled

    if demo_enabled:
        st.markdown("**Select Scenario:**")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("üü¢ Stable", width='stretch', disabled=st.session_state.demo_running):
                initialize_demo_generator('stable', fleet_size=90)
                st.success("‚úÖ Stable scenario initialized - All 90 servers streaming every 5s")

        with col2:
            if st.button("üü° Degrading", width='stretch', disabled=st.session_state.demo_running):
                initialize_demo_generator('degrading', fleet_size=90)
                st.success("‚úÖ Degrading scenario initialized - Servers will gradually degrade")

        with col3:
            if st.button("üî¥ Critical", width='stretch', disabled=st.session_state.demo_running):
                initialize_demo_generator('critical', fleet_size=90)
                st.success("‚úÖ Critical scenario initialized - Rapid degradation to critical levels")

        # Demo status
        if st.session_state.demo_running and st.session_state.demo_generator is not None:
            scenario_info = st.session_state.demo_generator.get_scenario_info()
            st.info(f"üé¨ Running: {st.session_state.demo_scenario.upper()}")

            # Show scenario progress
            tick_count = scenario_info['tick_count']
            elapsed = (datetime.now() - st.session_state.demo_start_time).total_seconds()
            st.caption(f"Ticks: {tick_count} | Elapsed: {elapsed:.0f}s | Streaming every 5s")

            # Show degrading servers
            if scenario_info['degrading_servers'] > 0:
                st.caption(f"‚ö†Ô∏è {scenario_info['degrading_servers']} servers degrading")

            if st.button("‚èπÔ∏è Stop Demo", width='stretch'):
                stop_demo()
                st.success("‚úÖ Demo stopped")

    st.divider()

    # Info
    st.subheader("üìä Dashboard Info")
    if st.session_state.last_update:
        st.caption(f"Last update: {st.session_state.last_update.strftime('%H:%M:%S')}")
    st.caption(f"Refresh interval: {refresh_interval}s")

    # Status indicator
    st.divider()
    if st.session_state.daemon_connected:
        if st.session_state.demo_running:
            st.info("üé¨ Demo Mode Active")
        else:
            st.success("üü¢ Dashboard Active")
    else:
        st.warning("üü° Daemon Offline")

# =============================================================================
# MAIN DASHBOARD
# =============================================================================

st.title("üîÆ TFT Monitoring Dashboard")
st.caption("Temporal Fusion Transformer - Real-time Server Monitoring & Prediction")

# Demo Mode: Stream data to daemon (time-based, no blocking)
if st.session_state.demo_running and st.session_state.demo_generator is not None:
    current_time = datetime.now()

    # Only generate tick if 5 seconds have passed since last tick
    should_tick = (
        st.session_state.last_demo_tick is None or
        (current_time - st.session_state.last_demo_tick).total_seconds() >= 5
    )

    if should_tick:
        # Generate next tick of data
        tick_data = st.session_state.demo_generator.generate_tick()

        # Convert to records and feed to daemon
        records = tick_data.to_dict('records')

        # Convert timestamps to strings for JSON serialization
        for record in records:
            if 'timestamp' in record:
                record['timestamp'] = str(record['timestamp'])

        if st.session_state.daemon_connected:
            # Feed data to daemon
            success = client.feed_data(records)

            if not success:
                st.warning("‚ö†Ô∏è Failed to feed demo data to daemon")

        # Update last tick time
        st.session_state.last_demo_tick = current_time

# Fetch current data (with smart caching to avoid re-fetching on UI changes)
if st.session_state.daemon_connected:
    # Only fetch if refresh interval has passed or no cached data
    should_fetch = (
        'cached_predictions' not in st.session_state or
        st.session_state.last_update is None or
        (datetime.now() - st.session_state.last_update).total_seconds() >= refresh_interval
    )

    if should_fetch:
        with st.spinner('üîÆ Fetching predictions from TFT model...'):
            predictions = client.get_predictions()
            alerts = client.get_alerts()
            st.session_state.last_update = datetime.now()

            # Cache for fast UI updates
            st.session_state.cached_predictions = predictions
            st.session_state.cached_alerts = alerts

        # Store in history
        if predictions:
            st.session_state.history.append({
                'timestamp': datetime.now(),
                'predictions': predictions
            })
            # Keep last 100 entries
            st.session_state.history = st.session_state.history[-100:]
    else:
        # Use cached data (much faster for dropdown changes!)
        predictions = st.session_state.get('cached_predictions')
        alerts = st.session_state.get('cached_alerts')
else:
    predictions = None
    alerts = None
    st.warning("‚ö†Ô∏è Daemon not connected. Connect to see live predictions.")

# =============================================================================
# TABS
# =============================================================================

tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8, tab9, tab10 = st.tabs([
    "üìä Overview",
    "üî• Heatmap",
    "‚ö†Ô∏è Top 5 Servers",
    "üìà Historical",
    "üí∞ Cost Avoidance",
    "ü§ñ Auto-Remediation",
    "üì± Alerting Strategy",
    "‚öôÔ∏è Advanced",
    "üìö Documentation",
    "üó∫Ô∏è Roadmap"
])

# =============================================================================
# TAB 1: OVERVIEW
# =============================================================================

with tab1:
    if predictions:
        # Check warmup status and show warning if model not ready
        try:
            status_response = requests.get(f"{daemon_url}/status", timeout=2)
            if status_response.ok:
                status_data = status_response.json()
                warmup = status_data.get('warmup', {})
                if not warmup.get('is_warmed_up', True):
                    progress = warmup.get('progress_percent', 0)
                    st.warning(f"""
                    ‚è≥ **Model Warming Up** ({progress:.0f}% complete)

                    The model is still learning from incoming data. Predictions may be inconsistent during warm-up.
                    Once warmed up, all metrics will tell a consistent story.

                    **What's happening:** The model has {warmup.get('current_size', 0)}/{warmup.get('required_size', 288)} data points needed per server for reliable predictions.
                    """, icon="‚è≥")
        except:
            pass

        # Environment status
        status_text, status_color, status_emoji = get_health_status(predictions)

        # Top KPIs
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            # Build status explanation
            if status_text == "Critical":
                help_text = "üî¥ CRITICAL: >30% of fleet is Critical/Imminent Failure (Risk 80+) OR >50% are in Danger/Warning state. Immediate action required."
            elif status_text == "Warning":
                help_text = "üü† WARNING: >10% of fleet is Critical/Imminent Failure (Risk 80+) OR >30% are in Danger/Warning state. Monitor closely."
            elif status_text == "Caution":
                help_text = "üü° CAUTION: >10% of fleet is Degrading (Risk 50+). Early warning - investigate soon."
            elif status_text == "Healthy":
                help_text = "üü¢ HEALTHY: <10% of fleet has elevated risk. Normal operations."
            else:
                help_text = "Status unknown - waiting for predictions."

            st.metric(
                label="Environment Status",
                value=f"{status_emoji} {status_text}",
                delta=None,
                help=help_text
            )

        with col2:
            env = predictions.get('environment', {})
            prob_30m = env.get('prob_30m', 0) * 100
            st.metric(
                label="Incident Risk (30m)",
                value=f"{prob_30m:.1f}%",
                delta=None,
                help="Probability of incident in next 30 minutes"
            )

        with col3:
            prob_8h = env.get('prob_8h', 0) * 100
            st.metric(
                label="Incident Risk (8h)",
                value=f"{prob_8h:.1f}%",
                delta=None,
                help="Probability of incident in next 8 hours"
            )

        with col4:
            server_preds = predictions.get('predictions', {})
            total_servers = len(server_preds)
            healthy_count = sum(1 for s, p in server_preds.items() if calculate_server_risk_score(p) < 20)
            st.metric(
                label="Fleet Status",
                value=f"{healthy_count}/{total_servers}",
                delta=None,
                help="Healthy servers / Total servers"
            )

        st.divider()

        # Actual vs Predicted Comparison (Management View)
        st.subheader("üéØ Actual State vs AI Prediction")
        st.markdown("**Show the power of predictive AI** - Current reality vs future forecast")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### üìç **Actual Current State**")
            st.markdown("_(What's happening RIGHT NOW)_")

            # Try to get actual scenario from generator
            try:
                generator_url = "http://localhost:8001"
                scenario_response = requests.get(f"{generator_url}/scenario/status", timeout=1)
                if scenario_response.ok:
                    status = scenario_response.json()
                    actual_scenario = status['scenario'].upper()
                    actual_affected = status.get('total_affected', 0)

                    if actual_scenario == 'HEALTHY':
                        st.success(f"‚úÖ **{actual_scenario}**")
                        st.metric("Affected Servers (Now)", f"{actual_affected}")
                        st.caption("Environment is currently operating normally")
                    elif actual_scenario == 'DEGRADING':
                        st.warning(f"‚ö†Ô∏è **{actual_scenario}**")
                        st.metric("Affected Servers (Now)", f"{actual_affected}")
                        st.caption("Some servers experiencing issues")
                    else:  # CRITICAL
                        st.error(f"üî¥ **{actual_scenario}**")
                        st.metric("Affected Servers (Now)", f"{actual_affected}")
                        st.caption("Multiple servers in critical state")
                else:
                    st.info("Actual state: Unknown (generator offline)")
            except:
                st.info("üí° Actual state: Unknown (connect metrics generator on port 8001)")

        with col2:
            st.markdown("### üîÆ **AI Prediction (Next 30min-8h)**")
            st.markdown("_(What the AI forecasts will happen)_")

            # Show prediction based on ACTUAL incident probabilities (not server risk scores)
            env = predictions.get('environment', {})
            prob_30m = env.get('prob_30m', 0) * 100
            prob_8h = env.get('prob_8h', 0) * 100

            # Determine prediction status based on incident probabilities
            if prob_30m > 70 or prob_8h > 85:
                predicted_status = "CRITICAL"
                st.error(f"üî¥ **{predicted_status}**")
                st.caption("‚ö†Ô∏è High probability of incidents ahead")
            elif prob_30m > 40 or prob_8h > 60:
                predicted_status = "WARNING"
                st.warning(f"üü† **{predicted_status}**")
                st.caption("‚ö†Ô∏è Elevated risk of incidents")
            elif prob_30m > 20 or prob_8h > 40:
                predicted_status = "CAUTION"
                st.warning(f"üü° **{predicted_status}**")
                st.caption("‚ö†Ô∏è Minor risk indicators detected")
            else:
                predicted_status = "HEALTHY"
                st.success(f"‚úÖ **{predicted_status}**")
                st.caption("AI predicts continued stability")

            st.metric("Predicted Incident Risk (30m)", f"{prob_30m:.1f}%")
            st.metric("Predicted Incident Risk (8h)", f"{prob_8h:.1f}%")

        # Insight box
        try:
            scenario_response = requests.get(f"{generator_url}/scenario/status", timeout=1)
            if scenario_response.ok:
                status = scenario_response.json()
                actual_scenario = status['scenario'].upper()

                if actual_scenario == 'HEALTHY' and predicted_status in ['CRITICAL', 'WARNING']:
                    st.warning(f"""
                    üéØ **This is the value of Predictive AI!**

                    - **Current Reality**: Environment is HEALTHY (no active issues)
                    - **AI Forecast**: {prob_30m:.0f}% risk in 30m, {prob_8h:.0f}% risk in 8h
                    - **Action Window**: Act NOW to prevent issues before they occur
                    - **Value**: Proactive prevention vs reactive firefighting
                    """)
                elif actual_scenario != 'HEALTHY' and predicted_status in ['CRITICAL', 'WARNING']:
                    st.info("""
                    ‚úÖ **AI accurately detecting ongoing issues**

                    The model is correctly identifying the current degradation and predicting continued problems.
                    """)
                elif actual_scenario == 'HEALTHY' and predicted_status == 'HEALTHY':
                    st.success("""
                    ‚úÖ **All systems stable**

                    Both current state and predictions show healthy operations.
                    """)
        except:
            pass

        st.divider()

        # Server Risk Distribution
        st.subheader("üìä Fleet Risk Distribution")

        if server_preds:
            # Calculate risk scores
            server_risks = []
            for server_name, server_pred in server_preds.items():
                risk_score = calculate_server_risk_score(server_pred)
                server_risks.append({
                    'Server': server_name,
                    'Risk Score': risk_score,
                    'Status': 'Critical' if risk_score >= 70 else
                             'Warning' if risk_score >= 40 else
                             'Caution' if risk_score >= 20 else 'Healthy'
                })

            risk_df = pd.DataFrame(server_risks)

            col1, col2 = st.columns([2, 1])

            with col1:
                # Bar chart of risks
                fig = px.bar(
                    risk_df.sort_values('Risk Score', ascending=False).head(15),
                    x='Server',
                    y='Risk Score',
                    color='Risk Score',
                    color_continuous_scale=['green', 'yellow', 'orange', 'red'],
                    range_color=[0, 100],
                    title="Top 15 Servers by Risk Score",
                    height=400
                )
                fig.update_layout(xaxis_tickangle=-45)
                st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False})

            with col2:
                # Risk distribution pie
                status_counts = risk_df['Status'].value_counts()
                fig = px.pie(
                    values=status_counts.values,
                    names=status_counts.index,
                    title="Server Status Distribution",
                    color=status_counts.index,
                    color_discrete_map={
                        'Healthy': 'green',
                        'Caution': 'yellow',
                        'Warning': 'orange',
                        'Critical': 'red'
                    },
                    height=400
                )
                st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False})

        st.divider()

        # Active Alerts with Real vs Predicted
        st.subheader("üîî Active Alerts")
        st.markdown("**Real-time comparison**: Current values vs AI predictions")

        if predictions and server_preds:
            # Build enhanced alert table with actual vs predicted
            alert_rows = []

            for server_name, server_pred in server_preds.items():
                risk_score = calculate_server_risk_score(server_pred)

                # Only show servers with risk >= 50 (Warning and above)
                if risk_score >= 50:
                    # Determine alert severity based on risk score
                    if risk_score >= 90:
                        severity = "üî¥ Imminent Failure"
                        priority = "Imminent Failure"
                    elif risk_score >= 80:
                        severity = "üî¥ Critical"
                        priority = "Critical"
                    elif risk_score >= 70:
                        severity = "üü† Danger"
                        priority = "Danger"
                    elif risk_score >= 60:
                        severity = "üü° Warning"
                        priority = "Warning"
                    elif risk_score >= 50:
                        severity = "üü¢ Degrading"
                        priority = "Degrading"
                    else:
                        continue

                    # Get actual vs predicted for key LINBORG metrics
                    # CPU: Calculate from idle (100 - idle) or sum components
                    cpu_idle_cur = server_pred.get('cpu_idle_pct', {}).get('current', 0)
                    cpu_user_cur = server_pred.get('cpu_user_pct', {}).get('current', 0)
                    cpu_sys_cur = server_pred.get('cpu_sys_pct', {}).get('current', 0)
                    cpu_iowait_cur = server_pred.get('cpu_iowait_pct', {}).get('current', 0)

                    cpu_actual = 100 - cpu_idle_cur if cpu_idle_cur > 0 else (cpu_user_cur + cpu_sys_cur + cpu_iowait_cur)

                    cpu_idle_p50 = server_pred.get('cpu_idle_pct', {}).get('p50', [])
                    if cpu_idle_p50 and len(cpu_idle_p50) >= 6:
                        cpu_predicted = 100 - np.mean(cpu_idle_p50[:6])
                    else:
                        cpu_predicted = cpu_actual

                    # I/O Wait - CRITICAL metric
                    iowait_actual = cpu_iowait_cur
                    iowait_p50 = server_pred.get('cpu_iowait_pct', {}).get('p50', [])
                    iowait_predicted = np.mean(iowait_p50[:6]) if iowait_p50 and len(iowait_p50) >= 6 else iowait_actual

                    # Memory
                    mem_actual = server_pred.get('mem_used_pct', {}).get('current', 0)
                    mem_p50 = server_pred.get('mem_used_pct', {}).get('p50', [])
                    mem_predicted = np.mean(mem_p50[:6]) if mem_p50 and len(mem_p50) >= 6 else mem_actual

                    alert_rows.append({
                        'Priority': priority,
                        'Server': server_name,
                        'Profile': get_server_profile(server_name),
                        'Risk': f"{risk_score:.0f}",
                        'CPU Now': f"{cpu_actual:.1f}%",
                        'CPU Predicted (30m)': f"{cpu_predicted:.1f}%",
                        'CPU Œî': f"{(cpu_predicted - cpu_actual):+.1f}%",
                        'I/O Wait Now': f"{iowait_actual:.1f}%",
                        'I/O Wait Predicted (30m)': f"{iowait_predicted:.1f}%",
                        'I/O Wait Œî': f"{(iowait_predicted - iowait_actual):+.1f}%",
                        'Mem Now': f"{mem_actual:.1f}%",
                        'Mem Predicted (30m)': f"{mem_predicted:.1f}%",
                        'Mem Œî': f"{(mem_predicted - mem_actual):+.1f}%"
                    })

            if alert_rows:
                st.markdown(f"**{len(alert_rows)} servers requiring attention**")

                # Sort by risk score (descending)
                alert_df = pd.DataFrame(alert_rows)
                alert_df = alert_df.sort_values('Risk', ascending=False)

                # Display the enhanced table
                st.dataframe(
                    alert_df,
                    width='stretch',
                    hide_index=True,
                    column_config={
                        'Priority': st.column_config.TextColumn('Priority', width='medium', help='Imminent Failure (90+) ‚Üí Critical (80-89) ‚Üí Danger (70-79) ‚Üí Warning (60-69) ‚Üí Degrading (50-59)'),
                        'Server': st.column_config.TextColumn('Server', width='medium', help='Server hostname'),
                        'Profile': st.column_config.TextColumn('Profile', width='medium', help='Server workload type (ML Compute, Database, Web API, etc.)'),
                        'Risk': st.column_config.TextColumn('Risk', width='small', help='Overall risk score (0-100). Higher = more urgent'),
                        'CPU Now': st.column_config.TextColumn('CPU Now', width='small', help='Current CPU utilization (% Used = 100 - Idle)'),
                        'CPU Predicted (30m)': st.column_config.TextColumn('CPU Pred', width='small', help='AI predicted CPU in next 30 minutes'),
                        'CPU Œî': st.column_config.TextColumn('CPU Œî', width='small', help='Predicted change (Delta). Positive (+) = increasing/degrading, Negative (-) = decreasing/improving'),
                        'I/O Wait Now': st.column_config.TextColumn('I/O Wait Now', width='small', help='Current I/O wait % - CRITICAL troubleshooting metric. High values indicate disk/storage bottleneck'),
                        'I/O Wait Predicted (30m)': st.column_config.TextColumn('I/O Wait Pred', width='small', help='AI predicted I/O wait in next 30 minutes'),
                        'I/O Wait Œî': st.column_config.TextColumn('I/O Œî', width='small', help='Predicted change (Delta). Positive (+) = increasing I/O contention'),
                        'Mem Now': st.column_config.TextColumn('Mem Now', width='small', help='Current memory utilization'),
                        'Mem Predicted (30m)': st.column_config.TextColumn('Mem Pred', width='small', help='AI predicted memory in next 30 minutes'),
                        'Mem Œî': st.column_config.TextColumn('Mem Œî', width='small', help='Predicted change (Delta). Positive (+) = increasing/degrading, Negative (-) = decreasing/improving')
                    }
                )

                # Summary metrics - Server counts by severity
                col1, col2, col3, col4 = st.columns(4)

                with col1:
                    critical_count = len([r for r in alert_rows if r['Priority'] in ['Imminent Failure', 'Critical']])
                    st.metric("üî¥ Critical+", critical_count, delta=None, help="Risk >= 80 - Immediate action required")

                with col2:
                    danger_count = len([r for r in alert_rows if r['Priority'] == 'Danger'])
                    st.metric("üü† Danger", danger_count, delta=None, help="Risk 70-79 - High priority attention needed")

                with col3:
                    warning_count = len([r for r in alert_rows if r['Priority'] == 'Warning'])
                    st.metric("üü° Warning", warning_count, delta=None, help="Risk 60-69 - Monitor closely")

                with col4:
                    degrading_count = len([r for r in alert_rows if r['Priority'] == 'Degrading'])
                    st.metric("üü¢ Degrading", degrading_count, delta=None, help="Risk 50-59 - Performance declining")

                # Show healthy/watch servers in separate row
                st.markdown("---")
                col1, col2 = st.columns(2)

                with col1:
                    total_servers = len(server_preds)
                    healthy_count = total_servers - len(alert_rows)
                    st.metric("‚úÖ Healthy", healthy_count, delta=None, help="Risk < 50 - Normal operations, not shown in alerts table")

                with col2:
                    # Watch = servers with risk 30-49 (not shown in alerts but worth noting)
                    watch_count = sum(1 for s, p in server_preds.items() if 30 <= calculate_server_risk_score(p) < 50)
                    st.metric("üëÅÔ∏è Watch", watch_count, delta=None, help="Risk 30-49 - Low concern, monitoring only")

                # Trend analysis (separate row, clearly marked as subset)
                st.markdown("---")
                st.markdown("**üìà Trend Analysis** (of the alerts above)")

                col1, col2 = st.columns(2)
                with col1:
                    # Calculate degrading metrics
                    degrading = sum(1 for r in alert_rows if '+' in r['CPU Œî'] or '+' in r['Mem Œî'])
                    pct_degrading = (degrading / len(alert_rows) * 100) if alert_rows else 0
                    st.metric(
                        "‚¨ÜÔ∏è Degrading",
                        f"{degrading}/{len(alert_rows)}",
                        delta=None,
                        help=f"{pct_degrading:.0f}% of alerts showing increasing CPU/Memory trends"
                    )

                with col2:
                    # Calculate improving metrics
                    improving = sum(1 for r in alert_rows if '-' in r['CPU Œî'] or '-' in r['Mem Œî'])
                    pct_improving = (improving / len(alert_rows) * 100) if alert_rows else 0
                    st.metric(
                        "‚¨áÔ∏è Improving",
                        f"{improving}/{len(alert_rows)}",
                        delta=None,
                        help=f"{pct_improving:.0f}% of alerts showing decreasing CPU/Memory trends"
                    )

                st.markdown("---")

                # Add insight with better context
                if critical_count > 0:
                    st.error(f"‚ö†Ô∏è **Action Required**: {critical_count} critical server(s) need immediate attention")
                elif danger_count > 0:
                    st.warning(f"‚ö†Ô∏è **High Priority**: {danger_count} server(s) in danger state")
                elif warning_count > 0:
                    st.warning(f"‚ö†Ô∏è **Monitor Closely**: {warning_count} server(s) showing warning signs")

                # Show summary explanation
                st.caption(f"üìä Total fleet: {total_servers} servers | Showing: {len(alert_rows)} alerts | Hidden: {healthy_count} healthy")

            else:
                st.success("‚úÖ No active alerts - All servers healthy!")
        else:
            st.info("No alert data available")

    else:
        st.info("üëà Connect to daemon to see live predictions")
        st.markdown("""
        **To get started:**

        1. Start the inference daemon:
           ```bash
           python tft_inference.py --daemon --port 8000
           ```

        2. The dashboard will automatically connect and display live predictions

        3. Predictions update every {refresh_interval} seconds
        """)

# =============================================================================
# TAB 2: HEATMAP
# =============================================================================

with tab2:
    st.subheader("üî• Server Fleet Heatmap")

    if predictions and predictions.get('predictions'):
        server_preds = predictions['predictions']

        # Metric options
        metric_options = {
            'Risk Score': 'risk',
            'CPU (p90)': 'cpu',
            'Memory (p90)': 'memory',
            'Latency (p90)': 'latency'
        }

        # Cache key for heatmap data (only recalculate when predictions change)
        predictions_hash = str(predictions.get('timestamp', '')) if predictions else ''
        cache_key = f'heatmap_data_{predictions_hash}'

        # Build or retrieve cached heatmap data for ALL metrics at once
        if cache_key not in st.session_state:
            # Calculate all metrics at once to avoid recalculation
            heatmap_cache = {}

            for metric_name, mk in metric_options.items():
                metric_data = []
                for server_name, server_pred in server_preds.items():
                    if mk == 'risk':
                        value = calculate_server_risk_score(server_pred)
                    elif mk == 'cpu':
                        cpu = server_pred.get('cpu_percent', {})
                        p90 = cpu.get('p90', [])
                        value = max(p90[:6]) if len(p90) >= 6 else (max(p90) if p90 else 0)
                    elif mk == 'memory':
                        mem = server_pred.get('memory_percent', {})
                        p90 = mem.get('p90', [])
                        value = max(p90[:6]) if len(p90) >= 6 else (max(p90) if p90 else 0)
                    elif mk == 'latency':
                        lat = server_pred.get('load_average', {})
                        p90 = lat.get('p90', [])
                        value = max(p90[:6]) if len(p90) >= 6 else (max(p90) if p90 else 0)
                    else:
                        value = 0

                    metric_data.append({
                        'Server': server_name,
                        'Value': value
                    })

                heatmap_cache[metric_name] = pd.DataFrame(metric_data).sort_values('Value', ascending=False)

            st.session_state[cache_key] = heatmap_cache

        # Fragment: Isolate heatmap rendering to prevent full app rerun on dropdown change
        @st.fragment
        def render_heatmap_fragment():
            """Render heatmap with metric selector - isolated to prevent full app rerun"""
            # Metric selector
            selected_metric = st.selectbox(
                "Select metric to display",
                options=list(metric_options.keys()),
                index=0,
                key='heatmap_metric_selector'
            )

            metric_key = metric_options[selected_metric]

            # Retrieve cached data for selected metric
            heatmap_df = st.session_state[cache_key][selected_metric]

            # Display as grid using st.columns (with @st.fragment this is now fast!)
            servers_per_row = 5
            rows = [heatmap_df.iloc[i:i+servers_per_row] for i in range(0, len(heatmap_df), servers_per_row)]

            # Render grid
            for row_data in rows:
                cols = st.columns(servers_per_row)
                for idx, (_, server_row) in enumerate(row_data.iterrows()):
                    if idx < len(cols):
                        with cols[idx]:
                            server_name = server_row['Server']
                            value = server_row['Value']

                            # Determine color
                            if metric_key == 'risk':
                                color = get_risk_color(value)
                            else:
                                # Scale color based on value
                                if value > 90:
                                    color = "#ff4444"
                                elif value > 70:
                                    color = "#ff9900"
                                elif value > 50:
                                    color = "#ffcc00"
                                else:
                                    color = "#44ff44"

                            # Display server card
                            st.markdown(
                                f'<div style="background-color: {color}; padding: 15px; border-radius: 5px; text-align: center; margin: 5px;">'
                                f'<strong style="color: #000;">{server_name}</strong><br>'
                                f'<span style="font-size: 24px; color: #000;">{value:.1f}</span>'
                                f'</div>',
                                unsafe_allow_html=True
                            )

            st.divider()

            # Summary statistics
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("Minimum", f"{heatmap_df['Value'].min():.1f}")
            with col2:
                st.metric("Average", f"{heatmap_df['Value'].mean():.1f}")
            with col3:
                st.metric("Maximum", f"{heatmap_df['Value'].max():.1f}")
            with col4:
                st.metric("Std Dev", f"{heatmap_df['Value'].std():.1f}")

        # Execute the fragment (only THIS section reruns on dropdown change!)
        render_heatmap_fragment()

    else:
        st.info("Connect to daemon to see server heatmap")

# =============================================================================
# TAB 3: TOP 5 SERVERS
# =============================================================================

with tab3:
    st.subheader("‚ö†Ô∏è Top 5 Problem Servers")

    if predictions and predictions.get('predictions'):
        server_preds = predictions['predictions']

        # Calculate risk scores
        server_risks = []
        for server_name, server_pred in server_preds.items():
            risk_score = calculate_server_risk_score(server_pred)
            server_risks.append({
                'server': server_name,
                'risk': risk_score,
                'pred': server_pred
            })

        # Sort by risk
        server_risks.sort(key=lambda x: x['risk'], reverse=True)
        top_5 = server_risks[:5]

        # Display each server
        for idx, server_info in enumerate(top_5):
            server_name = server_info['server']
            risk_score = server_info['risk']
            server_pred = server_info['pred']

            # Expander for each server
            with st.expander(f"#{idx+1} - {server_name} (Risk: {risk_score:.1f})", expanded=(idx == 0)):
                col1, col2 = st.columns([1, 2])

                with col1:
                    # Risk gauge
                    fig = go.Figure(go.Indicator(
                        mode="gauge+number",
                        value=risk_score,
                        title={'text': "Risk Score"},
                        gauge={
                            'axis': {'range': [0, 100]},
                            'bar': {'color': get_risk_color(risk_score)},
                            'steps': [
                                {'range': [0, 20], 'color': "lightgreen"},
                                {'range': [20, 40], 'color': "yellow"},
                                {'range': [40, 70], 'color': "orange"},
                                {'range': [70, 100], 'color': "red"}
                            ],
                            'threshold': {
                                'line': {'color': "black", 'width': 4},
                                'thickness': 0.75,
                                'value': risk_score
                            }
                        }
                    ))
                    fig.update_layout(height=250)
                    st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"gauge_{server_name}")

                with col2:
                    # Show current vs predicted side-by-side
                    st.markdown("**Current State vs Predictions (30min ahead)**")

                    # Create comparison table - LINBORG metrics
                    metric_rows = []

                    # CPU Used (100 - idle)
                    cpu_idle_cur = server_pred.get('cpu_idle_pct', {}).get('current', 0)
                    cpu_user_cur = server_pred.get('cpu_user_pct', {}).get('current', 0)
                    cpu_sys_cur = server_pred.get('cpu_sys_pct', {}).get('current', 0)
                    cpu_iowait_cur = server_pred.get('cpu_iowait_pct', {}).get('current', 0)

                    cpu_current = 100 - cpu_idle_cur if cpu_idle_cur > 0 else (cpu_user_cur + cpu_sys_cur + cpu_iowait_cur)

                    cpu_idle_p50 = server_pred.get('cpu_idle_pct', {}).get('p50', [])
                    if cpu_idle_p50 and len(cpu_idle_p50) >= 6:
                        cpu_future = 100 - np.mean(cpu_idle_p50[:6])
                        cpu_delta = cpu_future - cpu_current
                        cpu_delta_str = f"+{cpu_delta:.1f}%" if cpu_delta > 0 else f"{cpu_delta:.1f}%"
                        metric_rows.append({
                            'Metric': 'CPU Used',
                            'Current': f"{cpu_current:.1f}%",
                            'Predicted': f"{cpu_future:.1f}%",
                            'Œî': cpu_delta_str
                        })

                    # I/O Wait - CRITICAL
                    if 'cpu_iowait_pct' in server_pred:
                        iowait = server_pred['cpu_iowait_pct']
                        current = iowait.get('current', 0)
                        p50 = iowait.get('p50', [])
                        if p50:
                            future = np.mean(p50[:6]) if len(p50) >= 6 else np.mean(p50)
                            delta = future - current
                            delta_str = f"+{delta:.1f}%" if delta > 0 else f"{delta:.1f}%"
                            metric_rows.append({
                                'Metric': 'I/O Wait',
                                'Current': f"{current:.1f}%",
                                'Predicted': f"{future:.1f}%",
                                'Œî': delta_str
                            })

                    # Memory
                    if 'mem_used_pct' in server_pred:
                        mem = server_pred['mem_used_pct']
                        current = mem.get('current', 0)
                        p50 = mem.get('p50', [])
                        if p50:
                            future = np.mean(p50[:6]) if len(p50) >= 6 else np.mean(p50)
                            delta = future - current
                            delta_str = f"+{delta:.1f}%" if delta > 0 else f"{delta:.1f}%"
                            metric_rows.append({
                                'Metric': 'Memory',
                                'Current': f"{current:.1f}%",
                                'Predicted': f"{future:.1f}%",
                                'Œî': delta_str
                            })

                    # Load Average
                    if 'load_average' in server_pred:
                        load = server_pred['load_average']
                        current = load.get('current', 0)
                        p50 = load.get('p50', [])
                        if p50:
                            future = np.mean(p50[:6]) if len(p50) >= 6 else np.mean(p50)
                            delta = future - current
                            delta_str = f"+{delta:.1f}" if delta > 0 else f"{delta:.1f}"
                            metric_rows.append({
                                'Metric': 'Load Avg',
                                'Current': f"{current:.1f}",
                                'Predicted': f"{future:.1f}",
                                'Œî': delta_str
                            })

                    # Display comparison table
                    if metric_rows:
                        df_comparison = pd.DataFrame(metric_rows)
                        st.dataframe(df_comparison, width='stretch', hide_index=True)

                        # Highlight if significant degradation is predicted
                        degrading_metrics = [row for row in metric_rows if '+' in row['Œî']]
                        if len(degrading_metrics) >= 2:
                            st.warning(f"‚ö†Ô∏è {len(degrading_metrics)} metrics predicted to increase")

                # Prediction timeline
                st.markdown("**Prediction Timeline (Next 8 hours)**")

                # Create timeline chart for CPU (calculated from idle)
                if 'cpu_idle_pct' in server_pred:
                    cpu_idle = server_pred['cpu_idle_pct']
                    # Convert idle predictions to CPU used (100 - idle)
                    p10_idle = cpu_idle.get('p10', [])
                    p50_idle = cpu_idle.get('p50', [])
                    p90_idle = cpu_idle.get('p90', [])

                    # Invert: higher idle = lower CPU used, so p10 idle = p90 CPU, p90 idle = p10 CPU
                    p10 = [100 - x for x in p90_idle] if p90_idle else []
                    p50 = [100 - x for x in p50_idle] if p50_idle else []
                    p90 = [100 - x for x in p10_idle] if p10_idle else []

                    if p10 and p50 and p90:
                        # Create time axis (5-min intervals for 8 hours = 96 steps)
                        time_steps = list(range(len(p50)))

                        fig = go.Figure()

                        # Add confidence band
                        fig.add_trace(go.Scatter(
                            x=time_steps + time_steps[::-1],
                            y=p90 + p10[::-1],
                            fill='toself',
                            fillcolor='rgba(0,100,250,0.2)',
                            line=dict(color='rgba(255,255,255,0)'),
                            name='p10-p90 range',
                            showlegend=True
                        ))

                        # Add p50 line
                        fig.add_trace(go.Scatter(
                            x=time_steps,
                            y=p50,
                            mode='lines',
                            name='p50 (median)',
                            line=dict(color='blue', width=2)
                        ))

                        # Add threshold lines
                        fig.add_hline(y=90, line_dash="dash", line_color="red",
                                     annotation_text="Critical (90%)")
                        fig.add_hline(y=70, line_dash="dash", line_color="orange",
                                     annotation_text="Warning (70%)")

                        fig.update_layout(
                            title="CPU Forecast",
                            xaxis_title="Time Steps (5-min intervals)",
                            yaxis_title="CPU %",
                            height=300,
                            hovermode='x'
                        )

                        st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"forecast_{server_name}")

    else:
        st.info("Connect to daemon to see top problem servers")

# =============================================================================
# TAB 4: HISTORICAL
# =============================================================================

with tab4:
    st.subheader("üìà Historical Trends")

    if st.session_state.history:
        # Time range selector
        col1, col2 = st.columns(2)

        with col1:
            lookback_minutes = st.slider(
                "Lookback period (minutes)",
                min_value=5,
                max_value=60,
                value=30,
                step=5
            )

        with col2:
            metric_to_plot = st.selectbox(
                "Metric to display",
                ["Environment Risk (30m)", "Environment Risk (8h)", "Fleet Health"]
            )

        # Filter history
        cutoff_time = datetime.now() - timedelta(minutes=lookback_minutes)
        recent_history = [h for h in st.session_state.history if h['timestamp'] >= cutoff_time]

        if recent_history:
            # Extract data
            timestamps = [h['timestamp'] for h in recent_history]

            if metric_to_plot == "Environment Risk (30m)":
                values = [h['predictions'].get('environment', {}).get('prob_30m', 0) * 100
                         for h in recent_history]
                ylabel = "Probability (%)"
                title = "Environment Incident Risk (30 minutes)"

            elif metric_to_plot == "Environment Risk (8h)":
                values = [h['predictions'].get('environment', {}).get('prob_8h', 0) * 100
                         for h in recent_history]
                ylabel = "Probability (%)"
                title = "Environment Incident Risk (8 hours)"

            else:  # Fleet Health
                values = []
                for h in recent_history:
                    preds = h['predictions'].get('predictions', {})
                    if preds:
                        healthy = sum(1 for s, p in preds.items() if calculate_server_risk_score(p) < 20)
                        total = len(preds)
                        values.append((healthy / total * 100) if total > 0 else 0)
                    else:
                        values.append(0)
                ylabel = "Healthy %"
                title = "Fleet Health Percentage"

            # Create chart
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=timestamps,
                y=values,
                mode='lines+markers',
                name=metric_to_plot,
                line=dict(color='blue', width=2),
                marker=dict(size=6)
            ))

            fig.update_layout(
                title=title,
                xaxis_title="Time",
                yaxis_title=ylabel,
                height=400,
                hovermode='x'
            )

            st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False})

            # Statistics
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("Current", f"{values[-1]:.1f}")
            with col2:
                st.metric("Average", f"{np.mean(values):.1f}")
            with col3:
                st.metric("Min", f"{np.min(values):.1f}")
            with col4:
                st.metric("Max", f"{np.max(values):.1f}")

            # Download data
            st.divider()

            if st.button("üì• Download Historical Data (CSV)"):
                df = pd.DataFrame({
                    'timestamp': timestamps,
                    'value': values
                })
                csv = df.to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name=f"tft_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
        else:
            st.info(f"No data collected in the last {lookback_minutes} minutes")

    else:
        st.info("No historical data yet. Data will accumulate as the dashboard runs.")

# =============================================================================
# TAB 5: COST AVOIDANCE
# =============================================================================

with tab5:
    st.subheader("üí∞ Cost Avoidance Dashboard")
    st.markdown("**POC Vision**: Real-time financial impact tracking")

    if predictions and predictions.get('predictions'):
        server_preds = predictions['predictions']
        env = predictions.get('environment', {})

        # Cost assumptions
        st.markdown("### üíµ Cost Assumptions (Configurable)")
        col1, col2, col3 = st.columns(3)

        with col1:
            cost_per_outage_hour = st.number_input(
                "Outage Cost ($/hour)",
                value=50000,
                step=5000,
                help="Average cost per hour of downtime for a critical server"
            )

        with col2:
            avg_outage_duration = st.number_input(
                "Avg Outage Duration (hours)",
                value=2.5,
                step=0.5,
                help="Average duration of an incident if not prevented"
            )

        with col3:
            prevention_rate = st.slider(
                "Prevention Success Rate (%)",
                min_value=50,
                max_value=100,
                value=85,
                help="Percentage of predicted incidents successfully prevented"
            )

        st.divider()

        # Calculate savings
        st.markdown("### üìä Projected Cost Avoidance")

        # Count servers at risk
        high_risk_servers = [s for s, p in server_preds.items()
                            if calculate_server_risk_score(p) >= 70]
        medium_risk_servers = [s for s, p in server_preds.items()
                              if 40 <= calculate_server_risk_score(p) < 70]

        # Calculate potential cost avoidance
        critical_incidents_prevented = len(high_risk_servers) * (prevention_rate / 100)
        warning_incidents_prevented = len(medium_risk_servers) * 0.3 * (prevention_rate / 100)

        total_incidents_prevented = critical_incidents_prevented + warning_incidents_prevented
        cost_avoided_per_incident = cost_per_outage_hour * avg_outage_duration
        total_cost_avoided = total_incidents_prevented * cost_avoided_per_incident

        # Monthly and annual projections
        daily_avoidance = total_cost_avoided
        monthly_avoidance = daily_avoidance * 30
        annual_avoidance = daily_avoidance * 365

        # Display key metrics
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric(
                "Incidents Prevented (Daily)",
                f"{total_incidents_prevented:.1f}",
                delta=None
            )

        with col2:
            st.metric(
                "Daily Cost Avoidance",
                f"${daily_avoidance:,.0f}",
                delta=None
            )

        with col3:
            st.metric(
                "Monthly Projection",
                f"${monthly_avoidance:,.0f}",
                delta=None,
                help="30-day projection"
            )

        with col4:
            st.metric(
                "Annual Projection",
                f"${annual_avoidance:,.0f}",
                delta=None,
                help="365-day projection"
            )

        st.divider()

        # ROI Calculator
        st.markdown("### üéØ ROI Analysis")

        col1, col2 = st.columns(2)

        with col1:
            project_cost = st.number_input(
                "Project Investment ($)",
                value=250000,
                step=25000,
                help="Total cost for development, deployment, and first year operations"
            )

        with col2:
            months_to_roi = (project_cost / monthly_avoidance) if monthly_avoidance > 0 else float('inf')

            if months_to_roi < 12:
                st.success(f"‚úÖ **ROI in {months_to_roi:.1f} months**")
                st.markdown(f"**Payback**: {months_to_roi:.1f} months")
                st.markdown(f"**First Year Net**: ${(annual_avoidance - project_cost):,.0f}")
            else:
                st.info(f"üìä **ROI in {months_to_roi:.1f} months**")

        st.divider()

        # At-Risk Servers Detail
        st.markdown("### üéØ Current At-Risk Servers")

        if high_risk_servers or medium_risk_servers:
            risk_breakdown = []

            for server in high_risk_servers:
                risk_breakdown.append({
                    'Server': server,
                    'Risk Level': 'Critical',
                    'Risk Score': calculate_server_risk_score(server_preds[server]),
                    'Potential Cost if Incident': f"${cost_avoided_per_incident:,.0f}",
                    'Status': 'üî¥ Action Required'
                })

            for server in medium_risk_servers[:5]:  # Top 5 medium risk
                risk_breakdown.append({
                    'Server': server,
                    'Risk Level': 'Warning',
                    'Risk Score': calculate_server_risk_score(server_preds[server]),
                    'Potential Cost if Incident': f"${cost_avoided_per_incident * 0.6:,.0f}",
                    'Status': 'üü† Monitor'
                })

            df_risks = pd.DataFrame(risk_breakdown)
            st.dataframe(df_risks, width='stretch', hide_index=True)
        else:
            st.success("‚úÖ No high-risk servers detected - Fleet is healthy!")

        st.divider()

        # Implementation Note
        st.info("""
        **üí° POC Implementation Note:**

        This tab demonstrates the financial impact tracking capability. In production, we would:

        - **Track actual incidents** prevented vs predicted (accuracy metrics)
        - **Integrate with ITSM** (ServiceNow, JIRA) for actual incident costs
        - **Historical cost avoidance** dashboard showing cumulative savings
        - **Per-profile cost models** (e.g., ML compute downtime costs more than generic servers)
        - **Executive reporting** with monthly/quarterly summaries

        **Business Case**: This system pays for itself in ~3-5 months based on typical financial services downtime costs.
        """)

    else:
        st.info("Connect to daemon to see cost avoidance projections")

# =============================================================================
# TAB 6: AUTO-REMEDIATION
# =============================================================================

with tab6:
    st.subheader("ü§ñ Auto-Remediation Strategy")
    st.markdown("**POC Vision**: Autonomous incident prevention")

    if predictions and predictions.get('predictions'):
        server_preds = predictions['predictions']

        st.markdown("""
        ### üéØ Remediation Actions (Would Be Triggered)

        This tab shows what auto-remediation actions **would be triggered** in a production environment.
        """)

        # Analyze servers and determine remediation actions
        remediation_plan = []

        for server_name, server_pred in server_preds.items():
            risk_score = calculate_server_risk_score(server_pred)

            # Determine remediation based on profile and risk
            if risk_score >= 70:
                # Infer profile from server name
                profile = 'unknown'
                if server_name.startswith('ppml'):
                    profile = 'ml_compute'
                elif server_name.startswith('ppdb'):
                    profile = 'database'
                elif server_name.startswith('ppweb'):
                    profile = 'web_api'
                elif server_name.startswith('pprisk'):
                    profile = 'risk_analytics'
                elif server_name.startswith('ppetl'):
                    profile = 'data_ingest'
                elif server_name.startswith('ppcon'):
                    profile = 'conductor_mgmt'
                else:
                    profile = 'generic'

                # Get predicted CPU
                cpu = server_pred.get('cpu_percent', {})
                p90_cpu = cpu.get('p90', [])
                max_predicted_cpu = max(p90_cpu[:6]) if len(p90_cpu) >= 6 else 0

                # Determine action based on profile
                if profile == 'ml_compute':
                    action = "üîß Scale up compute resources (+2 vCPUs)"
                    integration = "Spectrum Conductor API: POST /resources/scale"
                    eta = "2 minutes"
                elif profile == 'database':
                    action = "üíæ Enable connection pooling, scale read replicas"
                    integration = "Database Management API"
                    eta = "5 minutes"
                elif profile == 'web_api':
                    action = "üåê Scale out (+2 instances), enable rate limiting"
                    integration = "Load Balancer API + Kubernetes HPA"
                    eta = "3 minutes"
                elif profile == 'risk_analytics':
                    action = "üìä Queue batch jobs, scale compute resources"
                    integration = "Job Scheduler API"
                    eta = "4 minutes"
                else:
                    action = "‚öôÔ∏è Alert on-call team for manual review"
                    integration = "PagerDuty API"
                    eta = "Immediate"

                remediation_plan.append({
                    'Server': server_name,
                    'Profile': profile.replace('_', ' ').title(),
                    'Risk Score': risk_score,
                    'Predicted CPU (p90)': f"{max_predicted_cpu:.1f}%",
                    'Auto-Remediation': action,
                    'Integration Point': integration,
                    'ETA to Remediate': eta,
                    'Status': 'üî¥ Would Trigger Now'
                })

        if remediation_plan:
            st.markdown(f"### üö® {len(remediation_plan)} Auto-Remediations Would Be Triggered")

            df_remediation = pd.DataFrame(remediation_plan)
            st.dataframe(df_remediation, width='stretch', hide_index=True)

            st.divider()

            # Summary stats
            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("Actions Queued", len(remediation_plan))

            with col2:
                auto_actions = len([r for r in remediation_plan if 'Scale' in r['Auto-Remediation'] or 'Enable' in r['Auto-Remediation']])
                st.metric("Autonomous Actions", auto_actions)

            with col3:
                manual_actions = len(remediation_plan) - auto_actions
                st.metric("Manual Review Required", manual_actions)

        else:
            st.success("‚úÖ No auto-remediation actions required - Fleet is healthy!")

        st.divider()

        # Integration Architecture
        st.markdown("### üèóÔ∏è Integration Architecture")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **Phase 1: Auto-Scaling (Weeks 1-2)**
            - ‚úÖ Spectrum Conductor API integration
            - ‚úÖ Kubernetes HPA triggers
            - ‚úÖ Database connection pool tuning
            - ‚úÖ Load balancer configuration

            **Expected Outcome**: 85% of incidents auto-remediated
            """)

        with col2:
            st.markdown("""
            **Phase 2: Advanced Actions (Weeks 3-4)**
            - üîÑ Job rescheduling (batch workloads)
            - üîÑ Traffic rerouting (degraded services)
            - üîÑ Cache warming (predicted load spikes)
            - üîÑ Proactive restarts (memory leaks)

            **Expected Outcome**: 95% incident prevention rate
            """)

        st.divider()

        # Approval Workflow
        st.markdown("### ‚úÖ Approval Workflow (Configurable)")

        st.markdown("""
        **Safety Controls** (Production Implementation):

        1. **Low Risk (Score < 50)**: Auto-approve, log only
        2. **Medium Risk (50-70)**: Auto-approve with 30-second delay, allow manual override
        3. **High Risk (70-85)**: Require senior engineer approval
        4. **Critical Risk (>85)**: Require director-level approval OR auto-approve during off-hours

        **Rollback Strategy**:
        - All actions are reversible within 15 minutes
        - Automatic rollback if metrics don't improve within 10 minutes
        - Manual override always available via dashboard or CLI
        """)

        st.divider()

        # Implementation Note
        st.info("""
        **üí° POC Implementation Note:**

        This tab demonstrates autonomous remediation capabilities. The full implementation would include:

        - **API integrations** with Spectrum Conductor, Kubernetes, load balancers, databases
        - **Approval workflows** with configurable risk thresholds and escalation paths
        - **Audit logging** of all automated actions for compliance
        - **Success metrics** tracking remediation effectiveness
        - **Rollback mechanisms** for automated actions that don't improve metrics
        - **Human override** capabilities at any point in the workflow

        **Impact**: Reduces MTTR (Mean Time To Resolution) from hours to minutes, achieving 95%+ incident prevention rate.
        """)

    else:
        st.info("Connect to daemon to see auto-remediation strategies")

# =============================================================================
# TAB 7: ALERTING STRATEGY
# =============================================================================

with tab7:
    st.subheader("üì± Alerting & Notification Strategy")
    st.markdown("**POC Vision**: Intelligent alert routing and escalation")

    if predictions and predictions.get('predictions'):
        server_preds = predictions['predictions']
        env = predictions.get('environment', {})

        st.markdown("""
        ### üéØ Alert Routing (Would Be Sent)

        This tab shows what alerts **would be sent** based on current predictions.
        """)

        # Determine alert levels
        prob_30m = env.get('prob_30m', 0)
        prob_8h = env.get('prob_8h', 0)

        # Collect alerts to send
        alerts_to_send = []

        # Environment-level alerts
        if prob_30m > 0.7:
            alerts_to_send.append({
                'Severity': 'üî¥ Critical',
                'Type': 'Environment',
                'Message': f'CRITICAL: Environment incident probability 30m = {prob_30m*100:.1f}%',
                'Recipients': 'On-Call Engineer (PagerDuty)',
                'Delivery Method': 'üìû Phone Call + SMS + App Push',
                'Action Required': 'Immediate investigation and response',
                'Escalation': '15 min ‚Üí Senior Engineer ‚Üí 30 min ‚Üí Director'
            })
        elif prob_30m > 0.4:
            alerts_to_send.append({
                'Severity': 'üü† Danger',
                'Type': 'Environment',
                'Message': f'DANGER: Environment degrading, incident probability 30m = {prob_30m*100:.1f}%',
                'Recipients': 'Engineering Team (Email + Slack)',
                'Delivery Method': 'üìß Email + üí¨ Slack #ops-alerts',
                'Action Required': 'Monitor closely, prepare for potential escalation',
                'Escalation': '30 min ‚Üí On-Call Engineer (PagerDuty)'
            })
        elif prob_8h > 0.5:
            alerts_to_send.append({
                'Severity': 'üü° Warning',
                'Type': 'Environment',
                'Message': f'WARNING: Elevated risk over 8 hours, probability = {prob_8h*100:.1f}%',
                'Recipients': 'Engineering Team (Email)',
                'Delivery Method': 'üìß Email to ops-team@company.com',
                'Action Required': 'Review dashboard, plan capacity if needed',
                'Escalation': 'None (informational)'
            })

        # Per-server alerts
        for server_name, server_pred in server_preds.items():
            risk_score = calculate_server_risk_score(server_pred)

            if risk_score >= 70:
                # Get profile
                profile = 'unknown'
                if server_name.startswith('ppml'):
                    profile = 'ML Compute'
                elif server_name.startswith('ppdb'):
                    profile = 'Database'
                elif server_name.startswith('ppweb'):
                    profile = 'Web/API'
                elif server_name.startswith('pprisk'):
                    profile = 'Risk Analytics'
                elif server_name.startswith('ppetl'):
                    profile = 'Data Ingest'
                else:
                    profile = 'Generic'

                # Determine severity based on new graduated scale
                if risk_score >= 90:
                    severity = 'üî¥ Imminent Failure'
                    recipients = 'On-Call Engineer (PagerDuty)'
                    delivery = 'üìû Phone + SMS + App'
                    escalation = '5 min ‚Üí CTO'
                elif risk_score >= 80:
                    severity = 'üî¥ Critical'
                    recipients = 'On-Call Engineer (PagerDuty)'
                    delivery = 'üìû Phone + SMS + App'
                    escalation = '15 min ‚Üí Senior ‚Üí 30 min ‚Üí Director'
                else:  # risk_score >= 70
                    severity = 'üü† Danger'
                    recipients = 'Server Team Lead (Slack)'
                    delivery = 'üí¨ Slack + Email'
                    escalation = '30 min ‚Üí On-Call'

                alerts_to_send.append({
                    'Severity': severity,
                    'Type': f'Server ({profile})',
                    'Message': f'{server_name}: Critical resource exhaustion predicted (Risk: {risk_score:.0f}/100)',
                    'Recipients': recipients,
                    'Delivery Method': delivery,
                    'Action Required': 'Check server health, trigger auto-remediation if available',
                    'Escalation': escalation
                })

        if alerts_to_send:
            st.markdown(f"### üîî {len(alerts_to_send)} Alerts Would Be Sent")

            df_alerts = pd.DataFrame(alerts_to_send)
            st.dataframe(df_alerts, width='stretch', hide_index=True)

            st.divider()

            # Alert summary by severity
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                imminent_count = len([a for a in alerts_to_send if 'Imminent Failure' in a['Severity']])
                st.metric("üî¥ Imminent Failure", imminent_count)

            with col2:
                critical_count = len([a for a in alerts_to_send if 'Critical' in a['Severity'] and 'Imminent' not in a['Severity']])
                st.metric("üî¥ Critical", critical_count)

            with col3:
                danger_count = len([a for a in alerts_to_send if 'Danger' in a['Severity']])
                st.metric("üü† Danger", danger_count)

            with col4:
                warning_count = len([a for a in alerts_to_send if 'Warning' in a['Severity']])
                st.metric("üü° Warning", warning_count)

        else:
            st.success("‚úÖ No alerts required - All systems healthy!")

        st.divider()

        # Alert Routing Matrix
        st.markdown("### üìã Alert Routing Matrix")

        routing_matrix = pd.DataFrame({
            'Severity': ['üî¥ Imminent Failure', 'üî¥ Critical', 'üü† Danger', 'üü° Warning', 'üü¢ Degrading', 'üëÅÔ∏è Watch'],
            'Threshold': ['Risk ‚â• 90', 'Risk 80-89', 'Risk 70-79', 'Risk 60-69', 'Risk 50-59', 'Risk 30-49'],
            'Initial Contact': ['On-Call Engineer (PagerDuty)', 'On-Call Engineer (PagerDuty)', 'Server Team Lead (Slack)', 'Server Team (Slack)', 'Engineering Team (Email)', 'Dashboard Only'],
            'Methods': ['Phone + SMS + App', 'Phone + SMS + App', 'Slack + Email', 'Slack + Email', 'Email only', 'Log only'],
            'Response SLA': ['5 minutes', '15 minutes', '30 minutes', '1 hour', '2 hours', 'Best effort'],
            'Escalation Path': ['5m ‚Üí CTO', '15m ‚Üí Senior ‚Üí 30m ‚Üí Director', '30m ‚Üí On-Call', '1h ‚Üí Team Lead', 'None', 'None']
        })

        st.dataframe(routing_matrix, width='stretch', hide_index=True)

        st.divider()

        # Integration Details
        st.markdown("### üîå Integration Points")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            **Immediate Alerting (Phase 1)**:
            - ‚úÖ PagerDuty integration (API)
            - ‚úÖ Slack webhooks (#ops-alerts, #server-ops)
            - ‚úÖ Email (SMTP to distribution lists)
            - ‚úÖ Dashboard notifications

            **Delivery Time**: < 30 seconds from prediction
            """)

        with col2:
            st.markdown("""
            **Advanced Features (Phase 2)**:
            - üîÑ SMS notifications (Twilio)
            - üîÑ Microsoft Teams integration
            - üîÑ ServiceNow ticket creation
            - üîÑ Mobile app push notifications

            **Intelligent Routing**: Context-aware escalation
            """)

        st.divider()

        # Alert Suppression
        st.markdown("### üîá Intelligent Alert Suppression")

        st.markdown("""
        **Smart Features** (Reduce Alert Fatigue):

        1. **Deduplication**: Same server, same issue ‚Üí single alert (no flooding)
        2. **Grouping**: Multiple servers in same profile degrading ‚Üí grouped alert
        3. **Scheduled Maintenance**: Suppress alerts during maintenance windows
        4. **Auto-Remediation Active**: Suppress alerts if auto-fix is already running
        5. **Escalation Delays**: Progressive escalation only if no acknowledgment

        **Result**: 80% reduction in alert noise, 95% increase in signal-to-noise ratio
        """)

        st.divider()

        # Implementation Note
        st.info("""
        **üí° POC Implementation Note:**

        This tab demonstrates intelligent alerting capabilities. Production implementation would include:

        - **Multi-channel integration**: PagerDuty, Slack, Email, SMS, Teams, ServiceNow
        - **Context-aware routing**: Profile-based escalation (e.g., DB issues go to DBA team)
        - **Alert lifecycle tracking**: From trigger ‚Üí acknowledgment ‚Üí resolution
        - **On-call schedule integration**: Route to current on-call engineer automatically
        - **Alert suppression rules**: Prevent alert storms during cascading incidents
        - **Feedback loop**: Track alert accuracy, adjust thresholds based on false positive rate

        **Impact**: Reduces alert fatigue by 80%, improves response time by 60%, ensures critical issues never go unnoticed.
        """)

    else:
        st.info("Connect to daemon to see alerting strategy")

# =============================================================================
# TAB 8: ADVANCED
# =============================================================================

with tab8:
    st.subheader("‚öôÔ∏è Advanced Settings & Diagnostics")

    # System Info
    st.markdown("### System Information")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**Dashboard**")
        st.code(f"""
Python Version: 3.10+
Streamlit Version: {st.__version__}
Session Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
History Size: {len(st.session_state.history)} entries
        """)

    with col2:
        st.markdown("**Daemon**")
        if st.session_state.daemon_connected:
            health = client.check_health()
            health_data = health.get('data', {})
            st.code(f"""
URL: {daemon_url}
Status: Connected ‚úÖ
Model Loaded: {health_data.get('model_loaded', 'Unknown')}
Uptime: {health_data.get('uptime', 'Unknown')}
            """)
        else:
            st.code(f"""
URL: {daemon_url}
Status: Not Connected ‚ùå
            """)

    st.divider()

    # Alert Thresholds
    st.markdown("### Alert Thresholds")

    st.info("üöß Alert threshold configuration coming soon")

    col1, col2 = st.columns(2)

    with col1:
        st.number_input("CPU Warning Threshold (%)", value=70, min_value=0, max_value=100)
        st.number_input("Memory Warning Threshold (%)", value=80, min_value=0, max_value=100)

    with col2:
        st.number_input("CPU Critical Threshold (%)", value=90, min_value=0, max_value=100)
        st.number_input("Memory Critical Threshold (%)", value=95, min_value=0, max_value=100)

    st.divider()

    # Debug Info
    with st.expander("üîç Debug Information"):
        st.markdown("**Session State**")
        st.json({
            'daemon_url': st.session_state.daemon_url,
            'daemon_connected': st.session_state.daemon_connected,
            'refresh_interval': st.session_state.refresh_interval,
            'history_size': len(st.session_state.history),
            'last_update': str(st.session_state.last_update)
        })

        if predictions:
            st.markdown("**Latest Predictions (Raw)**")
            st.json(predictions)

# =============================================================================
# TAB 9: DOCUMENTATION
# =============================================================================

with tab9:
    st.subheader("üìö Dashboard Documentation")
    st.markdown("**Complete guide to understanding and using the TFT Monitoring Dashboard**")

    # Table of Contents
    st.markdown("### üìñ Table of Contents")
    st.markdown("""
    1. [Overview & Features](#overview-features)
    2. [Understanding Risk Scores](#understanding-risk-scores)
    3. [Alert Priority Levels](#alert-priority-levels)
    4. [Contextual Intelligence](#contextual-intelligence)
    5. [Server Profiles](#server-profiles)
    6. [How to Interpret Alerts](#how-to-interpret-alerts)
    7. [Environment Status](#environment-status)
    8. [Trend Analysis](#trend-analysis)
    """)

    st.divider()

    # Section 1: Overview & Features
    st.markdown("### üéØ Overview & Features")
    st.markdown("""
    **TFT Monitoring Dashboard** is a predictive monitoring system that uses deep learning (Temporal Fusion Transformer)
    to forecast server health 30 minutes to 8 hours in advance.

    **Key Capabilities**:
    - **Real-time Monitoring**: Live metrics from 20+ servers across 7 profiles
    - **30-Minute Predictions**: AI forecasts CPU, Memory, Latency with 85-90% accuracy
    - **8-Hour Horizon**: Extended forecasts for capacity planning
    - **Contextual Intelligence**: Risk scoring considers server profiles, trends, and multi-metric correlations
    - **Graduated Alerts**: 7 severity levels from Healthy to Imminent Failure
    - **Early Warning**: 15-60 minute advance notice before problems become critical

    **Technology Stack**:
    - **Model**: PyTorch Forecasting Temporal Fusion Transformer (TFT)
    - **Architecture**: Microservices with REST/WebSocket APIs
    - **Dashboard**: Streamlit with real-time updates
    - **Training**: Transfer learning with profile-specific fine-tuning
    """)

    st.divider()

    # Section 2: Understanding Risk Scores
    st.markdown("### üìä Understanding Risk Scores")
    st.markdown("""
    Every server receives a **Risk Score (0-100)** that represents overall health and predicted trajectory.

    **Score Composition**:
    ```
    Final Risk = (Current State √ó 70%) + (Predictions √ó 30%)
    ```

    **Why 70/30 Weighting?**
    - **70% Current State**: Executives care about "what's on fire NOW"
    - **30% Predictions**: Early warning value without crying wolf

    **Risk Components**:
    - **CPU Usage**: Current and predicted utilization
    - **Memory Usage**: Current and predicted with profile-specific thresholds
    - **Latency**: Response time degradation
    - **Disk Usage**: Available space warnings
    - **Trend Velocity**: Rate of change (climbing vs. steady)
    - **Multi-Metric Correlation**: Compound stress detection
    """)

    # Risk Score Examples
    st.markdown("#### üî¢ Risk Score Examples")

    examples_df = pd.DataFrame({
        'Scenario': [
            'Normal Operations',
            'Steady High Load',
            'Degrading Performance',
            'Predicted Spike',
            'Compound Stress',
            'Imminent Failure'
        ],
        'CPU': ['25%', '70%', '40% ‚Üí 75%', '35% ‚Üí 95%', '85%', '99%'],
        'Memory': ['35%', '65%', '60% ‚Üí 80%', '50%', '90%', '99%'],
        'Latency': ['40ms', '80ms', '90ms ‚Üí 150ms', '60ms', '320ms', '1200ms'],
        'Risk Score': [8, 32, 58, 52, 83, 96],
        'Status': ['Healthy ‚úÖ', 'Watch üëÅÔ∏è', 'Degrading üü¢', 'Degrading üü¢', 'Critical üî¥', 'Imminent Failure üî¥']
    })

    st.dataframe(examples_df, hide_index=True, width='stretch')

    st.divider()

    # Section 3: Alert Priority Levels
    st.markdown("### üö® Alert Priority Levels")
    st.markdown("""
    The dashboard uses **7 graduated severity levels** instead of binary OK/CRITICAL alerts.
    This provides nuanced triage and graduated escalation.
    """)

    priority_df = pd.DataFrame({
        'Level': ['üî¥ Imminent Failure', 'üî¥ Critical', 'üü† Danger', 'üü° Warning', 'üü¢ Degrading', 'üëÅÔ∏è Watch', '‚úÖ Healthy'],
        'Risk Score': ['90-100', '80-89', '70-79', '60-69', '50-59', '30-49', '0-29'],
        'Meaning': [
            'Server about to crash or failing NOW',
            'Severe issues requiring immediate attention',
            'High-priority problems requiring urgent action',
            'Concerning trends that need monitoring',
            'Performance declining, investigate soon',
            'Low concern, background monitoring only',
            'Normal operations, no concerns'
        ],
        'Response': [
            'Drop everything, CTO escalation',
            'Page on-call engineer immediately',
            'Team lead engaged, urgent response',
            'Team awareness, monitor closely',
            'Email notification, investigate',
            'Dashboard only, no action needed',
            'No alerts generated'
        ],
        'SLA': ['5 minutes', '15 minutes', '30 minutes', '1 hour', '2 hours', 'Best effort', 'N/A']
    })

    st.dataframe(priority_df, hide_index=True, width='stretch')

    st.markdown("""
    **Key Insight**: Notice how the system provides graduated escalation. You don't go from "Healthy" to "Critical" -
    instead you progress through Watch ‚Üí Degrading ‚Üí Warning ‚Üí Danger, giving teams time to respond proactively.
    """)

    st.divider()

    # Section 4: Contextual Intelligence
    st.markdown("### üß† Contextual Intelligence: Beyond Simple Thresholds")
    st.markdown("""
    **Philosophy**: "40% CPU may be fine, or may be degrading - depends on context"

    Traditional monitoring uses **binary thresholds**:
    ```python
    if cpu > 80%:
        alert = "CRITICAL"  # Everything is suddenly on fire!
    else:
        alert = "OK"  # Everything is fine!
    ```

    **Problems**:
    - ‚ùå No context: 80% CPU on database = normal, 80% on web server = problem
    - ‚ùå No trends: 80% steady = fine, 40% ‚Üí 80% in 10 min = concerning
    - ‚ùå No prediction: Server at 60% but climbing fast will crash soon
    - ‚ùå Binary state: Everything is either OK or ON FIRE (no middle ground)
    - ‚ùå Ignores correlations: High CPU + high memory + high latency = compound risk

    **Our Approach**: Contextual intelligence using **fuzzy logic**
    """)

    # Contextual Factors
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### üéØ Context Factor 1: Server Profile")
        st.markdown("""
        **Same metric, different meaning**:

        **Database Server (ppdb001)**:
        - Memory: 98% = ‚úÖ **Healthy** (page cache is normal)
        - Risk Score: 8

        **ML Compute (ppml0001)**:
        - Memory: 98% = üî¥ **Critical** (OOM kill imminent)
        - Risk Score: 82

        The system understands that databases use 100% memory for
        caching (expected), while compute servers need headroom
        for allocations.
        """)

        st.markdown("#### üìà Context Factor 2: Trend Analysis")
        st.markdown("""
        **Same current value, different trajectory**:

        **Steady State**:
        - CPU: 40% for last 30 minutes
        - Risk: 0 (stable workload)

        **Rapid Climb**:
        - CPU: 20% ‚Üí 40% ‚Üí 60% (climbing 20%/10min)
        - Risk: 56 (will hit 100% in 20 minutes!)

        The system detects velocity and acceleration patterns.
        """)

    with col2:
        st.markdown("#### üîó Context Factor 3: Multi-Metric Correlation")
        st.markdown("""
        **Isolated spike vs. compound stress**:

        **Isolated CPU Spike**:
        - CPU: 85% (batch job)
        - Memory: 35%
        - Latency: 40ms
        - Risk: 28 (‚úÖ Healthy - just a batch job)

        **Compound Stress**:
        - CPU: 85% (same value!)
        - Memory: 90%
        - Latency: 350ms
        - Risk: 83 (üî¥ Critical - system under stress)
        """)

        st.markdown("#### üîÆ Context Factor 4: Prediction-Aware")
        st.markdown("""
        **Current vs. predicted state**:

        **Looks Fine Now, But...**:
        - Current CPU: 40%
        - Predicted (30m): 95%
        - Risk: 52 (üü¢ Degrading - early warning!)

        **Bad Now, Getting Better**:
        - Current CPU: 85%
        - Predicted (30m): 60%
        - Risk: 38 (üëÅÔ∏è Watch - resolving itself)
        """)

    st.markdown("""
    #### üéØ Result: Intelligent Risk Assessment

    The system combines all four context factors to produce a risk score that reflects **actual operational risk**,
    not just raw metric values. This eliminates false positives while providing earlier detection of real problems.
    """)

    st.divider()

    # Section 5: Server Profiles
    st.markdown("### üñ•Ô∏è Server Profiles")
    st.markdown("""
    The system automatically detects server profiles from hostnames and applies **profile-specific intelligence**.
    """)

    profiles_df = pd.DataFrame({
        'Profile': ['ML Compute', 'Database', 'Web API', 'Conductor Mgmt', 'Data Ingest', 'Risk Analytics', 'Generic'],
        'Hostname Pattern': ['ppml####', 'ppdb###', 'ppweb###', 'ppcon##', 'ppdi###', 'ppra###', 'ppsrv###'],
        'Characteristics': [
            'High CPU/Mem during training, Memory-intensive',
            'High memory (page cache), Query CPU spikes',
            'Low memory (stateless), Latency-sensitive',
            'Low CPU/Mem, Management workload',
            'High disk I/O, Network-intensive',
            'CPU-intensive analytics, Batch processing',
            'Balanced workload'
        ],
        'Memory Threshold': ['98%', '100%', '85%', '90%', '90%', '95%', '90%'],
        'Key Metrics': [
            'CPU, Memory allocation',
            'Query latency, Memory cache',
            'Request latency, Error rate',
            'Process health',
            'Disk I/O, Network throughput',
            'CPU usage, GC pauses',
            'Balanced monitoring'
        ]
    })

    st.dataframe(profiles_df, hide_index=True, width='stretch')

    st.info("""
    **Why Profile Awareness Matters**: A database at 100% memory is healthy (caching), but a web server at 100% memory
    is about to crash (memory leak). The system adjusts thresholds and risk calculations based on expected behavior patterns.
    """)

    st.divider()

    # Section 6: How to Interpret Alerts
    st.markdown("### üîî How to Interpret Alerts")
    st.markdown("""
    The **Active Alerts** table shows servers requiring attention (Risk ‚â• 50). Here's how to read it:
    """)

    st.markdown("#### üìã Alert Table Columns Explained")

    alert_columns_df = pd.DataFrame({
        'Column': ['Priority', 'Server', 'Profile', 'Risk', 'CPU Now', 'CPU Predicted (30m)', 'CPU Œî', 'Mem Now', 'Mem Predicted (30m)', 'Mem Œî', 'Latency Now', 'Latency Predicted (30m)', 'Latency Œî'],
        'Meaning': [
            'Severity level (Imminent Failure ‚Üí Critical ‚Üí Danger ‚Üí Warning ‚Üí Degrading)',
            'Server hostname (hover for details)',
            'Detected server workload type',
            'Overall risk score (0-100, higher = more urgent)',
            'Current CPU utilization percentage',
            'AI-predicted CPU in next 30 minutes',
            'Predicted change: + = increasing (degrading), - = decreasing (improving)',
            'Current memory utilization percentage',
            'AI-predicted memory in next 30 minutes',
            'Predicted change: + = increasing, - = decreasing',
            'Current system latency in milliseconds',
            'AI-predicted latency in next 30 minutes',
            'Predicted change: + = increasing, - = decreasing'
        ]
    })

    st.dataframe(alert_columns_df, hide_index=True, width='stretch')

    st.markdown("#### üéØ Priority Triage Strategy")

    st.markdown("""
    **Step 1: Check Critical+ Servers First**
    - Risk 90+ (Imminent Failure): Drop everything, server about to crash
    - Risk 80-89 (Critical): Immediate action, page on-call if after hours

    **Step 2: Review Danger/Warning Servers**
    - Risk 70-79 (Danger): High priority, team lead should investigate
    - Risk 60-69 (Warning): Monitor closely, team awareness

    **Step 3: Track Degrading Servers**
    - Risk 50-59 (Degrading): Early warnings, investigate during business hours

    **Step 4: Look for Patterns**
    - Multiple servers with same profile degrading? (Shared infrastructure issue)
    - All servers in datacenter showing latency? (Network problem)
    - Single server with multiple metrics elevated? (Compound stress)
    """)

    st.markdown("#### üìà Understanding Delta (Œî) Values")

    st.info("""
    **Delta values show predicted CHANGE**, not absolute values:

    - **CPU Œî: +15.2%** ‚Üí CPU will increase by 15.2% in next 30 minutes
    - **Mem Œî: -5.3%** ‚Üí Memory will decrease by 5.3% (improving)
    - **Latency Œî: +85ms** ‚Üí Latency will increase by 85ms (degrading)

    **üö® Red Flag Pattern**: All deltas positive (+) = server degrading across all metrics
    **‚úÖ Good Pattern**: All deltas negative (-) = server recovering across all metrics
    **‚ö†Ô∏è Mixed Pattern**: Some + some - = investigate further
    """)

    st.divider()

    # Section 7: Environment Status
    st.markdown("### üåç Environment Status")
    st.markdown("""
    The **Environment Status** indicator (top-left of Overview tab) shows fleet-wide health at a glance.
    """)

    env_status_df = pd.DataFrame({
        'Status': ['üî¥ Critical', 'üü† Warning', 'üü° Caution', 'üü¢ Healthy'],
        'Conditions': [
            '>30% of fleet Critical+ (Risk 80+) OR >50% elevated risk',
            '>10% of fleet Critical+ OR >30% elevated risk',
            '>10% of fleet Degrading (Risk 50+)',
            '<10% of fleet has elevated risk'
        ],
        'Interpretation': [
            'MAJOR INCIDENT: Multiple servers failing, immediate executive attention',
            'ELEVATED CONCERN: Significant portion of fleet affected, team mobilization',
            'EARLY WARNING: Some servers showing degradation, proactive investigation',
            'NORMAL OPERATIONS: Fleet healthy, routine monitoring'
        ],
        'Typical Action': [
            'War room, incident commander, all-hands response',
            'Team standup, resource allocation, incident tracking',
            'Email notifications, team awareness, monitoring',
            'No action required, continue normal operations'
        ]
    })

    st.dataframe(env_status_df, hide_index=True, width='stretch')

    st.markdown("""
    **Example Scenario**: You have 20 servers
    - 2 servers at Risk 85 (Critical)
    - 3 servers at Risk 72 (Danger)
    - 15 servers at Risk 20 (Healthy)

    **Calculation**:
    - Critical% = 2/20 = 10%
    - Elevated% = 5/20 = 25%

    **Status**: üü† **Warning** (10% critical, 25% elevated)
    **Action**: Team mobilization, incident tracking
    """)

    st.divider()

    # Section 8: Trend Analysis
    st.markdown("### üìä Trend Analysis")
    st.markdown("""
    Below the alert summary metrics, the **Trend Analysis** section shows movement patterns:
    """)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### ‚¨ÜÔ∏è Degrading Trends")
        st.markdown("""
        **Definition**: Servers with positive (+) delta values for CPU or Memory

        **What it means**:
        - Metrics increasing over next 30 minutes
        - Performance declining
        - Requires attention

        **Example**:
        - Alert table shows 12 servers
        - 8 have positive CPU Œî or Mem Œî
        - Display: "‚¨ÜÔ∏è Degrading: 8/12 (67%)"

        **Interpretation**: Most alerts are degrading situations (not recovering)
        """)

    with col2:
        st.markdown("#### ‚¨áÔ∏è Improving Trends")
        st.markdown("""
        **Definition**: Servers with negative (-) delta values for CPU or Memory

        **What it means**:
        - Metrics decreasing over next 30 minutes
        - Performance improving
        - Problems resolving themselves

        **Example**:
        - Alert table shows 12 servers
        - 4 have negative CPU Œî or Mem Œî
        - Display: "‚¨áÔ∏è Improving: 4/12 (33%)"

        **Interpretation**: Some servers recovering (maybe remediation already applied)
        """)

    st.warning("""
    **Important**: Trend percentages are calculated from **alerts only**, not total fleet.

    - If you have 12 alerts and 8 degrading ‚Üí "8/12" NOT "8/20"
    - This shows what proportion of your active problems are getting worse vs. better
    """)

    st.divider()

    # Best Practices
    st.markdown("### ‚úÖ Best Practices")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### üëç Do's")
        st.markdown("""
        - ‚úÖ **Check dashboard every 15-30 minutes** during business hours
        - ‚úÖ **Trust the risk scores** - they include context you might miss
        - ‚úÖ **Act on Degrading alerts proactively** before they become Critical
        - ‚úÖ **Look for patterns** across multiple servers
        - ‚úÖ **Use predictions** to plan maintenance windows
        - ‚úÖ **Correlate with deployments** - did we just push code?
        - ‚úÖ **Review Watch servers** periodically (Risk 30-49)
        - ‚úÖ **Trust profile-specific thresholds** (DB at 100% mem = OK)
        """)

    with col2:
        st.markdown("#### üëé Don'ts")
        st.markdown("""
        - ‚ùå **Don't ignore Degrading alerts** thinking "it's only 55% CPU"
        - ‚ùå **Don't panic at single metric spike** - look at overall risk score
        - ‚ùå **Don't override profile thresholds** without understanding context
        - ‚ùå **Don't dismiss predictions** as "just guesses"
        - ‚ùå **Don't create manual alerts** that duplicate dashboard intelligence
        - ‚ùå **Don't compare this to traditional monitoring** - it's predictive
        - ‚ùå **Don't ignore improving trends** - verify remediation worked
        """)

    st.divider()

    # Quick Reference
    st.markdown("### üöÄ Quick Reference Card")

    st.code("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         TFT MONITORING DASHBOARD - QUICK REFERENCE            ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë RISK SCORE FORMULA:                                           ‚ïë
‚ïë   Final Risk = (Current State √ó 70%) + (Predictions √ó 30%)   ‚ïë
‚ïë                                                                ‚ïë
‚ïë PRIORITY LEVELS:                                              ‚ïë
‚ïë   üî¥ Imminent Failure (90+)  ‚Üí 5-min SLA, CTO escalation     ‚ïë
‚ïë   üî¥ Critical (80-89)        ‚Üí 15-min SLA, page on-call      ‚ïë
‚ïë   üü† Danger (70-79)          ‚Üí 30-min SLA, team lead         ‚ïë
‚ïë   üü° Warning (60-69)         ‚Üí 1-hour SLA, team awareness    ‚ïë
‚ïë   üü¢ Degrading (50-59)       ‚Üí 2-hour SLA, email only        ‚ïë
‚ïë   üëÅÔ∏è Watch (30-49)           ‚Üí Background monitoring         ‚ïë
‚ïë   ‚úÖ Healthy (0-29)          ‚Üí No alerts                      ‚ïë
‚ïë                                                                ‚ïë
‚ïë ENVIRONMENT STATUS:                                           ‚ïë
‚ïë   üî¥ Critical  ‚Üí >30% Critical+ OR >50% elevated             ‚ïë
‚ïë   üü† Warning   ‚Üí >10% Critical+ OR >30% elevated             ‚ïë
‚ïë   üü° Caution   ‚Üí >10% Degrading                              ‚ïë
‚ïë   üü¢ Healthy   ‚Üí <10% elevated risk                          ‚ïë
‚ïë                                                                ‚ïë
‚ïë DELTA INTERPRETATION:                                         ‚ïë
‚ïë   Positive (+) ‚Üí Metrics increasing (degrading)               ‚ïë
‚ïë   Negative (-) ‚Üí Metrics decreasing (improving)               ‚ïë
‚ïë                                                                ‚ïë
‚ïë PROFILE-SPECIFIC THRESHOLDS:                                 ‚ïë
‚ïë   Database: 100% memory = NORMAL (page cache)                ‚ïë
‚ïë   ML Compute: 98% memory = CRITICAL (OOM risk)               ‚ïë
‚ïë   Web API: Latency > 200ms = SEVERE (user impact)           ‚ïë
‚ïë                                                                ‚ïë
‚ïë RESPONSE PRIORITY:                                            ‚ïë
‚ïë   1. Imminent Failure ‚Üí Drop everything                       ‚ïë
‚ïë   2. Critical ‚Üí Immediate action                              ‚ïë
‚ïë   3. Danger ‚Üí Urgent response                                 ‚ïë
‚ïë   4. Warning ‚Üí Monitor closely                                ‚ïë
‚ïë   5. Degrading ‚Üí Investigate soon                             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """, language="text")

    st.divider()

    st.success("""
    **üìö Documentation Complete!**

    This guide covers the core concepts and operational procedures for the TFT Monitoring Dashboard.
    For technical implementation details, see the Advanced tab. For future enhancements, see the Roadmap tab.
    """)

# =============================================================================
# TAB 10: ROADMAP
# =============================================================================

with tab10:
    st.subheader("üó∫Ô∏è Future Roadmap")
    st.markdown("**POC Success ‚Üí Production Excellence**: Planned enhancements for world-class monitoring")

    st.info("""
    **Philosophy**: This demo is already impressive. These enhancements would make it a **market-leading predictive monitoring platform**
    that competes with Datadog, New Relic, and Dynatrace.
    """)

    # Phase Overview
    st.markdown("### üìÖ Implementation Phases")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Phase 1", "5 Features", help="Production Essentials (Months 1-3)")
    with col2:
        st.metric("Phase 2", "5 Features", help="Scale & Reliability (Months 4-6)")
    with col3:
        st.metric("Phase 3", "5 Features", help="Advanced Automation (Months 7-12)")
    with col4:
        st.metric("Phase 4", "6 Features", help="Polish & Differentiation (Year 2)")

    st.divider()

    # Phase 1: Production Essentials
    with st.expander("üöÄ **Phase 1: Production Essentials** (Next 3 Months)", expanded=True):
        st.markdown("""
        ### 1. Automated Retraining Pipeline ‚≠ê‚≠ê‚≠ê
        **Priority**: HIGH | **Effort**: 2-3 weeks | **Value**: Production-critical

        Automatically detect fleet changes and retrain model when needed.

        **Features**:
        - Fleet drift monitoring (new/sunset servers detected)
        - Unknown prediction rate tracking
        - Automatic dataset regeneration from live metrics
        - Scheduled retraining workflows (nightly/weekly)
        - Blue-green model deployment with validation
        - Automatic rollback on validation failure

        **Business Value**: Zero-touch model maintenance, always-accurate predictions, scales to 1000+ servers

        ---

        ### 2. Action Recommendation System ‚≠ê‚≠ê‚≠ê
        **Priority**: HIGH | **Effort**: 4-6 weeks | **Value**: Game-changer

        Context-aware recommendations for predicted issues.

        **Recommendation Types**:
        - **Immediate Actions** (0-2h): "Scale web tier +2 servers", "Restart hung process", "Clear cache"
        - **Short-Term Actions** (2-8h): "Schedule deployment rollback", "Increase connection pool"
        - **Long-Term Actions** (1-7 days): "Optimize slow query", "Add database index"
        - **Preventive Actions**: "Schedule maintenance before predicted spike"

        **Confidence Scoring**: Action effectiveness, risk level, time required, reversibility

        **Business Value**: Reduce decision paralysis, empower junior SAs, faster MTTR by 70%

        ---

        ### 3. Advanced Dashboard Intelligence ‚≠ê‚≠ê‚≠ê
        **Priority**: HIGH | **Effort**: 3-4 weeks | **Value**: Better UX

        **Smart Features**:
        - **Predictive Insights**: "3 servers predicted to degrade in next 8 hours - ppweb001 likely CPU bottleneck (89% confidence)"
        - **What-If Analysis**: "What if I scale up this server?" ‚Üí Show prediction changes
        - **Trend Analysis**: "CPU trending up 12% week-over-week", "Memory leak detected"
        - **Intelligent Sorting**: Auto-prioritize by risk, group by profile, filter by confidence
        - **Comparison View**: Server vs server, current vs predicted, different scenarios

        **Business Value**: Faster decisions, reduced cognitive load, proactive operations

        ---

        ### 4. Alerting Integration ‚≠ê‚≠ê‚≠ê
        **Priority**: HIGH | **Effort**: 1-2 weeks | **Value**: Essential

        **Integrations**:
        - PagerDuty (create incidents for high-confidence predictions)
        - Slack (send notifications to channels)
        - Microsoft Teams (adaptive cards)
        - Email (digest emails)
        - JIRA/ServiceNow (auto-create tickets)

        **Smart Alerting**: Only actionable predictions, confidence-based routing, time-to-impact urgency, deduplication

        **Business Value**: Integrate with existing workflows, reduce alert fatigue, right alert at right time

        ---

        ### 5. Explainable AI (XAI) ‚≠ê‚≠ê‚≠ê
        **Priority**: HIGH | **Effort**: 3-4 weeks | **Value**: Trust & transparency

        **Techniques**:
        - SHAP values (feature importance)
        - Attention weights (which timesteps matter most)
        - Counterfactual explanations ("if X was lower, prediction would change")

        **Example Output**:
        ```
        Prediction: ppweb001 CPU ‚Üí 92% in 6 hours

        Explanation:
        ‚≠ê‚≠ê‚≠ê Recent trend (last 4h): +15% CPU increase
        ‚≠ê‚≠ê Historical pattern: Morning spike approaching (8 AM in 6h)
        ‚≠ê‚≠ê Similar servers: ppweb002/003 also trending up
        ‚≠ê Deployment correlation: New release 2h ago
        ```

        **Business Value**: Build trust, debug model errors, regulatory compliance, educational for SAs
        """)

    # Phase 2: Scale & Reliability
    with st.expander("üìà **Phase 2: Scale & Reliability** (Months 4-6)"):
        st.markdown("""
        ### 6. Online Learning During Inference
        Model learns from recent data without full retraining. Adapt to seasonal patterns automatically.

        ### 7. Model Performance Monitoring
        Track accuracy over time, confidence calibration, false positive/negative rates. Identify degradation early.

        ### 8. Multi-Region / Multi-Cluster Support
        Region selector in dashboard, cross-region anomaly correlation, region-specific models.

        ### 9. Root Cause Analysis (RCA) Engine
        Automatically identify likely causes: correlation analysis, dependency analysis, historical pattern matching, change correlation.

        ### 10. Observability Platform Integration
        Integrate with Datadog, New Relic, Prometheus. Ingest metrics, export predictions, correlate with logs/traces.
        """)

    # Phase 3: Advanced Automation
    with st.expander("ü§ñ **Phase 3: Advanced Automation** (Months 7-12)"):
        st.markdown("""
        ### 11. Automated Environment Fixes
        Auto-scaling triggers, service restarts, load balancer adjustments, cache clearing, circuit breaker activation.

        **Safety**: Confidence thresholds, approval workflows, rollback capability, audit logging, rate limiting.

        ### 12. Automated Runbook Execution
        Execute common remediation actions automatically: restart service, clear cache, scale service, rollback deployment.

        ### 13. Transfer Learning for New Environments
        Use pre-trained model for new customers/environments. Deploy predictions day 1 vs weeks of training.

        ### 14. Multi-Metric Predictions
        Predict CPU, memory, disk, network, latency simultaneously. Detect correlation issues.

        ### 15. Infrastructure-as-Code Integration
        Trigger infrastructure changes: Terraform, Ansible, Kubernetes, AWS Auto Scaling, CloudFormation.
        """)

    # Phase 4: Polish & Differentiation
    with st.expander("‚ú® **Phase 4: Polish & Differentiation** (Year 2)"):
        st.markdown("""
        ### 16. Mobile Dashboard
        Responsive design, push notifications, quick actions, simplified view, dark mode for on-call.

        ### 17. Historical Trend Dashboard
        30/60/90-day trends, capacity forecasting, cost projection, seasonality detection, growth rate analysis.

        ### 18. A/B Testing for Model Updates
        Deploy new model to 10% of fleet, compare vs old model, measure accuracy delta, gradual rollout.

        ### 19. Cloud Cost Predictions
        Predict next month's bill, identify optimization opportunities, forecast cost impact of scaling.

        ### 20. Executive Dashboard
        High-level metrics: system health score, incidents prevented, cost savings, MTTD/MTTR, uptime %.

        ### 21. Anomaly Detection Beyond Predictions
        Isolation Forest, Autoencoders, statistical process control. Catch issues predictions might miss.
        """)

    st.divider()

    # Competitive Positioning
    st.markdown("### üéØ Competitive Positioning")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        **vs. Datadog / New Relic**:
        - ‚úÖ 8-hour prediction horizon (they only alert on current state)
        - ‚úÖ Interactive scenario simulation (they're read-only)
        - ‚úÖ Action recommendations (they just show metrics)
        - ‚úÖ Profile-based transfer learning (they treat all servers the same)
        """)

    with col2:
        st.markdown("""
        **vs. Dynatrace**:
        - ‚úÖ Transparent ML (we explain predictions, they're black box)
        - ‚úÖ Customizable thresholds (we adapt to your environment)
        - ‚úÖ Open architecture (not vendor lock-in)
        - ‚úÖ Faster time-to-value (weeks not years)
        """)

    st.divider()

    # Success Metrics
    st.markdown("### üìä Success Metrics")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        **Technical Metrics**:
        - Prediction accuracy > 85%
        - False positive rate < 10%
        - Inference latency < 2s
        - System uptime > 99.9%
        """)

    with col2:
        st.markdown("""
        **Business Metrics**:
        - Issues prevented/month
        - Cost savings (downtime + optimization)
        - Time saved for SAs
        - Faster MTTR
        """)

    with col3:
        st.markdown("""
        **Adoption Metrics**:
        - Daily active users
        - Predictions acted upon (%)
        - User satisfaction score
        - Feature usage rates
        """)

    st.divider()

    # Call to Action
    st.success("""
    ### üöÄ Next Steps

    This roadmap transforms an impressive demo into a **market-leading predictive monitoring platform**. The key is:

    1. ‚úÖ **Start with the demo** (already killer - you're seeing it now!)
    2. **Validate with real users** (get feedback from SAs, app owners, management)
    3. **Prioritize ruthlessly** (build what matters most based on user needs)
    4. **Ship iteratively** (release Phase 1 features one at a time, learn fast)

    **The interactive scenario system is your differentiator.** Everything else enhances that core value proposition:
    **predict issues before they happen, and tell people what to do about it**.
    """)

    st.info("""
    **üìÑ Full Roadmap Document**: See `Docs/FUTURE_ROADMAP.md` for complete technical details, effort estimates,
    implementation priorities, and business value analysis for all 21 planned features.
    """)

# =============================================================================
# AUTO-REFRESH (Simple approach - only rerun when interval elapsed)
# =============================================================================

if auto_refresh and st.session_state.daemon_connected:
    # Check if enough time has passed since last update
    current_time = datetime.now()

    # Initialize last_update if needed
    if st.session_state.last_update is None:
        st.session_state.last_update = current_time

    time_since_update = (current_time - st.session_state.last_update).total_seconds()

    # Only rerun if the refresh interval has actually elapsed
    if st.session_state.demo_running and time_since_update >= 5:
        time.sleep(5)  # Wait the full interval
        st.rerun()
    elif time_since_update >= refresh_interval:
        time.sleep(refresh_interval)  # Wait the full interval
        st.rerun()

# =============================================================================
# FOOTER
# =============================================================================

st.divider()
st.caption("üîÆ TFT Monitoring Dashboard | Built with Streamlit | Powered by Temporal Fusion Transformer")
