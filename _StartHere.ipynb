{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üöÄ Tachyon Argus - TFT Training Quick Start\n",
    "\n",
    "## Simplified workflow for dataset creation and model training\n",
    "\n",
    "This notebook does two things:\n",
    "1. **Generate training dataset** - Create realistic server metrics data with inter-server dependencies\n",
    "2. **Train TFT model** - Train the Temporal Fusion Transformer\n",
    "\n",
    "The dashboard and inference daemon handle everything else!\n",
    "\n",
    "---\n",
    "\n",
    "**‚è±Ô∏è Estimated Times:**\n",
    "- Dataset generation (24h): ~30-60 seconds\n",
    "- Dataset generation (720h): ~5-10 minutes\n",
    "- Model training (10 epochs): ~3-5 hours on RTX 4090\n",
    "\n",
    "**üéØ After Training:**\n",
    "- Start system: `start_all.bat` (Windows) or `./start_all.sh` (Linux/Mac)\n",
    "- Dashboard: http://localhost:8050\n",
    "- API: http://localhost:8000\n",
    "\n",
    "---\n",
    "\n",
    "**üìä Argus Metrics Framework:**\n",
    "- 15 server metrics including cascade_impact\n",
    "- 7 server profiles with inter-server dependencies\n",
    "- Cascade failure simulation (database ‚Üí dependent services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to Python path (works from either root or Argus directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'Argus':\n",
    "    # Notebook is in Argus folder\n",
    "    argus_src = (current_dir / 'src').absolute()\n",
    "    argus_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    argus_src = (current_dir / 'Argus' / 'src').absolute()\n",
    "    argus_root = current_dir / 'Argus'\n",
    "\n",
    "if str(argus_src) not in sys.path:\n",
    "    sys.path.insert(0, str(argus_src))\n",
    "\n",
    "print(\"üéØ Tachyon Argus - TFT Training System\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Python path configured\")\n",
    "print(f\"üìÅ Argus source: {argus_src}\")\n",
    "print(f\"üìÅ Argus root: {argus_root}\")\n",
    "print(\"\\nüîß Configuration:\")\n",
    "print(f\"   Training directory: {argus_root}/training/\")\n",
    "print(f\"   Models directory: {argus_root}/models/\")\n",
    "print(\"   Prediction horizon: 96 steps (8 hours)\")\n",
    "print(\"   Context length: 288 steps (24 hours)\")\n",
    "print(\"   Metrics: 15 Argus metrics (including cascade_impact)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grw8o6ndz6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## System Health Check\n",
    "\n",
    "Verify your environment is ready for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ntc378ebr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Comprehensive System Check\n",
    "# Verify GPU, Python environment, dependencies, and system readiness\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "# Setup paths (same as Cell 1)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'Argus':\n",
    "    argus_src = (current_dir / 'src').absolute()\n",
    "    argus_root = current_dir\n",
    "else:\n",
    "    argus_src = (current_dir / 'Argus' / 'src').absolute()\n",
    "    argus_root = current_dir / 'Argus'\n",
    "\n",
    "if str(argus_src) not in sys.path:\n",
    "    sys.path.insert(0, str(argus_src))\n",
    "\n",
    "print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "print(\"‚ïë\" + \" \" * 20 + \"SYSTEM HEALTH CHECK\" + \" \" * 29 + \"‚ïë\")\n",
    "print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PYTHON ENVIRONMENT\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ Python Environment \" + \"‚îÄ\" * 47 + \"‚îê\")\n",
    "print(f\"‚îÇ Python Version:     {platform.python_version():<46}‚îÇ\")\n",
    "print(f\"‚îÇ Platform:           {platform.system()} {platform.release():<36}‚îÇ\")\n",
    "print(f\"‚îÇ Architecture:       {platform.machine():<46}‚îÇ\")\n",
    "\n",
    "# Working directory - handle long paths gracefully\n",
    "cwd = str(Path.cwd())\n",
    "if len(cwd) <= 45:\n",
    "    print(f\"‚îÇ Working Directory:  {cwd:<46}‚îÇ\")\n",
    "else:\n",
    "    print(f\"‚îÇ Working Directory:                                          ‚îÇ\")\n",
    "    chunk_size = 60\n",
    "    for i in range(0, len(cwd), chunk_size):\n",
    "        chunk = cwd[i:i+chunk_size]\n",
    "        print(f\"‚îÇ   {chunk:<64}‚îÇ\")\n",
    "\n",
    "# Show Argus root detection\n",
    "argus_root_str = str(argus_root)\n",
    "if len(argus_root_str) <= 45:\n",
    "    print(f\"‚îÇ Argus Root:         {argus_root_str:<46}‚îÇ\")\n",
    "else:\n",
    "    print(f\"‚îÇ Argus Root:                                                 ‚îÇ\")\n",
    "    for i in range(0, len(argus_root_str), chunk_size):\n",
    "        chunk = argus_root_str[i:i+chunk_size]\n",
    "        print(f\"‚îÇ   {chunk:<64}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. GPU AVAILABILITY & PYTORCH CUDA CHECK\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ GPU Status \" + \"‚îÄ\" * 54 + \"‚îê\")\n",
    "\n",
    "gpu_available = False\n",
    "gpu_name = \"Not available\"\n",
    "gpu_memory = 0\n",
    "cuda_version = \"N/A\"\n",
    "torch_cuda_enabled = False\n",
    "pytorch_installed = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    pytorch_installed = True\n",
    "    torch_cuda_enabled = torch.cuda.is_available()\n",
    "    gpu_available = torch_cuda_enabled\n",
    "    \n",
    "    if torch_cuda_enabled:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        cuda_version = torch.version.cuda\n",
    "        \n",
    "        print(f\"‚îÇ ‚úÖ GPU Detected:     {gpu_name[:45]:<45}‚îÇ\")\n",
    "        print(f\"‚îÇ    CUDA Version:     {cuda_version:<46}‚îÇ\")\n",
    "        print(f\"‚îÇ    Memory:           {gpu_memory:.1f} GB{' ' * 42}‚îÇ\")\n",
    "        print(f\"‚îÇ    PyTorch CUDA:     Enabled{' ' * 40}‚îÇ\")\n",
    "        \n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n",
    "                                   '--format=csv,noheader,nounits'], \n",
    "                                  capture_output=True, text=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                gpu_util, mem_used, mem_total = result.stdout.strip().split(',')\n",
    "                print(f\"‚îÇ    Utilization:      {gpu_util.strip()}%{' ' * 43}‚îÇ\")\n",
    "                print(f\"‚îÇ    Memory Used:      {mem_used.strip()} MB / {mem_total.strip()} MB{' ' * 28}‚îÇ\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚îÇ ‚ö†Ô∏è  PyTorch installed but CUDA not enabled{' ' * 24}‚îÇ\")\n",
    "        print(f\"‚îÇ    PyTorch Version:  {torch.__version__:<46}‚îÇ\")\n",
    "        print(f\"‚îÇ    Training will use CPU (20-40x slower){' ' * 25}‚îÇ\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(f\"‚îÇ ‚ùå PyTorch not installed{' ' * 42}‚îÇ\")\n",
    "    print(f\"‚îÇ   pip install torch --index-url{' ' * 34}‚îÇ\")\n",
    "    print(f\"‚îÇ     https://download.pytorch.org/whl/cu121{' ' * 24}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CRITICAL DEPENDENCIES\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ Critical Dependencies \" + \"‚îÄ\" * 43 + \"‚îê\")\n",
    "\n",
    "dependencies = {\n",
    "    'torch': 'PyTorch (Deep Learning)',\n",
    "    'lightning': 'PyTorch Lightning (Training)',\n",
    "    'pandas': 'Pandas (Data Processing)',\n",
    "    'numpy': 'NumPy (Numerical Computing)',\n",
    "    'pytorch_forecasting': 'PyTorch Forecasting (TFT Model)',\n",
    "    'fastapi': 'FastAPI (Inference API)',\n",
    "    'plotly': 'Plotly (Dashboard)',\n",
    "    'dash': 'Dash (Dashboard Framework)'\n",
    "}\n",
    "\n",
    "missing_deps = []\n",
    "installed_deps = []\n",
    "\n",
    "for package, description in dependencies.items():\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is not None:\n",
    "        try:\n",
    "            module = importlib.import_module(package)\n",
    "            version = getattr(module, '__version__', 'unknown')\n",
    "            status = \"‚úÖ\"\n",
    "            installed_deps.append(package)\n",
    "            pkg_display = f\"{package} ({version})\"\n",
    "        except:\n",
    "            status = \"‚ö†Ô∏è\"\n",
    "            pkg_display = package\n",
    "    else:\n",
    "        status = \"‚ùå\"\n",
    "        missing_deps.append(package)\n",
    "        pkg_display = package\n",
    "    \n",
    "    print(f\"‚îÇ {status} {pkg_display:<63}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DIRECTORY STRUCTURE\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ Directory Structure \" + \"‚îÄ\" * 46 + \"‚îê\")\n",
    "\n",
    "required_dirs = {\n",
    "    'training': argus_root / 'training',\n",
    "    'models': argus_root / 'models',\n",
    "    'checkpoints': argus_root / 'checkpoints',\n",
    "    'logs': argus_root / 'logs'\n",
    "}\n",
    "\n",
    "for name, path in required_dirs.items():\n",
    "    exists = path.exists()\n",
    "    status = \"‚úÖ\" if exists else \"‚ö†Ô∏è\"\n",
    "    existence = \"exists\" if exists else \"will be created\"\n",
    "    print(f\"‚îÇ {status} {name + '/':20} {existence:<44}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. EXISTING MODELS CHECK\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ Existing Models \" + \"‚îÄ\" * 50 + \"‚îê\")\n",
    "\n",
    "models_dir = argus_root / 'models'\n",
    "if models_dir.exists():\n",
    "    model_dirs = sorted(models_dir.glob('tft_model_*'), reverse=True)\n",
    "    \n",
    "    if model_dirs:\n",
    "        print(f\"‚îÇ Found {len(model_dirs)} trained model(s):{' ' * 40}‚îÇ\")\n",
    "        for i, model_dir in enumerate(model_dirs[:3], 1):\n",
    "            model_name = model_dir.name\n",
    "            model_size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file()) / (1024**2)\n",
    "            print(f\"‚îÇ   {i}. {model_name:<40} ({model_size:>6.1f} MB) ‚îÇ\")\n",
    "        if len(model_dirs) > 3:\n",
    "            print(f\"‚îÇ   ... and {len(model_dirs) - 3} more{' ' * 44}‚îÇ\")\n",
    "    else:\n",
    "        print(f\"‚îÇ No trained models found - ready for first training{' ' * 16}‚îÇ\")\n",
    "else:\n",
    "    print(f\"‚îÇ Models directory will be created on first training{' ' * 16}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OVERALL READINESS\n",
    "# ============================================================================\n",
    "print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "\n",
    "all_critical_deps = all(dep in installed_deps for dep in ['torch', 'lightning', 'pandas', 'pytorch_forecasting'])\n",
    "\n",
    "if all_critical_deps and torch_cuda_enabled:\n",
    "    print(\"‚ïë\" + \" \" * 15 + \"‚úÖ SYSTEM READY FOR TRAINING\" + \" \" * 24 + \"‚ïë\")\n",
    "    gpu_short = gpu_name[:20] if 'gpu_name' in dir() else 'GPU'\n",
    "    print(f\"‚ïë\" + \" \" * 15 + f\"Estimated: 10 epochs ‚âà 3-5 hours on {gpu_short}\" + \" \" * max(0, 12 - len(gpu_short)) + \"‚ïë\")\n",
    "elif all_critical_deps and pytorch_installed and not torch_cuda_enabled:\n",
    "    print(\"‚ïë\" + \" \" * 10 + \"‚ö†Ô∏è  PYTORCH INSTALLED WITHOUT CUDA SUPPORT\" + \" \" * 16 + \"‚ïë\")\n",
    "    print(\"‚ïë\" + \" \" * 15 + \"Training will be 20-40x slower on CPU\" + \" \" * 15 + \"‚ïë\")\n",
    "else:\n",
    "    print(\"‚ïë\" + \" \" * 12 + \"‚ùå MISSING DEPENDENCIES - INSTALL FIRST\" + \" \" * 17 + \"‚ïë\")\n",
    "\n",
    "print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_production",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Generation\n",
    "\n",
    "Creates realistic server metrics with:\n",
    "- **7 server profiles** (ML, DB, Web, Conductor, ETL, Risk, Generic)\n",
    "- **15 Argus metrics** including cascade_impact\n",
    "- **Inter-server dependencies** (database failures cascade to dependent services)\n",
    "- **Financial market hours patterns**\n",
    "\n",
    "### Server Dependency Graph:\n",
    "```\n",
    "DATABASE (upstream)\n",
    "    ‚îú‚îÄ‚îÄ WEB_API (connection_failures, request_queuing)\n",
    "    ‚îú‚îÄ‚îÄ DATA_INGEST (connection_failures, request_queuing)  \n",
    "    ‚îú‚îÄ‚îÄ RISK_ANALYTICS (data_starvation, idle_resources)\n",
    "    ‚îî‚îÄ‚îÄ CONDUCTOR_MGMT (connection_failures)\n",
    "\n",
    "CONDUCTOR_MGMT (upstream)\n",
    "    ‚îî‚îÄ‚îÄ ML_COMPUTE (idle_resources)\n",
    "\n",
    "DATA_INGEST (upstream)\n",
    "    ‚îî‚îÄ‚îÄ ML_COMPUTE (data_starvation)\n",
    "```\n",
    "\n",
    "**Adjust parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate Training Dataset\n",
    "# Expected time: 24h=30-60s | 720h=15-20min (optimized with parallelization)\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Add src/ to Python path (works from either root or Argus directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'Argus':\n",
    "    argus_src = (current_dir / 'src').absolute()\n",
    "    argus_root = current_dir\n",
    "else:\n",
    "    argus_src = (current_dir / 'Argus' / 'src').absolute()\n",
    "    argus_root = current_dir / 'Argus'\n",
    "\n",
    "if str(argus_src) not in sys.path:\n",
    "    sys.path.insert(0, str(argus_src))\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - SIMPLE TWO-PARAMETER SETUP\n",
    "# ============================================\n",
    "\n",
    "TRAINING_HOURS = 366        # Duration: 24 (1 day), 168 (1 week), 720 (30 days - production)\n",
    "TOTAL_SERVERS = 45         # Fleet size: 20 (demo), 90 (default), 400 (production)\n",
    "\n",
    "# Servers are AUTO-DISTRIBUTED across 7 profiles:\n",
    "#   - Web/API:       28% (user-facing services)\n",
    "#   - ML Compute:    22% (training workloads)\n",
    "#   - Database:      17% (critical infrastructure - CASCADE SOURCE)\n",
    "#   - Data Ingest:   11% (ETL pipelines)\n",
    "#   - Risk Analytics: 9% (EOD calculations)\n",
    "#   - Generic:        7% (utility, max 10)\n",
    "#   - Conductor:      6% (orchestration)\n",
    "\n",
    "TRAINING_DIR = str(argus_root / 'training')\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f\"üè¢ Argus Dataset Generation\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   Duration: {TRAINING_HOURS} hours ({TRAINING_HOURS/24:.1f} days)\")\n",
    "print(f\"   Fleet size: {TOTAL_SERVERS} servers (auto-distributed across 7 profiles)\")\n",
    "print(f\"   Output: {TRAINING_DIR}\")\n",
    "\n",
    "# Show expected distribution\n",
    "print(f\"\\nüìä Expected Profile Distribution:\")\n",
    "dist = {\n",
    "    'Web/API': int(TOTAL_SERVERS * 0.28),\n",
    "    'ML Compute': int(TOTAL_SERVERS * 0.22),\n",
    "    'Database': int(TOTAL_SERVERS * 0.17),\n",
    "    'Data Ingest': int(TOTAL_SERVERS * 0.11),\n",
    "    'Risk Analytics': int(TOTAL_SERVERS * 0.09),\n",
    "    'Generic': min(int(TOTAL_SERVERS * 0.07), 10),\n",
    "    'Conductor': int(TOTAL_SERVERS * 0.06)\n",
    "}\n",
    "for profile, count in dist.items():\n",
    "    cascade_note = \" (CASCADE SOURCE)\" if profile == 'Database' else \"\"\n",
    "    print(f\"   {profile:<15} ~{count:>3} servers{cascade_note}\")\n",
    "\n",
    "# Estimate rows and time\n",
    "expected_timestamps = TRAINING_HOURS * 3600 // 5  # 5-second intervals\n",
    "expected_rows = expected_timestamps * TOTAL_SERVERS\n",
    "print(f\"\\nüìà Expected Output:\")\n",
    "print(f\"   ~{expected_rows:,} rows ({expected_timestamps:,} timestamps √ó {TOTAL_SERVERS} servers)\")\n",
    "print(f\"   Parallelized generation: ~{TRAINING_HOURS // 24 * 2 + 1}-{TRAINING_HOURS // 24 * 4 + 2} minutes\")\n",
    "print()\n",
    "\n",
    "print(\"üîó Inter-Server Dependencies:\")\n",
    "print(\"   Database failures cascade to: Web/API, Data Ingest, Risk, Conductor\")\n",
    "print(\"   Impact types: connection_failures, request_queuing, data_starvation\")\n",
    "print(\"   cascade_impact metric: 0.0 (no impact) to 1.0 (full cascade)\")\n",
    "print()\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "# Import and run generator\n",
    "from generators.metrics_generator import main as generate_metrics\n",
    "\n",
    "# Set up command-line arguments - SIMPLE: just --servers and --hours\n",
    "old_argv = sys.argv\n",
    "sys.argv = [\n",
    "    'metrics_generator.py',\n",
    "    '--hours', str(TRAINING_HOURS),\n",
    "    '--servers', str(TOTAL_SERVERS),  # Auto-distributes across profiles!\n",
    "    '--out_dir', TRAINING_DIR,\n",
    "    '--format', 'parquet'\n",
    "]\n",
    "\n",
    "try:\n",
    "    generate_metrics()\n",
    "    print(\"\\n‚úÖ Dataset generation complete!\")\n",
    "    success = True\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Generation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    success = False\n",
    "finally:\n",
    "    sys.argv = old_argv\n",
    "\n",
    "_elapsed = time.time() - _start\n",
    "_mins = int(_elapsed // 60)\n",
    "_secs = int(_elapsed % 60)\n",
    "print(f\"\\n‚è±Ô∏è  Execution time: {_mins}m {_secs}s\")\n",
    "\n",
    "if success:\n",
    "    # Show what was created\n",
    "    training_path = Path(TRAINING_DIR)\n",
    "    parquet_files = list(training_path.glob(\"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        latest = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "        df = pd.read_parquet(latest)\n",
    "        \n",
    "        print(f\"\\nüìä Dataset Summary:\")\n",
    "        print(f\"   File: {latest.name}\")\n",
    "        print(f\"   Size: {latest.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"   Records: {len(df):,}\")\n",
    "        print(f\"   Servers: {df['server_name'].nunique()}\")\n",
    "        \n",
    "        # Show actual profile distribution\n",
    "        if 'profile' in df.columns:\n",
    "            profile_counts = df.groupby('profile')['server_name'].nunique()\n",
    "            print(f\"\\n   Profile Distribution:\")\n",
    "            for profile, count in profile_counts.sort_values(ascending=False).items():\n",
    "                print(f\"     {profile:<20} {count:>3} servers\")\n",
    "        \n",
    "        # Show cascade_impact stats\n",
    "        if 'cascade_impact' in df.columns:\n",
    "            cascade_affected = (df['cascade_impact'] > 0).sum()\n",
    "            cascade_pct = (cascade_affected / len(df)) * 100\n",
    "            print(f\"\\n   Cascade Impact:\")\n",
    "            print(f\"     Records affected: {cascade_affected:,} ({cascade_pct:.1f}%)\")\n",
    "            print(f\"     Max intensity: {df['cascade_impact'].max():.2f}\")\n",
    "        \n",
    "        print(f\"\\n   Time span: {(df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "        print(f\"\\nüéØ Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m50na52xq5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Explorer\n",
    "\n",
    "Executive-level dataset analysis and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6ny7cku4y",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Explorer - Executive Presentation View\n",
    "# Professional analysis with visualizations suitable for C-suite presentations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'Argus':\n",
    "    argus_root = current_dir\n",
    "else:\n",
    "    argus_root = current_dir / 'Argus'\n",
    "\n",
    "if str(argus_root / 'src') not in sys.path:\n",
    "    sys.path.insert(0, str(argus_root / 'src'))\n",
    "\n",
    "# Plotting imports\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Plotly not available - visualizations disabled\")\n",
    "    print(\"   Install: pip install plotly\")\n",
    "\n",
    "# Find the most recent dataset\n",
    "training_dir = argus_root / 'training'\n",
    "parquet_files = list(training_dir.glob(\"*.parquet\"))\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"‚ùå No dataset found. Please run the Dataset Generation cell first.\")\n",
    "else:\n",
    "    latest_file = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "    \n",
    "    print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "    print(\"‚ïë\" + \" \" * 18 + \"DATASET ANALYSIS REPORT\" + \" \" * 27 + \"‚ïë\")\n",
    "    print(\"‚ïë\" + \" \" * 15 + \"Tachyon Argus Predictive Monitoring\" + \" \" * 17 + \"‚ïë\")\n",
    "    print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "    print()\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"üìÇ Loading dataset: {latest_file.name}\")\n",
    "    df = pd.read_parquet(latest_file)\n",
    "    print(f\"‚úÖ Loaded {len(df):,} records\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXECUTIVE SUMMARY\n",
    "    # ========================================================================\n",
    "    print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "    print(\"‚ïë\" + \" \" * 22 + \"EXECUTIVE SUMMARY\" + \" \" * 29 + \"‚ïë\")\n",
    "    print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "    print()\n",
    "    \n",
    "    file_size_mb = latest_file.stat().st_size / (1024 * 1024)\n",
    "    time_span = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600\n",
    "    num_servers = df['server_name'].nunique()\n",
    "    num_profiles = df['profile'].nunique() if 'profile' in df.columns else 0\n",
    "    records_per_hour = len(df) / time_span if time_span > 0 else 0\n",
    "    \n",
    "    print(f\"‚îå‚îÄ Dataset Metrics \" + \"‚îÄ\" * 50 + \"‚îê\")\n",
    "    print(f\"‚îÇ Total Records:          {len(df):>12,} samples{' ' * 24}‚îÇ\")\n",
    "    print(f\"‚îÇ File Size:              {file_size_mb:>12.1f} MB{' ' * 27}‚îÇ\")\n",
    "    print(f\"‚îÇ Time Span:              {time_span:>12.1f} hours ({time_span/24:.1f} days){' ' * 13}‚îÇ\")\n",
    "    print(f\"‚îÇ Sampling Rate:          {records_per_hour:>12.1f} records/hour{' ' * 16}‚îÇ\")\n",
    "    print(f\"‚îÇ Date Range:             {df['timestamp'].min().strftime('%Y-%m-%d %H:%M'):<33}‚îÇ\")\n",
    "    print(f\"‚îÇ                    to   {df['timestamp'].max().strftime('%Y-%m-%d %H:%M'):<33}‚îÇ\")\n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FLEET COMPOSITION\n",
    "    # ========================================================================\n",
    "    print(f\"‚îå‚îÄ Fleet Composition \" + \"‚îÄ\" * 48 + \"‚îê\")\n",
    "    print(f\"‚îÇ Total Servers:          {num_servers:>12} servers{' ' * 25}‚îÇ\")\n",
    "    \n",
    "    if 'profile' in df.columns:\n",
    "        print(f\"‚îÇ Server Profiles:        {num_profiles:>12} types{' ' * 27}‚îÇ\")\n",
    "        print(f\"‚îÇ{' ' * 68}‚îÇ\")\n",
    "        \n",
    "        profile_counts = df.groupby('profile')['server_name'].nunique().sort_values(ascending=False)\n",
    "        for profile, count in profile_counts.items():\n",
    "            pct = (count / num_servers) * 100\n",
    "            print(f\"‚îÇ  {profile[:20]:<20} {count:>3} ({pct:>5.1f}%) ‚îÇ\")\n",
    "    \n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # METRICS COVERAGE (15 ARGUS METRICS)\n",
    "    # ========================================================================\n",
    "    print(f\"‚îå‚îÄ Argus Metrics Coverage \" + \"‚îÄ\" * 43 + \"‚îê\")\n",
    "    \n",
    "    from core.nordiq_metrics import NORDIQ_METRICS, NUM_NORDIQ_METRICS\n",
    "    \n",
    "    available_metrics = [m for m in NORDIQ_METRICS if m in df.columns]\n",
    "    coverage_pct = (len(available_metrics) / NUM_NORDIQ_METRICS) * 100\n",
    "    \n",
    "    print(f\"‚îÇ Argus Metrics:          {len(available_metrics):>12} / {NUM_NORDIQ_METRICS} ({coverage_pct:.0f}%){' ' * 20}‚îÇ\")\n",
    "    print(f\"‚îÇ{' ' * 68}‚îÇ\")\n",
    "    \n",
    "    # Group metrics by category (now includes cascade)\n",
    "    metric_categories = {\n",
    "        'CPU': ['cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct'],\n",
    "        'Memory': ['mem_used_pct', 'swap_used_pct'],\n",
    "        'Disk': ['disk_usage_pct'],\n",
    "        'Network': ['net_in_mb_s', 'net_out_mb_s'],\n",
    "        'Connections': ['back_close_wait', 'front_close_wait'],\n",
    "        'System': ['load_average', 'uptime_days'],\n",
    "        'Cascade': ['cascade_impact']  # NEW!\n",
    "    }\n",
    "    \n",
    "    for category, metrics in metric_categories.items():\n",
    "        category_available = [m for m in metrics if m in df.columns]\n",
    "        cat_pct = (len(category_available) / len(metrics)) * 100\n",
    "        status = \"‚úÖ\" if cat_pct == 100 else \"‚ö†Ô∏è\" if cat_pct > 0 else \"‚ùå\"\n",
    "        print(f\"‚îÇ  {status} {category:<15} {len(category_available):>2}/{len(metrics)} metrics ({cat_pct:>5.1f}%){' ' * 25}‚îÇ\")\n",
    "    \n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CASCADE IMPACT ANALYSIS (NEW!)\n",
    "    # ========================================================================\n",
    "    if 'cascade_impact' in df.columns:\n",
    "        print(f\"‚îå‚îÄ Cascade Impact Analysis \" + \"‚îÄ\" * 42 + \"‚îê\")\n",
    "        \n",
    "        cascade_affected = (df['cascade_impact'] > 0).sum()\n",
    "        cascade_pct = (cascade_affected / len(df)) * 100\n",
    "        \n",
    "        print(f\"‚îÇ Records with cascade:   {cascade_affected:>12,} ({cascade_pct:.1f}%){' ' * 16}‚îÇ\")\n",
    "        print(f\"‚îÇ Max cascade intensity:  {df['cascade_impact'].max():>12.3f}{' ' * 26}‚îÇ\")\n",
    "        print(f\"‚îÇ Mean (when active):     {df[df['cascade_impact'] > 0]['cascade_impact'].mean():>12.3f}{' ' * 26}‚îÇ\")\n",
    "        \n",
    "        # Cascade by profile\n",
    "        if 'profile' in df.columns:\n",
    "            print(f\"‚îÇ{' ' * 68}‚îÇ\")\n",
    "            print(f\"‚îÇ Cascade Impact by Profile:{' ' * 40}‚îÇ\")\n",
    "            cascade_by_profile = df[df['cascade_impact'] > 0].groupby('profile')['cascade_impact'].mean()\n",
    "            for profile, avg_impact in cascade_by_profile.sort_values(ascending=False).head(5).items():\n",
    "                print(f\"‚îÇ   {profile:<20} avg: {avg_impact:.3f}{' ' * 30}‚îÇ\")\n",
    "        \n",
    "        print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DATA QUALITY METRICS\n",
    "    # ========================================================================\n",
    "    print(f\"‚îå‚îÄ Data Quality \" + \"‚îÄ\" * 53 + \"‚îê\")\n",
    "    \n",
    "    total_cells = len(df) * len(available_metrics)\n",
    "    missing_cells = df[available_metrics].isna().sum().sum()\n",
    "    completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
    "    \n",
    "    print(f\"‚îÇ Completeness:           {completeness:>12.2f}%{' ' * 28}‚îÇ\")\n",
    "    print(f\"‚îÇ Missing Values:         {missing_cells:>12,} cells{' ' * 24}‚îÇ\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated(subset=['timestamp', 'server_name']).sum()\n",
    "    duplicate_pct = (duplicates / len(df)) * 100\n",
    "    print(f\"‚îÇ Duplicate Records:      {duplicates:>12,} ({duplicate_pct:.2f}%){' ' * 20}‚îÇ\")\n",
    "    \n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STATISTICAL SUMMARY\n",
    "    # ========================================================================\n",
    "    print(f\"‚îå‚îÄ Key Metrics Statistics \" + \"‚îÄ\" * 43 + \"‚îê\")\n",
    "    print(f\"‚îÇ {'Metric':<20} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10} ‚îÇ\")\n",
    "    print(f\"‚îÇ {'-'*20} {'-'*10} {'-'*10} {'-'*10} {'-'*10} ‚îÇ\")\n",
    "    \n",
    "    key_metrics = ['cpu_user_pct', 'mem_used_pct', 'disk_usage_pct', 'load_average', 'cascade_impact']\n",
    "    for metric in key_metrics:\n",
    "        if metric in df.columns:\n",
    "            stats = df[metric].describe()\n",
    "            print(f\"‚îÇ {metric:<20} {stats['mean']:>10.2f} {stats['std']:>10.2f} {stats['min']:>10.2f} {stats['max']:>10.2f} ‚îÇ\")\n",
    "    \n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VISUALIZATIONS\n",
    "    # ========================================================================\n",
    "    if PLOTLY_AVAILABLE:\n",
    "        print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "        print(\"‚ïë\" + \" \" * 20 + \"EXECUTIVE VISUALIZATIONS\" + \" \" * 24 + \"‚ïë\")\n",
    "        print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "        print()\n",
    "        \n",
    "        # 1. Fleet Distribution by Profile\n",
    "        if 'profile' in df.columns:\n",
    "            fig_fleet = px.pie(\n",
    "                profile_counts.reset_index(), \n",
    "                values='server_name', \n",
    "                names='profile',\n",
    "                title='Fleet Distribution by Server Profile',\n",
    "                color_discrete_sequence=px.colors.qualitative.Set3\n",
    "            )\n",
    "            fig_fleet.update_layout(font=dict(size=14), height=500)\n",
    "            fig_fleet.show()\n",
    "        \n",
    "        # 2. Cascade Impact Distribution (NEW!)\n",
    "        if 'cascade_impact' in df.columns and df['cascade_impact'].sum() > 0:\n",
    "            cascade_data = df[df['cascade_impact'] > 0]\n",
    "            fig_cascade = px.histogram(\n",
    "                cascade_data, \n",
    "                x='cascade_impact',\n",
    "                color='profile' if 'profile' in df.columns else None,\n",
    "                title='Cascade Impact Distribution by Profile',\n",
    "                nbins=50\n",
    "            )\n",
    "            fig_cascade.update_layout(font=dict(size=12), height=400)\n",
    "            fig_cascade.show()\n",
    "        \n",
    "        # 3. CPU Heatmap by Profile and Hour\n",
    "        if all(m in df.columns for m in ['cpu_user_pct']) and 'profile' in df.columns:\n",
    "            df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "            heatmap_data = df.groupby(['hour', 'profile'])['cpu_user_pct'].mean().reset_index()\n",
    "            heatmap_pivot = heatmap_data.pivot(index='profile', columns='hour', values='cpu_user_pct')\n",
    "            \n",
    "            fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "                z=heatmap_pivot.values,\n",
    "                x=heatmap_pivot.columns,\n",
    "                y=heatmap_pivot.index,\n",
    "                colorscale='RdYlGn_r',\n",
    "                colorbar=dict(title=\"CPU %\")\n",
    "            ))\n",
    "            fig_heatmap.update_layout(\n",
    "                title='CPU Utilization by Profile and Hour',\n",
    "                xaxis_title='Hour of Day',\n",
    "                yaxis_title='Server Profile',\n",
    "                height=400\n",
    "            )\n",
    "            fig_heatmap.show()\n",
    "        \n",
    "        print(\"‚úÖ Executive visualizations generated\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # READINESS ASSESSMENT\n",
    "    # ========================================================================\n",
    "    print()\n",
    "    print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "    \n",
    "    is_ready = (\n",
    "        len(df) >= 1000 and\n",
    "        num_servers >= 5 and\n",
    "        completeness >= 95.0 and\n",
    "        len(available_metrics) >= 14  # At least 14 of 15 metrics\n",
    "    )\n",
    "    \n",
    "    if is_ready:\n",
    "        print(\"‚ïë\" + \" \" * 15 + \"‚úÖ DATASET READY FOR TRAINING\" + \" \" * 23 + \"‚ïë\")\n",
    "        print(f\"‚ïë\" + \" \" * 10 + f\"{len(df):,} records | {num_servers} servers | {len(available_metrics)}/15 metrics\" + \" \" * 10 + \"‚ïë\")\n",
    "    else:\n",
    "        print(\"‚ïë\" + \" \" * 12 + \"‚ö†Ô∏è  DATASET MAY NEED MORE DATA\" + \" \" * 26 + \"‚ïë\")\n",
    "    \n",
    "    print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_training",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Trains the Temporal Fusion Transformer with:\n",
    "- Profile-based transfer learning\n",
    "- GPU acceleration (if available)\n",
    "- Early stopping to prevent overfitting\n",
    "- **15 Argus metrics including cascade_impact**\n",
    "\n",
    "**Adjust parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Train TFT Model\n",
    "# Expected time: 10 epochs=3-5h | 20 epochs=6-10h\n",
    "# STREAMING MODE: ~10x less memory usage for large datasets\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to Python path (works from either root or Argus directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'Argus':\n",
    "    argus_src = (current_dir / 'src').absolute()\n",
    "    argus_root = current_dir\n",
    "else:\n",
    "    argus_src = (current_dir / 'Argus' / 'src').absolute()\n",
    "    argus_root = current_dir / 'Argus'\n",
    "\n",
    "if str(argus_src) not in sys.path:\n",
    "    sys.path.insert(0, str(argus_src))\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - ADJUST THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "TRAINING_EPOCHS = 10      # Recommended: 10-20 epochs\n",
    "\n",
    "# STREAMING MODE: Use for large datasets (30+ days, 90+ servers)\n",
    "# - Loads time chunks one at a time instead of full dataset\n",
    "# - Memory: ~2-4 GB instead of 130+ GB\n",
    "USE_STREAMING_MODE = True  # Set to True for large datasets\n",
    "\n",
    "# IMPORTANT: Training must run from Argus directory for paths to work correctly\n",
    "original_dir = Path.cwd()\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f\"ü§ñ Tachyon Argus Model Training\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   Epochs: {TRAINING_EPOCHS}\")\n",
    "print(f\"   Dataset: ./training/ (relative to Argus/)\")\n",
    "print(f\"   Mode: {'STREAMING (memory-efficient)' if USE_STREAMING_MODE else 'Standard (full dataset in memory)'}\")\n",
    "print(f\"   Metrics: 15 Argus metrics (including cascade_impact)\")\n",
    "print()\n",
    "\n",
    "# Estimate training time\n",
    "est_mins_low = TRAINING_EPOCHS * 20\n",
    "est_mins_high = TRAINING_EPOCHS * 30\n",
    "if USE_STREAMING_MODE:\n",
    "    est_mins_low = int(est_mins_low * 1.2)\n",
    "    est_mins_high = int(est_mins_high * 1.2)\n",
    "print(f\"‚è±Ô∏è  Estimated time: {est_mins_low//60}h {est_mins_low%60}m - {est_mins_high//60}h {est_mins_high%60}m\")\n",
    "print(f\"   (Based on ~20-30 minutes per epoch on RTX 4090)\")\n",
    "print()\n",
    "print(\"üöÄ Starting training...\")\n",
    "print()\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "# Import and run trainer\n",
    "from training.tft_trainer import train_model\n",
    "\n",
    "try:\n",
    "    # CRITICAL: Change to Argus directory before training\n",
    "    os.chdir(argus_root)\n",
    "    print(f\"[INFO] Working directory: {Path.cwd()}\")\n",
    "    \n",
    "    model_path = train_model(\n",
    "        dataset_path='./training/',\n",
    "        epochs=TRAINING_EPOCHS,\n",
    "        per_server=False,\n",
    "        streaming=USE_STREAMING_MODE\n",
    "    )\n",
    "    \n",
    "    if model_path:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìÅ Model saved: {model_path}\")\n",
    "        print()\n",
    "        print(\"üéØ Transfer Learning Enabled:\")\n",
    "        print(\"   ‚úÖ Model learned patterns for each server profile\")\n",
    "        print(\"   ‚úÖ Model learned cascade dependency patterns\")\n",
    "        print(\"   ‚úÖ New servers get strong predictions from day 1\")\n",
    "        print(\"   ‚úÖ No retraining needed when adding servers of known types\")\n",
    "        print()\n",
    "        print(\"üí° Next Steps:\")\n",
    "        print(\"   1. Start system: start_all.bat (Windows) or ./start_all.sh (Linux/Mac)\")\n",
    "        print(\"   2. Open dashboard: http://localhost:8050\")\n",
    "        print(\"   3. API endpoint: http://localhost:8000\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Training failed - check logs above\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    os.chdir(original_dir)\n",
    "    print(f\"\\n[INFO] Restored working directory: {Path.cwd()}\")\n",
    "\n",
    "_elapsed = time.time() - _start\n",
    "_hours = int(_elapsed // 3600)\n",
    "_mins = int((_elapsed % 3600) // 60)\n",
    "_secs = int(_elapsed % 60)\n",
    "print(f\"\\n‚è±Ô∏è  Execution time: {_hours}h {_mins}m {_secs}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Complete!\n",
    "\n",
    "### What you've built:\n",
    "\n",
    "**Profile-Based Transfer Learning**\n",
    "- Model learned patterns for 7 server profiles\n",
    "- New servers get accurate predictions immediately\n",
    "- No retraining needed for known server types\n",
    "\n",
    "**Inter-Server Cascade Dependencies**\n",
    "- Model understands database ‚Üí service dependencies\n",
    "- Predicts cascade impact propagation\n",
    "- 15 Argus metrics including cascade_impact\n",
    "\n",
    "**Production-Ready System**\n",
    "- 8-hour forecast horizon (96 steps)\n",
    "- Quantile uncertainty estimates (p10, p50, p90)\n",
    "- Safetensors model format\n",
    "\n",
    "---\n",
    "\n",
    "### Launch the System:\n",
    "\n",
    "**Windows:**\n",
    "```bash\n",
    "cd Argus\n",
    "start_all.bat\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "cd Argus\n",
    "./start_all.sh\n",
    "```\n",
    "\n",
    "**Manual start (development):**\n",
    "```bash\n",
    "# Terminal 1 - Inference daemon\n",
    "cd Argus\n",
    "conda activate py310\n",
    "python src/daemons/tft_inference_daemon.py --port 8000\n",
    "\n",
    "# Terminal 2 - Metrics generator (with cascade scenario)\n",
    "cd Argus\n",
    "conda activate py310\n",
    "python src/daemons/metrics_generator_daemon.py --stream --servers 20\n",
    "\n",
    "# Terminal 3 - Dashboard\n",
    "cd Argus\n",
    "conda activate py310\n",
    "python dash_app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Access Points:\n",
    "\n",
    "- **Dashboard:** http://localhost:8050\n",
    "- **Inference API:** http://localhost:8000\n",
    "- **Metrics Generator API:** http://localhost:8001\n",
    "- **Health Check:** http://localhost:8000/health\n",
    "\n",
    "---\n",
    "\n",
    "### Dashboard Scenario Buttons:\n",
    "\n",
    "| Button | Description |\n",
    "|--------|-------------|\n",
    "| üü¢ Healthy | All servers healthy (force_healthy mode) |\n",
    "| üü° Degrading | Gradual performance degradation |\n",
    "| üî¥ Critical | Critical server failures |\n",
    "| üîó Cascade | Database cascade failure simulation |\n",
    "\n",
    "---\n",
    "\n",
    "### Documentation:\n",
    "\n",
    "- **[Argus/README.md](Argus/README.md)** - Complete system overview\n",
    "- **[Argus/Docs/SERVER_PROFILES.md](Argus/Docs/SERVER_PROFILES.md)** - 7 server profiles explained\n",
    "- **[Argus/Docs/GETTING_STARTED.md](Argus/Docs/GETTING_STARTED.md)** - Setup and configuration\n",
    "- **[Docs/ARCHITECTURE_GUIDE.md](Docs/ARCHITECTURE_GUIDE.md)** - System architecture and data contract\n",
    "\n",
    "---\n",
    "\n",
    "**Your Tachyon Argus predictive monitoring system is ready!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
