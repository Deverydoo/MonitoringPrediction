{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# üöÄ TFT Training Quick Start\n\n## Simplified workflow for dataset creation and model training\n\nThis notebook does two things:\n1. **Generate training dataset** - Create realistic server metrics data\n2. **Train TFT model** - Train the Temporal Fusion Transformer\n\nThe dashboard and inference daemon handle everything else!\n\n---\n\n**‚è±Ô∏è Estimated Times:**\n- Dataset generation (24h): ~30-60 seconds\n- Dataset generation (720h): ~5-10 minutes\n- Model training (10 epochs): ~3-5 hours on RTX 4090\n\n**üéØ After Training:**\n- Start system: `start_all.bat` (Windows) or `./start_all.sh` (Linux/Mac)\n- Dashboard: http://localhost:8501\n- API: http://localhost:8000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup and Configuration\nimport sys\nimport time\nfrom pathlib import Path\n\n# Add src/ to Python path (works from either root or NordIQ directory)\ncurrent_dir = Path.cwd()\nif current_dir.name == 'NordIQ':\n    # Notebook is in NordIQ folder\n    nordiq_src = (current_dir / 'src').absolute()\nelse:\n    # Notebook is in root folder\n    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n\nif str(nordiq_src) not in sys.path:\n    sys.path.insert(0, str(nordiq_src))\n\nprint(\"üéØ TFT Training System\")\nprint(\"=\" * 70)\nprint(\"‚úÖ Python path configured\")\nprint(f\"üìÅ NordIQ source: {nordiq_src}\")\nprint(\"\\nüîß Configuration:\")\nprint(\"   Training directory: ./training/\")\nprint(\"   Models directory: ./models/\")\nprint(\"   Prediction horizon: 96 steps (8 hours)\")\nprint(\"   Context length: 288 steps (24 hours)\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "section_production",
   "metadata": {},
   "source": "---\n\n## Cell 2: Generate Training Dataset\n\nCreates realistic server metrics with:\n- 7 server profiles (ML, DB, Web, Conductor, ETL, Risk, Generic)\n- Financial market hours patterns\n- 14 LINBORG-compatible metrics\n\n**Adjust parameters below:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Generate Training Dataset\n# Expected time: 24h=30-60s | 720h=5-10min\n\nimport sys\nimport time\nfrom pathlib import Path\nimport pandas as pd\n\n# Add src/ to Python path (works from either root or NordIQ directory)\ncurrent_dir = Path.cwd()\nif current_dir.name == 'NordIQ':\n    # Notebook is in NordIQ folder\n    nordiq_src = (current_dir / 'src').absolute()\nelse:\n    # Notebook is in root folder\n    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n\nif str(nordiq_src) not in sys.path:\n    sys.path.insert(0, str(nordiq_src))\n\n# ============================================\n# CONFIGURATION - ADJUST THESE VALUES\n# ============================================\n\nTRAINING_HOURS = 24        # Options: 24, 168, 720 (recommended: 720 for production)\nNUM_ML_COMPUTE = 5         # ML training nodes\nNUM_DATABASE = 4           # Database servers\nNUM_WEB_API = 6            # Web/API servers\nNUM_CONDUCTOR_MGMT = 1     # Conductor management\nNUM_DATA_INGEST = 2        # ETL/streaming servers\nNUM_RISK_ANALYTICS = 1     # Risk calculation servers\nNUM_GENERIC = 1            # Generic/utility servers\n\nTRAINING_DIR = './training'\n\n# ============================================\n\nprint(f\"üè¢ Dataset Generation\")\nprint(\"-\" * 70)\nprint(f\"‚öôÔ∏è  Configuration:\")\nprint(f\"   Duration: {TRAINING_HOURS} hours ({TRAINING_HOURS/24:.1f} days)\")\ntotal_servers = NUM_ML_COMPUTE + NUM_DATABASE + NUM_WEB_API + NUM_CONDUCTOR_MGMT + NUM_DATA_INGEST + NUM_RISK_ANALYTICS + NUM_GENERIC\nprint(f\"   Total servers: {total_servers}\")\nprint(f\"   Output: {TRAINING_DIR}\")\nprint()\n\n_start = time.time()\n\n# Import and run generator\nfrom generators.metrics_generator import main as generate_metrics\n\n# Set up command-line arguments for the generator\nold_argv = sys.argv\nsys.argv = [\n    'metrics_generator.py',\n    '--hours', str(TRAINING_HOURS),\n    '--num_ml_compute', str(NUM_ML_COMPUTE),\n    '--num_database', str(NUM_DATABASE),\n    '--num_web_api', str(NUM_WEB_API),\n    '--num_conductor_mgmt', str(NUM_CONDUCTOR_MGMT),\n    '--num_data_ingest', str(NUM_DATA_INGEST),\n    '--num_risk_analytics', str(NUM_RISK_ANALYTICS),\n    '--num_generic', str(NUM_GENERIC),\n    '--out_dir', TRAINING_DIR,\n    '--format', 'parquet'\n]\n\ntry:\n    generate_metrics()\n    print(\"\\n‚úÖ Dataset generation complete!\")\n    success = True\nexcept Exception as e:\n    print(f\"\\n‚ùå Generation failed: {e}\")\n    success = False\nfinally:\n    sys.argv = old_argv\n\n_elapsed = time.time() - _start\n_mins = int(_elapsed // 60)\n_secs = int(_elapsed % 60)\nprint(f\"\\n‚è±Ô∏è  Execution time: {_mins}m {_secs}s\")\n\nif success:\n    # Show what was created\n    training_path = Path(TRAINING_DIR)\n    parquet_files = list(training_path.glob(\"*.parquet\"))\n    \n    if parquet_files:\n        latest = max(parquet_files, key=lambda p: p.stat().st_mtime)\n        df = pd.read_parquet(latest)\n        \n        print(f\"\\nüìä Dataset Summary:\")\n        print(f\"   File: {latest.name}\")\n        print(f\"   Size: {latest.stat().st_size / (1024*1024):.1f} MB\")\n        print(f\"   Records: {len(df):,}\")\n        print(f\"   Servers: {df['server_name'].nunique()}\")\n        print(f\"   Profiles: {sorted(df['profile'].unique())}\")\n        print(f\"   Time span: {(df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600:.1f} hours\")\n        print(f\"\\nüéØ Ready for training!\")"
  },
  {
   "cell_type": "markdown",
   "id": "section_training",
   "metadata": {},
   "source": "---\n\n## Cell 3: Train TFT Model\n\nTrains the Temporal Fusion Transformer with:\n- Profile-based transfer learning\n- GPU acceleration (if available)\n- Early stopping to prevent overfitting\n\n**Adjust parameters below:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Train TFT Model\n# Expected time: 10 epochs=3-5h | 20 epochs=6-10h\n\nimport sys\nimport time\nfrom pathlib import Path\n\n# Add src/ to Python path (works from either root or NordIQ directory)\ncurrent_dir = Path.cwd()\nif current_dir.name == 'NordIQ':\n    # Notebook is in NordIQ folder\n    nordiq_src = (current_dir / 'src').absolute()\nelse:\n    # Notebook is in root folder\n    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n\nif str(nordiq_src) not in sys.path:\n    sys.path.insert(0, str(nordiq_src))\n\n# ============================================\n# CONFIGURATION - ADJUST THESE VALUES\n# ============================================\n\nTRAINING_EPOCHS = 10       # Recommended: 10-20 epochs\nDATASET_PATH = './training'\n\n# ============================================\n\nprint(f\"ü§ñ Model Training\")\nprint(\"-\" * 70)\nprint(f\"‚öôÔ∏è  Configuration:\")\nprint(f\"   Epochs: {TRAINING_EPOCHS}\")\nprint(f\"   Dataset: {DATASET_PATH}\")\nprint(f\"   Mode: Fleet-wide with profile-based transfer learning\")\nprint()\n\n# Estimate training time\nest_mins_low = TRAINING_EPOCHS * 20\nest_mins_high = TRAINING_EPOCHS * 30\nprint(f\"‚è±Ô∏è  Estimated time: {est_mins_low//60}h {est_mins_low%60}m - {est_mins_high//60}h {est_mins_high%60}m\")\nprint(f\"   (Based on ~20-30 minutes per epoch on RTX 4090)\")\nprint()\nprint(\"üöÄ Starting training...\")\nprint()\n\n_start = time.time()\n\n# Import and run trainer\nfrom training.tft_trainer import train_model\n\ntry:\n    model_path = train_model(\n        dataset_path=DATASET_PATH,\n        epochs=TRAINING_EPOCHS,\n        per_server=False  # Fleet-wide training with profiles\n    )\n    \n    if model_path:\n        print(\"\\n\" + \"=\" * 70)\n        print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n        print(\"=\" * 70)\n        print(f\"üìÅ Model saved: {model_path}\")\n        print()\n        print(\"üéØ Transfer Learning Enabled:\")\n        print(\"   ‚úÖ Model learned patterns for each server profile\")\n        print(\"   ‚úÖ New servers get strong predictions from day 1\")\n        print(\"   ‚úÖ No retraining needed when adding servers of known types\")\n        print()\n        print(\"üí° Next Steps:\")\n        print(\"   1. Start system: start_all.bat (Windows) or ./start_all.sh (Linux/Mac)\")\n        print(\"   2. Open dashboard: http://localhost:8501\")\n        print(\"   3. API endpoint: http://localhost:8000\")\n    else:\n        print(\"\\n‚ùå Training failed - check logs above\")\n        \nexcept Exception as e:\n    print(f\"\\n‚ùå Training error: {e}\")\n    import traceback\n    traceback.print_exc()\n\n_elapsed = time.time() - _start\n_hours = int(_elapsed // 3600)\n_mins = int((_elapsed % 3600) // 60)\n_secs = int(_elapsed % 60)\nprint(f\"\\n‚è±Ô∏è  Execution time: {_hours}h {_mins}m {_secs}s\")"
  },
  {
   "cell_type": "markdown",
   "id": "section_summary",
   "metadata": {},
   "source": "---\n\n## üéâ Training Complete!\n\n### What you've built:\n\n‚úÖ **Profile-Based Transfer Learning**\n- Model learned patterns for 7 server profiles\n- New servers get accurate predictions immediately\n- No retraining needed for known server types\n\n‚úÖ **Production-Ready System**\n- 8-hour forecast horizon (96 steps)\n- Quantile uncertainty estimates (p10, p50, p90)\n- 14 LINBORG-compatible metrics\n- Safetensors model format\n\n---\n\n### üöÄ Launch the System:\n\n**Windows:**\n```bash\nstart_all.bat\n```\n\n**Linux/Mac:**\n```bash\n./start_all.sh\n```\n\n**Manual start (development):**\n```bash\n# Terminal 1 - Inference daemon\nconda activate py310\npython NordIQ/src/daemons/tft_inference_daemon.py --port 8000\n\n# Terminal 2 - Metrics generator\nconda activate py310\npython NordIQ/src/daemons/metrics_generator_daemon.py --stream --servers 20\n\n# Terminal 3 - Dashboard\nconda activate py310\nstreamlit run NordIQ/src/dashboard/tft_dashboard_web.py\n```\n\n---\n\n### üìä Access Points:\n\n- **Dashboard:** http://localhost:8501\n- **Inference API:** http://localhost:8000\n- **Metrics Generator API:** http://localhost:8001\n- **Health Check:** http://localhost:8000/health\n\n---\n\n### üìö Documentation:\n\n- **[NordIQ/README.md](NordIQ/README.md)** - Complete system overview\n- **[NordIQ/Docs/SERVER_PROFILES.md](NordIQ/Docs/SERVER_PROFILES.md)** - 7 server profiles explained\n- **[NordIQ/Docs/API_KEY_SETUP.md](NordIQ/Docs/API_KEY_SETUP.md)** - Security configuration\n- **[NordIQ/Docs/DATA_CONTRACT.md](NordIQ/Docs/DATA_CONTRACT.md)** - Metrics schema\n\n---\n\n### üîÑ Incremental Training:\n\nTo add more training epochs later (recommended for continuous learning):\n\n```bash\n# Add 1-5 epochs per week\npython NordIQ/src/training/tft_trainer.py --epochs 5 --incremental\n```\n\nThe system will add epochs to your existing model without starting over!\n\n---\n\n**üéâ Your predictive monitoring system is ready!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}