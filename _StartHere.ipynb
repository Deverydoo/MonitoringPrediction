{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# üöÄ TFT Training Quick Start\n",
    "\n",
    "## Simplified workflow for dataset creation and model training\n",
    "\n",
    "This notebook does two things:\n",
    "1. **Generate training dataset** - Create realistic server metrics data\n",
    "2. **Train TFT model** - Train the Temporal Fusion Transformer\n",
    "\n",
    "The dashboard and inference daemon handle everything else!\n",
    "\n",
    "---\n",
    "\n",
    "**‚è±Ô∏è Estimated Times:**\n",
    "- Dataset generation (24h): ~30-60 seconds\n",
    "- Dataset generation (720h): ~5-10 minutes\n",
    "- Model training (10 epochs): ~3-5 hours on RTX 4090\n",
    "\n",
    "**üéØ After Training:**\n",
    "- Start system: `start_all.bat` (Windows) or `./start_all.sh` (Linux/Mac)\n",
    "- Dashboard: http://localhost:8050\n",
    "- API: http://localhost:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to Python path (works from either root or NordIQ directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    # Notebook is in NordIQ folder\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "print(\"üéØ TFT Training System\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Python path configured\")\n",
    "print(f\"üìÅ NordIQ source: {nordiq_src}\")\n",
    "print(f\"üìÅ NordIQ root: {nordiq_root}\")\n",
    "print(\"\\nüîß Configuration:\")\n",
    "print(f\"   Training directory: {nordiq_root}/training/\")\n",
    "print(f\"   Models directory: {nordiq_root}/models/\")\n",
    "print(\"   Prediction horizon: 96 steps (8 hours)\")\n",
    "print(\"   Context length: 288 steps (24 hours)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grw8o6ndz6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## System Health Check\n",
    "\n",
    "Verify your environment is ready for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ntc378ebr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Comprehensive System Check\n",
    "# Verify GPU, Python environment, dependencies, and system readiness\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "# Setup paths (same as Cell 1)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "print(\"‚ïë\" + \" \" * 20 + \"SYSTEM HEALTH CHECK\" + \" \" * 29 + \"‚ïë\")\n",
    "print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PYTHON ENVIRONMENT\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ Python Environment \" + \"‚îÄ\" * 47 + \"‚îê\")\n",
    "print(f\"‚îÇ Python Version:     {platform.python_version():<46}‚îÇ\")\n",
    "print(f\"‚îÇ Platform:           {platform.system()} {platform.release():<36}‚îÇ\")\n",
    "print(f\"‚îÇ Architecture:       {platform.machine():<46}‚îÇ\")\n",
    "\n",
    "# Working directory - handle long paths gracefully\n",
    "cwd = str(Path.cwd())\n",
    "if len(cwd) <= 45:\n",
    "    print(f\"‚îÇ Working Directory:  {cwd:<46}‚îÇ\")\n",
    "else:\n",
    "    # Split long paths across multiple lines\n",
    "    print(f\"‚îÇ Working Directory:                                          ‚îÇ\")\n",
    "    # Show path in chunks of 60 characters\n",
    "    chunk_size = 60\n",
    "    for i in range(0, len(cwd), chunk_size):\n",
    "        chunk = cwd[i:i+chunk_size]\n",
    "        print(f\"‚îÇ   {chunk:<64}‚îÇ\")\n",
    "\n",
    "# Show NordIQ root detection\n",
    "nordiq_root_str = str(nordiq_root)\n",
    "if len(nordiq_root_str) <= 45:\n",
    "    print(f\"‚îÇ NordIQ Root:        {nordiq_root_str:<46}‚îÇ\")\n",
    "else:\n",
    "    print(f\"‚îÇ NordIQ Root:                                                ‚îÇ\")\n",
    "    for i in range(0, len(nordiq_root_str), chunk_size):\n",
    "        chunk = nordiq_root_str[i:i+chunk_size]\n",
    "        print(f\"‚îÇ   {chunk:<64}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. GPU AVAILABILITY & PYTORCH CUDA CHECK\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ GPU Status \" + \"‚îÄ\" * 54 + \"‚îê\")\n",
    "\n",
    "gpu_available = False\n",
    "gpu_name = \"Not available\"\n",
    "gpu_memory = 0\n",
    "cuda_version = \"N/A\"\n",
    "torch_cuda_enabled = False\n",
    "pytorch_installed = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    pytorch_installed = True\n",
    "    torch_cuda_enabled = torch.cuda.is_available()\n",
    "    gpu_available = torch_cuda_enabled\n",
    "    \n",
    "    if torch_cuda_enabled:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        cuda_version = torch.version.cuda\n",
    "        \n",
    "        print(f\"‚îÇ ‚úÖ GPU Detected:     {gpu_name[:45]:<45}‚îÇ\")\n",
    "        print(f\"‚îÇ    CUDA Version:     {cuda_version:<46}‚îÇ\")\n",
    "        print(f\"‚îÇ    Memory:           {gpu_memory:.1f} GB{' ' * 42}‚îÇ\")\n",
    "        print(f\"‚îÇ    PyTorch CUDA:     Enabled{' ' * 40}‚îÇ\")\n",
    "        \n",
    "        # GPU utilization\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n",
    "                                   '--format=csv,noheader,nounits'], \n",
    "                                  capture_output=True, text=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                gpu_util, mem_used, mem_total = result.stdout.strip().split(',')\n",
    "                print(f\"‚îÇ    Utilization:      {gpu_util.strip()}%{' ' * 43}‚îÇ\")\n",
    "                print(f\"‚îÇ    Memory Used:      {mem_used.strip()} MB / {mem_total.strip()} MB{' ' * 28}‚îÇ\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    else:\n",
    "        # PyTorch installed but CUDA not available\n",
    "        print(\"‚îÇ ‚ö†Ô∏è  PyTorch installed but CUDA not enabled{' ' * 24}‚îÇ\")\n",
    "        print(f\"‚îÇ    PyTorch Version:  {torch.__version__:<46}‚îÇ\")\n",
    "        print(f\"‚îÇ    CUDA Built:       {torch.version.cuda if torch.version.cuda else 'No (CPU-only)':<46}‚îÇ\")\n",
    "        print(f\"‚îÇ    CUDA Available:   {torch_cuda_enabled}{' ' * 41}‚îÇ\")\n",
    "        print(\"‚îÇ{' ' * 68}‚îÇ\")\n",
    "        print(\"‚îÇ ‚ö†Ô∏è  Training will use CPU (20-40x slower){' ' * 25}‚îÇ\")\n",
    "        print(\"‚îÇ    Expected time:    10 epochs ‚âà 20-40 hours{' ' * 21}‚îÇ\")\n",
    "        print(\"‚îÇ{' ' * 68}‚îÇ\")\n",
    "        print(\"‚îÇ üí° To enable GPU:{' ' * 50}‚îÇ\")\n",
    "        print(\"‚îÇ    1. Verify NVIDIA GPU is present (nvidia-smi){' ' * 19}‚îÇ\")\n",
    "        print(\"‚îÇ    2. Install CUDA Toolkit (nvidia.com/cuda){' ' * 22}‚îÇ\")\n",
    "        print(\"‚îÇ    3. Reinstall PyTorch with CUDA:{' ' * 34}‚îÇ\")\n",
    "        print(\"‚îÇ       pip uninstall torch{' ' * 42}‚îÇ\")\n",
    "        print(\"‚îÇ       pip install torch --index-url{' ' * 30}‚îÇ\")\n",
    "        print(\"‚îÇ         https://download.pytorch.org/whl/cu121{' ' * 20}‚îÇ\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚îÇ ‚ùå PyTorch not installed{' ' * 42}‚îÇ\")\n",
    "    print(\"‚îÇ{' ' * 68}‚îÇ\")\n",
    "    print(\"‚îÇ Install with GPU support:{' ' * 42}‚îÇ\")\n",
    "    print(\"‚îÇ   pip install torch --index-url{' ' * 34}‚îÇ\")\n",
    "    print(\"‚îÇ     https://download.pytorch.org/whl/cu121{' ' * 24}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CRITICAL DEPENDENCIES\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ Critical Dependencies \" + \"‚îÄ\" * 43 + \"‚îê\")\n",
    "\n",
    "dependencies = {\n",
    "    'torch': 'PyTorch (Deep Learning)',\n",
    "    'lightning': 'PyTorch Lightning (Training)',\n",
    "    'pandas': 'Pandas (Data Processing)',\n",
    "    'numpy': 'NumPy (Numerical Computing)',\n",
    "    'pytorch_forecasting': 'PyTorch Forecasting (TFT Model)',\n",
    "    'fastapi': 'FastAPI (Inference API)',\n",
    "    'plotly': 'Plotly (Dashboard)',\n",
    "    'dash': 'Dash (Dashboard Framework)'\n",
    "}\n",
    "\n",
    "missing_deps = []\n",
    "installed_deps = []\n",
    "\n",
    "for package, description in dependencies.items():\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is not None:\n",
    "        try:\n",
    "            module = importlib.import_module(package)\n",
    "            version = getattr(module, '__version__', 'unknown')\n",
    "            status = \"‚úÖ\"\n",
    "            installed_deps.append(package)\n",
    "            pkg_display = f\"{package} ({version})\"\n",
    "        except:\n",
    "            status = \"‚ö†Ô∏è\"\n",
    "            pkg_display = package\n",
    "    else:\n",
    "        status = \"‚ùå\"\n",
    "        missing_deps.append(package)\n",
    "        pkg_display = package\n",
    "    \n",
    "    print(f\"‚îÇ {status} {pkg_display:<63}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DIRECTORY STRUCTURE\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ Directory Structure \" + \"‚îÄ\" * 46 + \"‚îê\")\n",
    "\n",
    "required_dirs = {\n",
    "    'training': nordiq_root / 'training',\n",
    "    'models': nordiq_root / 'models',\n",
    "    'checkpoints': nordiq_root / 'checkpoints',\n",
    "    'logs': nordiq_root / 'logs'\n",
    "}\n",
    "\n",
    "for name, path in required_dirs.items():\n",
    "    exists = path.exists()\n",
    "    status = \"‚úÖ\" if exists else \"‚ö†Ô∏è\"\n",
    "    existence = \"exists\" if exists else \"will be created\"\n",
    "    print(f\"‚îÇ {status} {name + '/':20} {existence:<44}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. EXISTING MODELS CHECK\n",
    "# ============================================================================\n",
    "print(\"‚îå‚îÄ Existing Models \" + \"‚îÄ\" * 50 + \"‚îê\")\n",
    "\n",
    "models_dir = nordiq_root / 'models'\n",
    "if models_dir.exists():\n",
    "    model_dirs = sorted(models_dir.glob('tft_model_*'), reverse=True)\n",
    "    \n",
    "    if model_dirs:\n",
    "        print(f\"‚îÇ Found {len(model_dirs)} trained model(s):{' ' * 40}‚îÇ\")\n",
    "        for i, model_dir in enumerate(model_dirs[:3], 1):  # Show last 3\n",
    "            model_name = model_dir.name\n",
    "            model_size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file()) / (1024**2)\n",
    "            print(f\"‚îÇ   {i}. {model_name:<40} ({model_size:>6.1f} MB) ‚îÇ\")\n",
    "        if len(model_dirs) > 3:\n",
    "            print(f\"‚îÇ   ... and {len(model_dirs) - 3} more{' ' * 44}‚îÇ\")\n",
    "    else:\n",
    "        print(\"‚îÇ No trained models found - ready for first training{' ' * 16}‚îÇ\")\n",
    "else:\n",
    "    print(\"‚îÇ Models directory will be created on first training{' ' * 16}‚îÇ\")\n",
    "\n",
    "print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OVERALL READINESS\n",
    "# ============================================================================\n",
    "print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "\n",
    "all_critical_deps = all(dep in installed_deps for dep in ['torch', 'lightning', 'pandas', 'pytorch_forecasting'])\n",
    "\n",
    "if all_critical_deps and torch_cuda_enabled:\n",
    "    print(\"‚ïë\" + \" \" * 15 + \"‚úÖ SYSTEM READY FOR TRAINING\" + \" \" * 24 + \"‚ïë\")\n",
    "    print(\"‚ïë\" + \" \" * 15 + f\"Estimated: 10 epochs ‚âà 3-5 hours on {gpu_name[:20]}\" + \" \" * (12 - len(gpu_name[:20])) + \"‚ïë\")\n",
    "elif all_critical_deps and pytorch_installed and not torch_cuda_enabled:\n",
    "    print(\"‚ïë\" + \" \" * 10 + \"‚ö†Ô∏è  PYTORCH INSTALLED WITHOUT CUDA SUPPORT\" + \" \" * 16 + \"‚ïë\")\n",
    "    print(\"‚ïë\" + \" \" * 15 + \"Training will be 20-40x slower on CPU\" + \" \" * 15 + \"‚ïë\")\n",
    "    print(\"‚ïë\" + \" \" * 15 + \"Estimated: 10 epochs ‚âà 20-40 hours\" + \" \" * 19 + \"‚ïë\")\n",
    "else:\n",
    "    print(\"‚ïë\" + \" \" * 12 + \"‚ùå MISSING DEPENDENCIES - INSTALL FIRST\" + \" \" * 17 + \"‚ïë\")\n",
    "    if missing_deps:\n",
    "        print(\"‚ïë\" + \" \" * 15 + f\"Missing: {', '.join(missing_deps)}\" + \" \" * (53 - len(', '.join(missing_deps))) + \"‚ïë\")\n",
    "\n",
    "print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "print()\n",
    "\n",
    "# Clean up\n",
    "if not all_critical_deps:\n",
    "    print(\"‚ö†Ô∏è  Install missing packages:\")\n",
    "    print(\"   pip install torch lightning pandas pytorch-forecasting fastapi plotly dash\")\n",
    "    print()\n",
    "elif pytorch_installed and not torch_cuda_enabled:\n",
    "    print(\"üí° GPU training available but PyTorch lacks CUDA support\")\n",
    "    print(\"   Reinstall PyTorch with CUDA:\")\n",
    "    print(\"   pip uninstall torch\")\n",
    "    print(\"   pip install torch --index-url https://download.pytorch.org/whl/cu121\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_production",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Generation\n",
    "\n",
    "Creates realistic server metrics with:\n",
    "- 7 server profiles (ML, DB, Web, Conductor, ETL, Risk, Generic)\n",
    "- Financial market hours patterns\n",
    "- 14 LINBORG-compatible metrics\n",
    "\n",
    "**Adjust parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Generate Training Dataset\n",
    "# Expected time: 24h=30-60s | 720h=15-20min (optimized with parallelization)\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Add src/ to Python path (works from either root or NordIQ directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    # Notebook is in NordIQ folder\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - SIMPLE TWO-PARAMETER SETUP\n",
    "# ============================================\n",
    "\n",
    "TRAINING_HOURS = 168        # Duration: 24 (1 day), 168 (1 week), 720 (30 days - production)\n",
    "TOTAL_SERVERS = 90         # Fleet size: 20 (demo), 90 (default), 400 (production)\n",
    "\n",
    "# Servers are AUTO-DISTRIBUTED across 7 profiles:\n",
    "#   - Web/API:       28% (user-facing services)\n",
    "#   - ML Compute:    22% (training workloads)\n",
    "#   - Database:      17% (critical infrastructure)\n",
    "#   - Data Ingest:   11% (ETL pipelines)\n",
    "#   - Risk Analytics: 9% (EOD calculations)\n",
    "#   - Generic:        7% (utility, max 10)\n",
    "#   - Conductor:      6% (orchestration)\n",
    "\n",
    "TRAINING_DIR = str(nordiq_root / 'training')\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f\"üè¢ Dataset Generation\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"‚öôÔ∏è  Configuration:\")\n",
    "print(f\"   Duration: {TRAINING_HOURS} hours ({TRAINING_HOURS/24:.1f} days)\")\n",
    "print(f\"   Fleet size: {TOTAL_SERVERS} servers (auto-distributed across 7 profiles)\")\n",
    "print(f\"   Output: {TRAINING_DIR}\")\n",
    "\n",
    "# Show expected distribution\n",
    "print(f\"\\nüìä Expected Profile Distribution:\")\n",
    "dist = {\n",
    "    'Web/API': int(TOTAL_SERVERS * 0.28),\n",
    "    'ML Compute': int(TOTAL_SERVERS * 0.22),\n",
    "    'Database': int(TOTAL_SERVERS * 0.17),\n",
    "    'Data Ingest': int(TOTAL_SERVERS * 0.11),\n",
    "    'Risk Analytics': int(TOTAL_SERVERS * 0.09),\n",
    "    'Generic': min(int(TOTAL_SERVERS * 0.07), 10),\n",
    "    'Conductor': int(TOTAL_SERVERS * 0.06)\n",
    "}\n",
    "for profile, count in dist.items():\n",
    "    print(f\"   {profile:<15} ~{count:>3} servers\")\n",
    "\n",
    "# Estimate rows and time\n",
    "expected_timestamps = TRAINING_HOURS * 3600 // 5  # 5-second intervals\n",
    "expected_rows = expected_timestamps * TOTAL_SERVERS\n",
    "print(f\"\\nüìà Expected Output:\")\n",
    "print(f\"   ~{expected_rows:,} rows ({expected_timestamps:,} timestamps √ó {TOTAL_SERVERS} servers)\")\n",
    "print(f\"   Parallelized generation: ~{TRAINING_HOURS // 24 * 2 + 1}-{TRAINING_HOURS // 24 * 4 + 2} minutes\")\n",
    "print()\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "# Import and run generator\n",
    "from generators.metrics_generator import main as generate_metrics\n",
    "\n",
    "# Set up command-line arguments - SIMPLE: just --servers and --hours\n",
    "old_argv = sys.argv\n",
    "sys.argv = [\n",
    "    'metrics_generator.py',\n",
    "    '--hours', str(TRAINING_HOURS),\n",
    "    '--servers', str(TOTAL_SERVERS),  # Auto-distributes across profiles!\n",
    "    '--out_dir', TRAINING_DIR,\n",
    "    '--format', 'parquet'\n",
    "]\n",
    "\n",
    "try:\n",
    "    generate_metrics()\n",
    "    print(\"\\n‚úÖ Dataset generation complete!\")\n",
    "    success = True\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Generation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    success = False\n",
    "finally:\n",
    "    sys.argv = old_argv\n",
    "\n",
    "_elapsed = time.time() - _start\n",
    "_mins = int(_elapsed // 60)\n",
    "_secs = int(_elapsed % 60)\n",
    "print(f\"\\n‚è±Ô∏è  Execution time: {_mins}m {_secs}s\")\n",
    "\n",
    "if success:\n",
    "    # Show what was created\n",
    "    training_path = Path(TRAINING_DIR)\n",
    "    parquet_files = list(training_path.glob(\"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        latest = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "        df = pd.read_parquet(latest)\n",
    "        \n",
    "        print(f\"\\nüìä Dataset Summary:\")\n",
    "        print(f\"   File: {latest.name}\")\n",
    "        print(f\"   Size: {latest.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"   Records: {len(df):,}\")\n",
    "        print(f\"   Servers: {df['server_name'].nunique()}\")\n",
    "        \n",
    "        # Show actual profile distribution\n",
    "        if 'profile' in df.columns:\n",
    "            profile_counts = df.groupby('profile')['server_name'].nunique()\n",
    "            print(f\"\\n   Profile Distribution:\")\n",
    "            for profile, count in profile_counts.sort_values(ascending=False).items():\n",
    "                print(f\"     {profile:<20} {count:>3} servers\")\n",
    "        \n",
    "        print(f\"\\n   Time span: {(df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "        print(f\"\\nüéØ Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m50na52xq5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Explorer\n",
    "\n",
    "Executive-level dataset analysis and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6ny7cku4y",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset Explorer - Executive Presentation View\n",
    "# Professional analysis with visualizations suitable for C-suite presentations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_root / 'src') not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_root / 'src'))\n",
    "\n",
    "# Plotting imports\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Plotly not available - visualizations disabled\")\n",
    "    print(\"   Install: pip install plotly\")\n",
    "\n",
    "# Find the most recent dataset\n",
    "training_dir = nordiq_root / 'training'\n",
    "parquet_files = list(training_dir.glob(\"*.parquet\"))\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"‚ùå No dataset found. Please run the Dataset Generation cell first.\")\n",
    "else:\n",
    "    latest_file = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "    \n",
    "    print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "    print(\"‚ïë\" + \" \" * 18 + \"DATASET ANALYSIS REPORT\" + \" \" * 27 + \"‚ïë\")\n",
    "    print(\"‚ïë\" + \" \" * 15 + \"ArgusAI Predictive Monitoring\" + \" \" * 24 + \"‚ïë\")\n",
    "    print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "    print()\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"üìÇ Loading dataset: {latest_file.name}\")\n",
    "    df = pd.read_parquet(latest_file)\n",
    "    print(f\"‚úÖ Loaded {len(df):,} records\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXECUTIVE SUMMARY\n",
    "    # ========================================================================\n",
    "    print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "    print(\"‚ïë\" + \" \" * 22 + \"EXECUTIVE SUMMARY\" + \" \" * 29 + \"‚ïë\")\n",
    "    print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "    print()\n",
    "    \n",
    "    file_size_mb = latest_file.stat().st_size / (1024 * 1024)\n",
    "    time_span = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600\n",
    "    num_servers = df['server_name'].nunique()\n",
    "    num_profiles = df['profile'].nunique() if 'profile' in df.columns else 0\n",
    "    records_per_hour = len(df) / time_span if time_span > 0 else 0\n",
    "    \n",
    "    print(f\"‚îå‚îÄ Dataset Metrics \" + \"‚îÄ\" * 50 + \"‚îê\")\n",
    "    print(f\"‚îÇ Total Records:          {len(df):>12,} samples{' ' * 24}‚îÇ\")\n",
    "    print(f\"‚îÇ File Size:              {file_size_mb:>12.1f} MB{' ' * 27}‚îÇ\")\n",
    "    print(f\"‚îÇ Time Span:              {time_span:>12.1f} hours ({time_span/24:.1f} days){' ' * 13}‚îÇ\")\n",
    "    print(f\"‚îÇ Sampling Rate:          {records_per_hour:>12.1f} records/hour{' ' * 16}‚îÇ\")\n",
    "    print(f\"‚îÇ Date Range:             {df['timestamp'].min().strftime('%Y-%m-%d %H:%M'):<33}‚îÇ\")\n",
    "    print(f\"‚îÇ                    to   {df['timestamp'].max().strftime('%Y-%m-%d %H:%M'):<33}‚îÇ\")\n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FLEET COMPOSITION\n",
    "    # ========================================================================\n",
    "    print(f\"‚îå‚îÄ Fleet Composition \" + \"‚îÄ\" * 48 + \"‚îê\")\n",
    "    print(f\"‚îÇ Total Servers:          {num_servers:>12} servers{' ' * 25}‚îÇ\")\n",
    "    \n",
    "    if 'profile' in df.columns:\n",
    "        print(f\"‚îÇ Server Profiles:        {num_profiles:>12} types{' ' * 27}‚îÇ\")\n",
    "        print(f\"‚îÇ{' ' * 68}‚îÇ\")\n",
    "        \n",
    "        profile_counts = df.groupby('profile')['server_name'].nunique().sort_values(ascending=False)\n",
    "        for profile, count in profile_counts.items():\n",
    "            pct = (count / num_servers) * 100\n",
    "            bar_length = int(pct / 2)  # Scale to 50 chars max\n",
    "            bar = \"‚ñà\" * bar_length + \"‚ñë\" * (50 - bar_length)\n",
    "            print(f\"‚îÇ  {profile[:20]:<20} {count:>3} ({pct:>5.1f}%) ‚îÇ\")\n",
    "    \n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # METRICS COVERAGE\n",
    "    # ========================================================================\n",
    "    print(f\"‚îå‚îÄ Metrics Coverage \" + \"‚îÄ\" * 49 + \"‚îê\")\n",
    "    \n",
    "    from core.nordiq_metrics import NORDIQ_METRICS\n",
    "    \n",
    "    available_metrics = [m for m in NORDIQ_METRICS if m in df.columns]\n",
    "    coverage_pct = (len(available_metrics) / len(NORDIQ_METRICS)) * 100\n",
    "    \n",
    "    print(f\"‚îÇ LINBORG Metrics:        {len(available_metrics):>12} / {len(NORDIQ_METRICS)} ({coverage_pct:.0f}%){' ' * 20}‚îÇ\")\n",
    "    print(f\"‚îÇ{' ' * 68}‚îÇ\")\n",
    "    \n",
    "    # Group metrics by category\n",
    "    metric_categories = {\n",
    "        'CPU': ['cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct'],\n",
    "        'Memory': ['mem_used_pct', 'swap_used_pct'],\n",
    "        'Disk': ['disk_usage_pct'],\n",
    "        'Network': ['net_in_mb_s', 'net_out_mb_s'],\n",
    "        'Connections': ['back_close_wait', 'front_close_wait'],\n",
    "        'System': ['load_average', 'uptime_days']\n",
    "    }\n",
    "    \n",
    "    for category, metrics in metric_categories.items():\n",
    "        category_available = [m for m in metrics if m in df.columns]\n",
    "        cat_pct = (len(category_available) / len(metrics)) * 100\n",
    "        status = \"‚úÖ\" if cat_pct == 100 else \"‚ö†Ô∏è\" if cat_pct > 0 else \"‚ùå\"\n",
    "        print(f\"‚îÇ  {status} {category:<15} {len(category_available):>2}/{len(metrics)} metrics ({cat_pct:>5.1f}%){' ' * 25}‚îÇ\")\n",
    "    \n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DATA QUALITY METRICS\n",
    "    # ========================================================================\n",
    "    print(f\"‚îå‚îÄ Data Quality \" + \"‚îÄ\" * 53 + \"‚îê\")\n",
    "    \n",
    "    total_cells = len(df) * len(available_metrics)\n",
    "    missing_cells = df[available_metrics].isna().sum().sum()\n",
    "    completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
    "    \n",
    "    print(f\"‚îÇ Completeness:           {completeness:>12.2f}%{' ' * 28}‚îÇ\")\n",
    "    print(f\"‚îÇ Missing Values:         {missing_cells:>12,} cells{' ' * 24}‚îÇ\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated(subset=['timestamp', 'server_name']).sum()\n",
    "    duplicate_pct = (duplicates / len(df)) * 100\n",
    "    print(f\"‚îÇ Duplicate Records:      {duplicates:>12,} ({duplicate_pct:.2f}%){' ' * 20}‚îÇ\")\n",
    "    \n",
    "    # Temporal consistency\n",
    "    if 'timestamp' in df.columns:\n",
    "        df_sorted = df.sort_values(['server_name', 'timestamp'])\n",
    "        time_diffs = df_sorted.groupby('server_name')['timestamp'].diff()\n",
    "        median_interval = time_diffs.median().total_seconds() / 60\n",
    "        print(f\"‚îÇ Sampling Interval:      {median_interval:>12.1f} minutes (median){' ' * 14}‚îÇ\")\n",
    "    \n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STATISTICAL SUMMARY\n",
    "    # ========================================================================\n",
    "    print(f\"‚îå‚îÄ Key Metrics Statistics \" + \"‚îÄ\" * 43 + \"‚îê\")\n",
    "    print(f\"‚îÇ {'Metric':<20} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10} ‚îÇ\")\n",
    "    print(f\"‚îÇ {'-'*20} {'-'*10} {'-'*10} {'-'*10} {'-'*10} ‚îÇ\")\n",
    "    \n",
    "    key_metrics = ['cpu_user_pct', 'mem_used_pct', 'disk_usage_pct', 'load_average']\n",
    "    for metric in key_metrics:\n",
    "        if metric in df.columns:\n",
    "            stats = df[metric].describe()\n",
    "            print(f\"‚îÇ {metric:<20} {stats['mean']:>10.2f} {stats['std']:>10.2f} {stats['min']:>10.2f} {stats['max']:>10.2f} ‚îÇ\")\n",
    "    \n",
    "    print(\"‚îî\" + \"‚îÄ\" * 68 + \"‚îò\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VISUALIZATIONS (Executive Charts)\n",
    "    # ========================================================================\n",
    "    if PLOTLY_AVAILABLE:\n",
    "        print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "        print(\"‚ïë\" + \" \" * 20 + \"EXECUTIVE VISUALIZATIONS\" + \" \" * 24 + \"‚ïë\")\n",
    "        print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "        print()\n",
    "        \n",
    "        # 1. Fleet Distribution by Profile\n",
    "        if 'profile' in df.columns:\n",
    "            fig_fleet = px.pie(\n",
    "                profile_counts.reset_index(), \n",
    "                values='server_name', \n",
    "                names='profile',\n",
    "                title='Fleet Distribution by Server Profile',\n",
    "                color_discrete_sequence=px.colors.qualitative.Set3\n",
    "            )\n",
    "            fig_fleet.update_layout(\n",
    "                font=dict(size=14),\n",
    "                showlegend=True,\n",
    "                height=500\n",
    "            )\n",
    "            fig_fleet.show()\n",
    "        \n",
    "        # 2. Resource Utilization Heatmap\n",
    "        if all(m in df.columns for m in ['cpu_user_pct', 'mem_used_pct', 'disk_usage_pct']):\n",
    "            # Sample data for heatmap (aggregate by hour and profile)\n",
    "            df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "            \n",
    "            if 'profile' in df.columns:\n",
    "                heatmap_data = df.groupby(['hour', 'profile'])['cpu_user_pct'].mean().reset_index()\n",
    "                heatmap_pivot = heatmap_data.pivot(index='profile', columns='hour', values='cpu_user_pct')\n",
    "                \n",
    "                fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "                    z=heatmap_pivot.values,\n",
    "                    x=heatmap_pivot.columns,\n",
    "                    y=heatmap_pivot.index,\n",
    "                    colorscale='RdYlGn_r',\n",
    "                    text=heatmap_pivot.values.round(1),\n",
    "                    texttemplate='%{text}%',\n",
    "                    textfont={\"size\": 10},\n",
    "                    colorbar=dict(title=\"CPU %\")\n",
    "                ))\n",
    "                \n",
    "                fig_heatmap.update_layout(\n",
    "                    title='CPU Utilization Patterns by Profile and Hour of Day',\n",
    "                    xaxis_title='Hour of Day',\n",
    "                    yaxis_title='Server Profile',\n",
    "                    font=dict(size=12),\n",
    "                    height=400\n",
    "                )\n",
    "                fig_heatmap.show()\n",
    "        \n",
    "        # 3. Time Series Overview\n",
    "        if 'timestamp' in df.columns and 'cpu_user_pct' in df.columns:\n",
    "            # Sample every Nth point for performance\n",
    "            sample_size = min(10000, len(df))\n",
    "            df_sample = df.sample(n=sample_size).sort_values('timestamp')\n",
    "            \n",
    "            fig_ts = go.Figure()\n",
    "            \n",
    "            if 'profile' in df.columns:\n",
    "                for profile in df['profile'].unique()[:5]:  # Limit to 5 profiles\n",
    "                    profile_data = df_sample[df_sample['profile'] == profile]\n",
    "                    fig_ts.add_trace(go.Scatter(\n",
    "                        x=profile_data['timestamp'],\n",
    "                        y=profile_data['cpu_user_pct'],\n",
    "                        mode='markers',\n",
    "                        name=profile,\n",
    "                        marker=dict(size=3, opacity=0.6)\n",
    "                    ))\n",
    "            \n",
    "            fig_ts.update_layout(\n",
    "                title='CPU Utilization Over Time by Profile',\n",
    "                xaxis_title='Time',\n",
    "                yaxis_title='CPU User %',\n",
    "                font=dict(size=12),\n",
    "                height=500,\n",
    "                hovermode='closest'\n",
    "            )\n",
    "            fig_ts.show()\n",
    "        \n",
    "        print()\n",
    "        print(\"‚úÖ Executive visualizations generated successfully\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # READINESS ASSESSMENT\n",
    "    # ========================================================================\n",
    "    print(\"‚ïî\" + \"‚ïê\" * 68 + \"‚ïó\")\n",
    "    \n",
    "    # Determine readiness\n",
    "    min_required_records = 1000\n",
    "    min_required_servers = 5\n",
    "    min_completeness = 95.0\n",
    "    \n",
    "    is_ready = (\n",
    "        len(df) >= min_required_records and\n",
    "        num_servers >= min_required_servers and\n",
    "        completeness >= min_completeness and\n",
    "        len(available_metrics) >= 10\n",
    "    )\n",
    "    \n",
    "    if is_ready:\n",
    "        print(\"‚ïë\" + \" \" * 15 + \"‚úÖ DATASET READY FOR TRAINING\" + \" \" * 23 + \"‚ïë\")\n",
    "        print(\"‚ïë\" + \" \" * 15 + f\"{len(df):,} records | {num_servers} servers | {completeness:.1f}% complete\" + \" \" * (38 - len(f\"{len(df):,} records | {num_servers} servers | {completeness:.1f}% complete\")) + \"‚ïë\")\n",
    "    else:\n",
    "        print(\"‚ïë\" + \" \" * 12 + \"‚ö†Ô∏è  DATASET MAY NEED MORE DATA\" + \" \" * 26 + \"‚ïë\")\n",
    "        \n",
    "        if len(df) < min_required_records:\n",
    "            print(\"‚ïë\" + \" \" * 15 + f\"‚ö†Ô∏è  Only {len(df):,} records (recommend {min_required_records:,}+)\" + \" \" * (48 - len(f\"Only {len(df):,} records (recommend {min_required_records:,}+)\")) + \"‚ïë\")\n",
    "        if num_servers < min_required_servers:\n",
    "            print(\"‚ïë\" + \" \" * 15 + f\"‚ö†Ô∏è  Only {num_servers} servers (recommend {min_required_servers}+)\" + \" \" * (48 - len(f\"Only {num_servers} servers (recommend {min_required_servers}+)\")) + \"‚ïë\")\n",
    "        if completeness < min_completeness:\n",
    "            print(\"‚ïë\" + \" \" * 15 + f\"‚ö†Ô∏è  {completeness:.1f}% complete (recommend {min_completeness}%+)\" + \" \" * (48 - len(f\"{completeness:.1f}% complete (recommend {min_completeness}%+)\")) + \"‚ïë\")\n",
    "    \n",
    "    print(\"‚ïö\" + \"‚ïê\" * 68 + \"‚ïù\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_training",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Trains the Temporal Fusion Transformer with:\n",
    "- Profile-based transfer learning\n",
    "- GPU acceleration (if available)\n",
    "- Early stopping to prevent overfitting\n",
    "\n",
    "**Adjust parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Train TFT Model\n# Expected time: 10 epochs=3-5h | 20 epochs=6-10h\n# STREAMING MODE: ~10x less memory usage for large datasets\n\nimport sys\nimport os\nimport time\nfrom pathlib import Path\n\n# Add src/ to Python path (works from either root or NordIQ directory)\ncurrent_dir = Path.cwd()\nif current_dir.name == 'NordIQ':\n    # Notebook is in NordIQ folder\n    nordiq_src = (current_dir / 'src').absolute()\n    nordiq_root = current_dir\nelse:\n    # Notebook is in root folder\n    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n    nordiq_root = current_dir / 'NordIQ'\n\nif str(nordiq_src) not in sys.path:\n    sys.path.insert(0, str(nordiq_src))\n\n# ============================================\n# CONFIGURATION - ADJUST THESE VALUES\n# ============================================\n\nTRAINING_EPOCHS = 1       # Recommended: 10-20 epochs\n\n# STREAMING MODE: Use for large datasets (30+ days, 90+ servers)\n# - Loads time chunks one at a time instead of full dataset\n# - Memory: ~2-4 GB instead of 130+ GB\n# - Chunk size configured in model_config.py (streaming_chunk_hours)\nUSE_STREAMING_MODE = True  # Set to True for large datasets\n\n# CHUNK SIZE: Adjust in NordIQ/src/core/config/model_config.py\n# - 'streaming_chunk_hours': 2   (safest, ~2-4 GB memory)\n# - 'streaming_chunk_hours': 4   (faster, ~4-8 GB memory)\n# - 'streaming_chunk_hours': 8   (fastest, ~8-12 GB memory)\n# After changing, regenerate dataset to create new partitions!\n\n# IMPORTANT: Training must run from NordIQ directory for paths to work correctly\noriginal_dir = Path.cwd()\n\n# ============================================\n\nprint(f\"ü§ñ Model Training\")\nprint(\"-\" * 70)\nprint(f\"‚öôÔ∏è  Configuration:\")\nprint(f\"   Epochs: {TRAINING_EPOCHS}\")\nprint(f\"   Dataset: ./training/ (relative to NordIQ/)\")\nprint(f\"   Mode: {'STREAMING (memory-efficient)' if USE_STREAMING_MODE else 'Standard (full dataset in memory)'}\")\nprint()\n\nif USE_STREAMING_MODE:\n    # Show current chunk config\n    try:\n        from core.config.model_config import MODEL_CONFIG\n        chunk_hours = MODEL_CONFIG.get('streaming_chunk_hours', 2)\n    except:\n        chunk_hours = 2\n    \n    print(\"üì¶ Streaming Mode Details:\")\n    print(f\"   - Chunk size: {chunk_hours} hours (config: streaming_chunk_hours)\")\n    print(f\"   - Memory usage: ~{chunk_hours * 1.5:.0f}-{chunk_hours * 2:.0f} GB per chunk\")\n    print(\"   - Full dataset seen each epoch (all chunks processed)\")\n    print()\n\n# Estimate training time\nest_mins_low = TRAINING_EPOCHS * 20\nest_mins_high = TRAINING_EPOCHS * 30\nif USE_STREAMING_MODE:\n    est_mins_low = int(est_mins_low * 1.2)  # Slightly slower\n    est_mins_high = int(est_mins_high * 1.2)\nprint(f\"‚è±Ô∏è  Estimated time: {est_mins_low//60}h {est_mins_low%60}m - {est_mins_high//60}h {est_mins_high%60}m\")\nprint(f\"   (Based on ~20-30 minutes per epoch on RTX 4090)\")\nprint()\nprint(\"üöÄ Starting training...\")\nprint()\n\n_start = time.time()\n\n# Import and run trainer\nfrom training.tft_trainer import train_model\n\ntry:\n    # CRITICAL: Change to NordIQ directory before training\n    os.chdir(nordiq_root)\n    print(f\"[INFO] Working directory: {Path.cwd()}\")\n    \n    model_path = train_model(\n        dataset_path='./training/',\n        epochs=TRAINING_EPOCHS,\n        per_server=False,\n        streaming=USE_STREAMING_MODE\n    )\n    \n    if model_path:\n        print(\"\\n\" + \"=\" * 70)\n        print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n        print(\"=\" * 70)\n        print(f\"üìÅ Model saved: {model_path}\")\n        print()\n        print(\"üéØ Transfer Learning Enabled:\")\n        print(\"   ‚úÖ Model learned patterns for each server profile\")\n        print(\"   ‚úÖ New servers get strong predictions from day 1\")\n        print(\"   ‚úÖ No retraining needed when adding servers of known types\")\n        print()\n        print(\"üí° Next Steps:\")\n        print(\"   1. Start system: start_all.bat (Windows) or ./start_all.sh (Linux/Mac)\")\n        print(\"   2. Open dashboard: http://localhost:8050\")\n        print(\"   3. API endpoint: http://localhost:8000\")\n    else:\n        print(\"\\n‚ùå Training failed - check logs above\")\n        \nexcept Exception as e:\n    print(f\"\\n‚ùå Training error: {e}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    os.chdir(original_dir)\n    print(f\"\\n[INFO] Restored working directory: {Path.cwd()}\")\n\n_elapsed = time.time() - _start\n_hours = int(_elapsed // 3600)\n_mins = int((_elapsed % 3600) // 60)\n_secs = int(_elapsed % 60)\nprint(f\"\\n‚è±Ô∏è  Execution time: {_hours}h {_mins}m {_secs}s\")"
  },
  {
   "cell_type": "markdown",
   "id": "section_summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Training Complete!\n",
    "\n",
    "### What you've built:\n",
    "\n",
    "‚úÖ **Profile-Based Transfer Learning**\n",
    "- Model learned patterns for 7 server profiles\n",
    "- New servers get accurate predictions immediately\n",
    "- No retraining needed for known server types\n",
    "\n",
    "‚úÖ **Production-Ready System**\n",
    "- 8-hour forecast horizon (96 steps)\n",
    "- Quantile uncertainty estimates (p10, p50, p90)\n",
    "- 14 LINBORG-compatible metrics\n",
    "- Safetensors model format\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Launch the System:\n",
    "\n",
    "**Windows:**\n",
    "```bash\n",
    "cd NordIQ\n",
    "start_all.bat\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "cd NordIQ\n",
    "./start_all.sh\n",
    "```\n",
    "\n",
    "**Manual start (development):**\n",
    "```bash\n",
    "# Terminal 1 - Inference daemon\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python src/daemons/tft_inference_daemon.py --port 8000\n",
    "\n",
    "# Terminal 2 - Metrics generator\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python src/daemons/metrics_generator_daemon.py --stream --servers 20\n",
    "\n",
    "# Terminal 3 - Dashboard\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python dash_app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Access Points:\n",
    "\n",
    "- **Dashboard:** http://localhost:8050\n",
    "- **Inference API:** http://localhost:8000\n",
    "- **Metrics Generator API:** http://localhost:8001\n",
    "- **Health Check:** http://localhost:8000/health\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Documentation:\n",
    "\n",
    "- **[NordIQ/README.md](NordIQ/README.md)** - Complete system overview\n",
    "- **[NordIQ/Docs/SERVER_PROFILES.md](NordIQ/Docs/SERVER_PROFILES.md)** - 7 server profiles explained\n",
    "- **[NordIQ/Docs/GETTING_STARTED.md](NordIQ/Docs/GETTING_STARTED.md)** - Setup and configuration\n",
    "- **[Docs/ARCHITECTURE_GUIDE.md](Docs/ARCHITECTURE_GUIDE.md)** - System architecture and data contract\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Incremental Training:\n",
    "\n",
    "To add more training epochs later (recommended for continuous learning):\n",
    "\n",
    "```bash\n",
    "cd NordIQ\n",
    "python src/training/tft_trainer.py --epochs 5 --incremental\n",
    "```\n",
    "\n",
    "The system will add epochs to your existing model without starting over!\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Your predictive monitoring system is ready!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}