{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸš€ TFT Training Quick Start\n",
    "\n",
    "## Simplified workflow for dataset creation and model training\n",
    "\n",
    "This notebook does two things:\n",
    "1. **Generate training dataset** - Create realistic server metrics data\n",
    "2. **Train TFT model** - Train the Temporal Fusion Transformer\n",
    "\n",
    "The dashboard and inference daemon handle everything else!\n",
    "\n",
    "---\n",
    "\n",
    "**â±ï¸ Estimated Times:**\n",
    "- Dataset generation (24h): ~30-60 seconds\n",
    "- Dataset generation (720h): ~5-10 minutes\n",
    "- Model training (10 epochs): ~3-5 hours on RTX 4090\n",
    "\n",
    "**ğŸ¯ After Training:**\n",
    "- Start system: `start_all.bat` (Windows) or `./start_all.sh` (Linux/Mac)\n",
    "- Dashboard: http://localhost:8050\n",
    "- API: http://localhost:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ TFT Training System\n",
      "======================================================================\n",
      "âœ… Python path configured\n",
      "ğŸ“ NordIQ source: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ\\src\n",
      "ğŸ“ NordIQ root: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ\n",
      "\n",
      "ğŸ”§ Configuration:\n",
      "   Training directory: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ/training/\n",
      "   Models directory: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ/models/\n",
      "   Prediction horizon: 96 steps (8 hours)\n",
      "   Context length: 288 steps (24 hours)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to Python path (works from either root or NordIQ directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    # Notebook is in NordIQ folder\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "print(\"ğŸ¯ TFT Training System\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… Python path configured\")\n",
    "print(f\"ğŸ“ NordIQ source: {nordiq_src}\")\n",
    "print(f\"ğŸ“ NordIQ root: {nordiq_root}\")\n",
    "print(\"\\nğŸ”§ Configuration:\")\n",
    "print(f\"   Training directory: {nordiq_root}/training/\")\n",
    "print(f\"   Models directory: {nordiq_root}/models/\")\n",
    "print(\"   Prediction horizon: 96 steps (8 hours)\")\n",
    "print(\"   Context length: 288 steps (24 hours)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grw8o6ndz6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## System Health Check\n",
    "\n",
    "Verify your environment is ready for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ntc378ebr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    SYSTEM HEALTH CHECK                             â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”Œâ”€ Python Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Python Version:     3.10.16                                       â”‚\n",
      "â”‚ Platform:           Windows 10                                  â”‚\n",
      "â”‚ Architecture:       AMD64                                         â”‚\n",
      "â”‚ Working Directory:  D:\\Vibe_Projects\\MonitoringPrediction         â”‚\n",
      "â”‚ NordIQ Root:        D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€ GPU Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ âœ… GPU Detected:     NVIDIA GeForce RTX 4090                      â”‚\n",
      "â”‚    CUDA Version:     11.8                                          â”‚\n",
      "â”‚    Memory:           22.5 GB                                          â”‚\n",
      "â”‚    PyTorch CUDA:     Enabled                                        â”‚\n",
      "â”‚    Utilization:      3%                                           â”‚\n",
      "â”‚    Memory Used:      1780 MB / 23028 MB                            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€ Critical Dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ âœ… torch (2.0.1+cu118)                                            â”‚\n",
      "â”‚ âœ… lightning (2.0.2)                                              â”‚\n",
      "â”‚ âœ… pandas (2.2.2)                                                 â”‚\n",
      "â”‚ âœ… numpy (1.26.4)                                                 â”‚\n",
      "â”‚ âœ… pytorch_forecasting (1.0.0)                                    â”‚\n",
      "â”‚ âœ… fastapi (0.88.0)                                               â”‚\n",
      "â”‚ âœ… plotly (6.3.1)                                                 â”‚\n",
      "â”‚ âœ… dash (3.2.0)                                                   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€ Directory Structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ âœ… training/            exists                                      â”‚\n",
      "â”‚ âœ… models/              exists                                      â”‚\n",
      "â”‚ âš ï¸ checkpoints/         will be created                             â”‚\n",
      "â”‚ âœ… logs/                exists                                      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€ Existing Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ No trained models found - ready for first training{' ' * 16}â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘               âœ… SYSTEM READY FOR TRAINING                        â•‘\n",
      "â•‘               Estimated: 10 epochs â‰ˆ 3-5 hours on NVIDIA GeForce RTX 4â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Comprehensive System Check\n",
    "# Verify GPU, Python environment, dependencies, and system readiness\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "# Setup paths (same as Cell 1)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "print(\"â•‘\" + \" \" * 20 + \"SYSTEM HEALTH CHECK\" + \" \" * 29 + \"â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PYTHON ENVIRONMENT\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ Python Environment \" + \"â”€\" * 47 + \"â”\")\n",
    "print(f\"â”‚ Python Version:     {platform.python_version():<46}â”‚\")\n",
    "print(f\"â”‚ Platform:           {platform.system()} {platform.release():<36}â”‚\")\n",
    "print(f\"â”‚ Architecture:       {platform.machine():<46}â”‚\")\n",
    "\n",
    "# Working directory - handle long paths gracefully\n",
    "cwd = str(Path.cwd())\n",
    "if len(cwd) <= 45:\n",
    "    print(f\"â”‚ Working Directory:  {cwd:<46}â”‚\")\n",
    "else:\n",
    "    # Split long paths across multiple lines\n",
    "    print(f\"â”‚ Working Directory:                                          â”‚\")\n",
    "    # Show path in chunks of 60 characters\n",
    "    chunk_size = 60\n",
    "    for i in range(0, len(cwd), chunk_size):\n",
    "        chunk = cwd[i:i+chunk_size]\n",
    "        print(f\"â”‚   {chunk:<64}â”‚\")\n",
    "\n",
    "# Show NordIQ root detection\n",
    "nordiq_root_str = str(nordiq_root)\n",
    "if len(nordiq_root_str) <= 45:\n",
    "    print(f\"â”‚ NordIQ Root:        {nordiq_root_str:<46}â”‚\")\n",
    "else:\n",
    "    print(f\"â”‚ NordIQ Root:                                                â”‚\")\n",
    "    for i in range(0, len(nordiq_root_str), chunk_size):\n",
    "        chunk = nordiq_root_str[i:i+chunk_size]\n",
    "        print(f\"â”‚   {chunk:<64}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. GPU AVAILABILITY & PYTORCH CUDA CHECK\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ GPU Status \" + \"â”€\" * 54 + \"â”\")\n",
    "\n",
    "gpu_available = False\n",
    "gpu_name = \"Not available\"\n",
    "gpu_memory = 0\n",
    "cuda_version = \"N/A\"\n",
    "torch_cuda_enabled = False\n",
    "pytorch_installed = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    pytorch_installed = True\n",
    "    torch_cuda_enabled = torch.cuda.is_available()\n",
    "    gpu_available = torch_cuda_enabled\n",
    "    \n",
    "    if torch_cuda_enabled:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        cuda_version = torch.version.cuda\n",
    "        \n",
    "        print(f\"â”‚ âœ… GPU Detected:     {gpu_name[:45]:<45}â”‚\")\n",
    "        print(f\"â”‚    CUDA Version:     {cuda_version:<46}â”‚\")\n",
    "        print(f\"â”‚    Memory:           {gpu_memory:.1f} GB{' ' * 42}â”‚\")\n",
    "        print(f\"â”‚    PyTorch CUDA:     Enabled{' ' * 40}â”‚\")\n",
    "        \n",
    "        # GPU utilization\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n",
    "                                   '--format=csv,noheader,nounits'], \n",
    "                                  capture_output=True, text=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                gpu_util, mem_used, mem_total = result.stdout.strip().split(',')\n",
    "                print(f\"â”‚    Utilization:      {gpu_util.strip()}%{' ' * 43}â”‚\")\n",
    "                print(f\"â”‚    Memory Used:      {mem_used.strip()} MB / {mem_total.strip()} MB{' ' * 28}â”‚\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    else:\n",
    "        # PyTorch installed but CUDA not available\n",
    "        print(\"â”‚ âš ï¸  PyTorch installed but CUDA not enabled{' ' * 24}â”‚\")\n",
    "        print(f\"â”‚    PyTorch Version:  {torch.__version__:<46}â”‚\")\n",
    "        print(f\"â”‚    CUDA Built:       {torch.version.cuda if torch.version.cuda else 'No (CPU-only)':<46}â”‚\")\n",
    "        print(f\"â”‚    CUDA Available:   {torch_cuda_enabled}{' ' * 41}â”‚\")\n",
    "        print(\"â”‚{' ' * 68}â”‚\")\n",
    "        print(\"â”‚ âš ï¸  Training will use CPU (20-40x slower){' ' * 25}â”‚\")\n",
    "        print(\"â”‚    Expected time:    10 epochs â‰ˆ 20-40 hours{' ' * 21}â”‚\")\n",
    "        print(\"â”‚{' ' * 68}â”‚\")\n",
    "        print(\"â”‚ ğŸ’¡ To enable GPU:{' ' * 50}â”‚\")\n",
    "        print(\"â”‚    1. Verify NVIDIA GPU is present (nvidia-smi){' ' * 19}â”‚\")\n",
    "        print(\"â”‚    2. Install CUDA Toolkit (nvidia.com/cuda){' ' * 22}â”‚\")\n",
    "        print(\"â”‚    3. Reinstall PyTorch with CUDA:{' ' * 34}â”‚\")\n",
    "        print(\"â”‚       pip uninstall torch{' ' * 42}â”‚\")\n",
    "        print(\"â”‚       pip install torch --index-url{' ' * 30}â”‚\")\n",
    "        print(\"â”‚         https://download.pytorch.org/whl/cu121{' ' * 20}â”‚\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"â”‚ âŒ PyTorch not installed{' ' * 42}â”‚\")\n",
    "    print(\"â”‚{' ' * 68}â”‚\")\n",
    "    print(\"â”‚ Install with GPU support:{' ' * 42}â”‚\")\n",
    "    print(\"â”‚   pip install torch --index-url{' ' * 34}â”‚\")\n",
    "    print(\"â”‚     https://download.pytorch.org/whl/cu121{' ' * 24}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CRITICAL DEPENDENCIES\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ Critical Dependencies \" + \"â”€\" * 43 + \"â”\")\n",
    "\n",
    "dependencies = {\n",
    "    'torch': 'PyTorch (Deep Learning)',\n",
    "    'lightning': 'PyTorch Lightning (Training)',\n",
    "    'pandas': 'Pandas (Data Processing)',\n",
    "    'numpy': 'NumPy (Numerical Computing)',\n",
    "    'pytorch_forecasting': 'PyTorch Forecasting (TFT Model)',\n",
    "    'fastapi': 'FastAPI (Inference API)',\n",
    "    'plotly': 'Plotly (Dashboard)',\n",
    "    'dash': 'Dash (Dashboard Framework)'\n",
    "}\n",
    "\n",
    "missing_deps = []\n",
    "installed_deps = []\n",
    "\n",
    "for package, description in dependencies.items():\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is not None:\n",
    "        try:\n",
    "            module = importlib.import_module(package)\n",
    "            version = getattr(module, '__version__', 'unknown')\n",
    "            status = \"âœ…\"\n",
    "            installed_deps.append(package)\n",
    "            pkg_display = f\"{package} ({version})\"\n",
    "        except:\n",
    "            status = \"âš ï¸\"\n",
    "            pkg_display = package\n",
    "    else:\n",
    "        status = \"âŒ\"\n",
    "        missing_deps.append(package)\n",
    "        pkg_display = package\n",
    "    \n",
    "    print(f\"â”‚ {status} {pkg_display:<63}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DIRECTORY STRUCTURE\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ Directory Structure \" + \"â”€\" * 46 + \"â”\")\n",
    "\n",
    "required_dirs = {\n",
    "    'training': nordiq_root / 'training',\n",
    "    'models': nordiq_root / 'models',\n",
    "    'checkpoints': nordiq_root / 'checkpoints',\n",
    "    'logs': nordiq_root / 'logs'\n",
    "}\n",
    "\n",
    "for name, path in required_dirs.items():\n",
    "    exists = path.exists()\n",
    "    status = \"âœ…\" if exists else \"âš ï¸\"\n",
    "    existence = \"exists\" if exists else \"will be created\"\n",
    "    print(f\"â”‚ {status} {name + '/':20} {existence:<44}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. EXISTING MODELS CHECK\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ Existing Models \" + \"â”€\" * 50 + \"â”\")\n",
    "\n",
    "models_dir = nordiq_root / 'models'\n",
    "if models_dir.exists():\n",
    "    model_dirs = sorted(models_dir.glob('tft_model_*'), reverse=True)\n",
    "    \n",
    "    if model_dirs:\n",
    "        print(f\"â”‚ Found {len(model_dirs)} trained model(s):{' ' * 40}â”‚\")\n",
    "        for i, model_dir in enumerate(model_dirs[:3], 1):  # Show last 3\n",
    "            model_name = model_dir.name\n",
    "            model_size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file()) / (1024**2)\n",
    "            print(f\"â”‚   {i}. {model_name:<40} ({model_size:>6.1f} MB) â”‚\")\n",
    "        if len(model_dirs) > 3:\n",
    "            print(f\"â”‚   ... and {len(model_dirs) - 3} more{' ' * 44}â”‚\")\n",
    "    else:\n",
    "        print(\"â”‚ No trained models found - ready for first training{' ' * 16}â”‚\")\n",
    "else:\n",
    "    print(\"â”‚ Models directory will be created on first training{' ' * 16}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OVERALL READINESS\n",
    "# ============================================================================\n",
    "print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "\n",
    "all_critical_deps = all(dep in installed_deps for dep in ['torch', 'lightning', 'pandas', 'pytorch_forecasting'])\n",
    "\n",
    "if all_critical_deps and torch_cuda_enabled:\n",
    "    print(\"â•‘\" + \" \" * 15 + \"âœ… SYSTEM READY FOR TRAINING\" + \" \" * 24 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \" * 15 + f\"Estimated: 10 epochs â‰ˆ 3-5 hours on {gpu_name[:20]}\" + \" \" * (12 - len(gpu_name[:20])) + \"â•‘\")\n",
    "elif all_critical_deps and pytorch_installed and not torch_cuda_enabled:\n",
    "    print(\"â•‘\" + \" \" * 10 + \"âš ï¸  PYTORCH INSTALLED WITHOUT CUDA SUPPORT\" + \" \" * 16 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \" * 15 + \"Training will be 20-40x slower on CPU\" + \" \" * 15 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \" * 15 + \"Estimated: 10 epochs â‰ˆ 20-40 hours\" + \" \" * 19 + \"â•‘\")\n",
    "else:\n",
    "    print(\"â•‘\" + \" \" * 12 + \"âŒ MISSING DEPENDENCIES - INSTALL FIRST\" + \" \" * 17 + \"â•‘\")\n",
    "    if missing_deps:\n",
    "        print(\"â•‘\" + \" \" * 15 + f\"Missing: {', '.join(missing_deps)}\" + \" \" * (53 - len(', '.join(missing_deps))) + \"â•‘\")\n",
    "\n",
    "print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "print()\n",
    "\n",
    "# Clean up\n",
    "if not all_critical_deps:\n",
    "    print(\"âš ï¸  Install missing packages:\")\n",
    "    print(\"   pip install torch lightning pandas pytorch-forecasting fastapi plotly dash\")\n",
    "    print()\n",
    "elif pytorch_installed and not torch_cuda_enabled:\n",
    "    print(\"ğŸ’¡ GPU training available but PyTorch lacks CUDA support\")\n",
    "    print(\"   Reinstall PyTorch with CUDA:\")\n",
    "    print(\"   pip uninstall torch\")\n",
    "    print(\"   pip install torch --index-url https://download.pytorch.org/whl/cu121\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_production",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Generation\n",
    "\n",
    "Creates realistic server metrics with:\n",
    "- 7 server profiles (ML, DB, Web, Conductor, ETL, Risk, Generic)\n",
    "- Financial market hours patterns\n",
    "- 14 LINBORG-compatible metrics\n",
    "\n",
    "**Adjust parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¢ Dataset Generation\n",
      "----------------------------------------------------------------------\n",
      "âš™ï¸  Configuration:\n",
      "   Duration: 366 hours (15.2 days)\n",
      "   Fleet size: 45 servers (auto-distributed across 7 profiles)\n",
      "   Output: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ\\training\n",
      "\n",
      "ğŸ“Š Expected Profile Distribution:\n",
      "   Web/API         ~ 12 servers\n",
      "   ML Compute      ~  9 servers\n",
      "   Database        ~  7 servers\n",
      "   Data Ingest     ~  4 servers\n",
      "   Risk Analytics  ~  4 servers\n",
      "   Generic         ~  3 servers\n",
      "   Conductor       ~  2 servers\n",
      "\n",
      "ğŸ“ˆ Expected Output:\n",
      "   ~11,858,400 rows (263,520 timestamps Ã— 45 servers)\n",
      "   Parallelized generation: ~31-62 minutes\n",
      "\n",
      "âš ï¸  WARNING: Requested 366 hours, rounding up to 384 hours\n",
      "           Dataset generation aligned to 24-hour cycles (16 days)\n",
      "ğŸš€ Enhanced Fleet Telemetry Generator (OPTIMIZED)\n",
      "============================================================\n",
      "ğŸ“¡ Creating server fleet...\n",
      "â° Generating timestamp schedule...\n",
      "\n",
      "âš¡ Generating data for 45 servers Ã— 276,480 timestamps\n",
      "   Workers: 31\n",
      "   Processing servers...\n",
      "   [45/45] 100% complete - 12,441,600 rows generated\n",
      "\n",
      "ğŸ’¾ Concatenating 45 server datasets...\n",
      "   Sorting data...\n",
      "\n",
      "ğŸ“¦ Creating time-chunked partitions (2-hour slices)...\n",
      "   Created 193 time chunks (2 hours each)\n",
      "   Writing time-chunked Parquet...\n",
      "ğŸ“Š Time-chunked Parquet: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ\\training\\server_metrics_partitioned/ (12,441,600 rows, 193 chunks, 1261.8 MB)\n",
      "   Chunk manifest written: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ\\training\\server_metrics_partitioned/chunk_manifest.json\n",
      "   Skipping single Parquet (dataset too large, use time-chunked)\n",
      "\n",
      "â° Time Range:\n",
      "   Start: 2025-11-19 13:20:53.131456+00:00\n",
      "   End:   2025-12-05 13:20:48.131456+00:00\n",
      "   Duration: 15 days 23:59:55\n",
      "   Chunks: 193 Ã— 2 hours each\n",
      "\n",
      "ğŸ“‹ Generation Summary:\n",
      "   Time range: 2025-11-19 13:20:53.131456+00:00 to 2025-12-05 13:20:48.131456+00:00\n",
      "   Total duration: 384 hours (16 days)\n",
      "   Total timestamps: 276,480\n",
      "   Total rows: 12,441,600\n",
      "   Total servers: 45\n",
      "   Problem children: 4 (8.9%)\n",
      "\n",
      "ğŸ­ Scenario Injection (for training):\n",
      "   Servers with scenarios: ~30% (problem children + 20% random)\n",
      "   Est. degrading incidents: ~123 (4/week/server)\n",
      "   Est. critical incidents: ~46 (1.5/week/server)\n",
      "   Degrading ramp: 2 hours gradual climb to 75% CPU, 80% MEM\n",
      "   Critical ramp: 30 min rapid climb to 92% CPU, 95% MEM\n",
      "\n",
      "âš¡ Performance:\n",
      "   Generation time: 144.4 seconds (2.4 minutes)\n",
      "   Throughput: 86,149 rows/second\n",
      "\n",
      "âœ… Fleet telemetry generation complete!\n",
      "\n",
      "âœ… Dataset generation complete!\n",
      "\n",
      "â±ï¸  Execution time: 2m 24s\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Generate Training Dataset\n",
    "# Expected time: 24h=30-60s | 720h=15-20min (optimized with parallelization)\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Add src/ to Python path (works from either root or NordIQ directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    # Notebook is in NordIQ folder\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - SIMPLE TWO-PARAMETER SETUP\n",
    "# ============================================\n",
    "\n",
    "TRAINING_HOURS = 366        # Duration: 24 (1 day), 168 (1 week), 720 (30 days - production)\n",
    "TOTAL_SERVERS = 45         # Fleet size: 20 (demo), 90 (default), 400 (production)\n",
    "\n",
    "# Servers are AUTO-DISTRIBUTED across 7 profiles:\n",
    "#   - Web/API:       28% (user-facing services)\n",
    "#   - ML Compute:    22% (training workloads)\n",
    "#   - Database:      17% (critical infrastructure)\n",
    "#   - Data Ingest:   11% (ETL pipelines)\n",
    "#   - Risk Analytics: 9% (EOD calculations)\n",
    "#   - Generic:        7% (utility, max 10)\n",
    "#   - Conductor:      6% (orchestration)\n",
    "\n",
    "TRAINING_DIR = str(nordiq_root / 'training')\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f\"ğŸ¢ Dataset Generation\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"âš™ï¸  Configuration:\")\n",
    "print(f\"   Duration: {TRAINING_HOURS} hours ({TRAINING_HOURS/24:.1f} days)\")\n",
    "print(f\"   Fleet size: {TOTAL_SERVERS} servers (auto-distributed across 7 profiles)\")\n",
    "print(f\"   Output: {TRAINING_DIR}\")\n",
    "\n",
    "# Show expected distribution\n",
    "print(f\"\\nğŸ“Š Expected Profile Distribution:\")\n",
    "dist = {\n",
    "    'Web/API': int(TOTAL_SERVERS * 0.28),\n",
    "    'ML Compute': int(TOTAL_SERVERS * 0.22),\n",
    "    'Database': int(TOTAL_SERVERS * 0.17),\n",
    "    'Data Ingest': int(TOTAL_SERVERS * 0.11),\n",
    "    'Risk Analytics': int(TOTAL_SERVERS * 0.09),\n",
    "    'Generic': min(int(TOTAL_SERVERS * 0.07), 10),\n",
    "    'Conductor': int(TOTAL_SERVERS * 0.06)\n",
    "}\n",
    "for profile, count in dist.items():\n",
    "    print(f\"   {profile:<15} ~{count:>3} servers\")\n",
    "\n",
    "# Estimate rows and time\n",
    "expected_timestamps = TRAINING_HOURS * 3600 // 5  # 5-second intervals\n",
    "expected_rows = expected_timestamps * TOTAL_SERVERS\n",
    "print(f\"\\nğŸ“ˆ Expected Output:\")\n",
    "print(f\"   ~{expected_rows:,} rows ({expected_timestamps:,} timestamps Ã— {TOTAL_SERVERS} servers)\")\n",
    "print(f\"   Parallelized generation: ~{TRAINING_HOURS // 24 * 2 + 1}-{TRAINING_HOURS // 24 * 4 + 2} minutes\")\n",
    "print()\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "# Import and run generator\n",
    "from generators.metrics_generator import main as generate_metrics\n",
    "\n",
    "# Set up command-line arguments - SIMPLE: just --servers and --hours\n",
    "old_argv = sys.argv\n",
    "sys.argv = [\n",
    "    'metrics_generator.py',\n",
    "    '--hours', str(TRAINING_HOURS),\n",
    "    '--servers', str(TOTAL_SERVERS),  # Auto-distributes across profiles!\n",
    "    '--out_dir', TRAINING_DIR,\n",
    "    '--format', 'parquet'\n",
    "]\n",
    "\n",
    "try:\n",
    "    generate_metrics()\n",
    "    print(\"\\nâœ… Dataset generation complete!\")\n",
    "    success = True\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Generation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    success = False\n",
    "finally:\n",
    "    sys.argv = old_argv\n",
    "\n",
    "_elapsed = time.time() - _start\n",
    "_mins = int(_elapsed // 60)\n",
    "_secs = int(_elapsed % 60)\n",
    "print(f\"\\nâ±ï¸  Execution time: {_mins}m {_secs}s\")\n",
    "\n",
    "if success:\n",
    "    # Show what was created\n",
    "    training_path = Path(TRAINING_DIR)\n",
    "    parquet_files = list(training_path.glob(\"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        latest = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "        df = pd.read_parquet(latest)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Dataset Summary:\")\n",
    "        print(f\"   File: {latest.name}\")\n",
    "        print(f\"   Size: {latest.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"   Records: {len(df):,}\")\n",
    "        print(f\"   Servers: {df['server_name'].nunique()}\")\n",
    "        \n",
    "        # Show actual profile distribution\n",
    "        if 'profile' in df.columns:\n",
    "            profile_counts = df.groupby('profile')['server_name'].nunique()\n",
    "            print(f\"\\n   Profile Distribution:\")\n",
    "            for profile, count in profile_counts.sort_values(ascending=False).items():\n",
    "                print(f\"     {profile:<20} {count:>3} servers\")\n",
    "        \n",
    "        print(f\"\\n   Time span: {(df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "        print(f\"\\nğŸ¯ Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m50na52xq5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Explorer\n",
    "\n",
    "Executive-level dataset analysis and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6ny7cku4y",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset Explorer - Executive Presentation View\n",
    "# Professional analysis with visualizations suitable for C-suite presentations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_root / 'src') not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_root / 'src'))\n",
    "\n",
    "# Plotting imports\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"âš ï¸  Plotly not available - visualizations disabled\")\n",
    "    print(\"   Install: pip install plotly\")\n",
    "\n",
    "# Find the most recent dataset\n",
    "training_dir = nordiq_root / 'training'\n",
    "parquet_files = list(training_dir.glob(\"*.parquet\"))\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"âŒ No dataset found. Please run the Dataset Generation cell first.\")\n",
    "else:\n",
    "    latest_file = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "    \n",
    "    print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "    print(\"â•‘\" + \" \" * 18 + \"DATASET ANALYSIS REPORT\" + \" \" * 27 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \" * 15 + \"ArgusAI Predictive Monitoring\" + \" \" * 24 + \"â•‘\")\n",
    "    print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "    print()\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"ğŸ“‚ Loading dataset: {latest_file.name}\")\n",
    "    df = pd.read_parquet(latest_file)\n",
    "    print(f\"âœ… Loaded {len(df):,} records\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXECUTIVE SUMMARY\n",
    "    # ========================================================================\n",
    "    print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "    print(\"â•‘\" + \" \" * 22 + \"EXECUTIVE SUMMARY\" + \" \" * 29 + \"â•‘\")\n",
    "    print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "    print()\n",
    "    \n",
    "    file_size_mb = latest_file.stat().st_size / (1024 * 1024)\n",
    "    time_span = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600\n",
    "    num_servers = df['server_name'].nunique()\n",
    "    num_profiles = df['profile'].nunique() if 'profile' in df.columns else 0\n",
    "    records_per_hour = len(df) / time_span if time_span > 0 else 0\n",
    "    \n",
    "    print(f\"â”Œâ”€ Dataset Metrics \" + \"â”€\" * 50 + \"â”\")\n",
    "    print(f\"â”‚ Total Records:          {len(df):>12,} samples{' ' * 24}â”‚\")\n",
    "    print(f\"â”‚ File Size:              {file_size_mb:>12.1f} MB{' ' * 27}â”‚\")\n",
    "    print(f\"â”‚ Time Span:              {time_span:>12.1f} hours ({time_span/24:.1f} days){' ' * 13}â”‚\")\n",
    "    print(f\"â”‚ Sampling Rate:          {records_per_hour:>12.1f} records/hour{' ' * 16}â”‚\")\n",
    "    print(f\"â”‚ Date Range:             {df['timestamp'].min().strftime('%Y-%m-%d %H:%M'):<33}â”‚\")\n",
    "    print(f\"â”‚                    to   {df['timestamp'].max().strftime('%Y-%m-%d %H:%M'):<33}â”‚\")\n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FLEET COMPOSITION\n",
    "    # ========================================================================\n",
    "    print(f\"â”Œâ”€ Fleet Composition \" + \"â”€\" * 48 + \"â”\")\n",
    "    print(f\"â”‚ Total Servers:          {num_servers:>12} servers{' ' * 25}â”‚\")\n",
    "    \n",
    "    if 'profile' in df.columns:\n",
    "        print(f\"â”‚ Server Profiles:        {num_profiles:>12} types{' ' * 27}â”‚\")\n",
    "        print(f\"â”‚{' ' * 68}â”‚\")\n",
    "        \n",
    "        profile_counts = df.groupby('profile')['server_name'].nunique().sort_values(ascending=False)\n",
    "        for profile, count in profile_counts.items():\n",
    "            pct = (count / num_servers) * 100\n",
    "            bar_length = int(pct / 2)  # Scale to 50 chars max\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"â”‚  {profile[:20]:<20} {count:>3} ({pct:>5.1f}%) â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # METRICS COVERAGE\n",
    "    # ========================================================================\n",
    "    print(f\"â”Œâ”€ Metrics Coverage \" + \"â”€\" * 49 + \"â”\")\n",
    "    \n",
    "    from core.nordiq_metrics import NORDIQ_METRICS\n",
    "    \n",
    "    available_metrics = [m for m in NORDIQ_METRICS if m in df.columns]\n",
    "    coverage_pct = (len(available_metrics) / len(NORDIQ_METRICS)) * 100\n",
    "    \n",
    "    print(f\"â”‚ LINBORG Metrics:        {len(available_metrics):>12} / {len(NORDIQ_METRICS)} ({coverage_pct:.0f}%){' ' * 20}â”‚\")\n",
    "    print(f\"â”‚{' ' * 68}â”‚\")\n",
    "    \n",
    "    # Group metrics by category\n",
    "    metric_categories = {\n",
    "        'CPU': ['cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct'],\n",
    "        'Memory': ['mem_used_pct', 'swap_used_pct'],\n",
    "        'Disk': ['disk_usage_pct'],\n",
    "        'Network': ['net_in_mb_s', 'net_out_mb_s'],\n",
    "        'Connections': ['back_close_wait', 'front_close_wait'],\n",
    "        'System': ['load_average', 'uptime_days']\n",
    "    }\n",
    "    \n",
    "    for category, metrics in metric_categories.items():\n",
    "        category_available = [m for m in metrics if m in df.columns]\n",
    "        cat_pct = (len(category_available) / len(metrics)) * 100\n",
    "        status = \"âœ…\" if cat_pct == 100 else \"âš ï¸\" if cat_pct > 0 else \"âŒ\"\n",
    "        print(f\"â”‚  {status} {category:<15} {len(category_available):>2}/{len(metrics)} metrics ({cat_pct:>5.1f}%){' ' * 25}â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DATA QUALITY METRICS\n",
    "    # ========================================================================\n",
    "    print(f\"â”Œâ”€ Data Quality \" + \"â”€\" * 53 + \"â”\")\n",
    "    \n",
    "    total_cells = len(df) * len(available_metrics)\n",
    "    missing_cells = df[available_metrics].isna().sum().sum()\n",
    "    completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
    "    \n",
    "    print(f\"â”‚ Completeness:           {completeness:>12.2f}%{' ' * 28}â”‚\")\n",
    "    print(f\"â”‚ Missing Values:         {missing_cells:>12,} cells{' ' * 24}â”‚\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated(subset=['timestamp', 'server_name']).sum()\n",
    "    duplicate_pct = (duplicates / len(df)) * 100\n",
    "    print(f\"â”‚ Duplicate Records:      {duplicates:>12,} ({duplicate_pct:.2f}%){' ' * 20}â”‚\")\n",
    "    \n",
    "    # Temporal consistency\n",
    "    if 'timestamp' in df.columns:\n",
    "        df_sorted = df.sort_values(['server_name', 'timestamp'])\n",
    "        time_diffs = df_sorted.groupby('server_name')['timestamp'].diff()\n",
    "        median_interval = time_diffs.median().total_seconds() / 60\n",
    "        print(f\"â”‚ Sampling Interval:      {median_interval:>12.1f} minutes (median){' ' * 14}â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STATISTICAL SUMMARY\n",
    "    # ========================================================================\n",
    "    print(f\"â”Œâ”€ Key Metrics Statistics \" + \"â”€\" * 43 + \"â”\")\n",
    "    print(f\"â”‚ {'Metric':<20} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10} â”‚\")\n",
    "    print(f\"â”‚ {'-'*20} {'-'*10} {'-'*10} {'-'*10} {'-'*10} â”‚\")\n",
    "    \n",
    "    key_metrics = ['cpu_user_pct', 'mem_used_pct', 'disk_usage_pct', 'load_average']\n",
    "    for metric in key_metrics:\n",
    "        if metric in df.columns:\n",
    "            stats = df[metric].describe()\n",
    "            print(f\"â”‚ {metric:<20} {stats['mean']:>10.2f} {stats['std']:>10.2f} {stats['min']:>10.2f} {stats['max']:>10.2f} â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VISUALIZATIONS (Executive Charts)\n",
    "    # ========================================================================\n",
    "    if PLOTLY_AVAILABLE:\n",
    "        print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "        print(\"â•‘\" + \" \" * 20 + \"EXECUTIVE VISUALIZATIONS\" + \" \" * 24 + \"â•‘\")\n",
    "        print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "        print()\n",
    "        \n",
    "        # 1. Fleet Distribution by Profile\n",
    "        if 'profile' in df.columns:\n",
    "            fig_fleet = px.pie(\n",
    "                profile_counts.reset_index(), \n",
    "                values='server_name', \n",
    "                names='profile',\n",
    "                title='Fleet Distribution by Server Profile',\n",
    "                color_discrete_sequence=px.colors.qualitative.Set3\n",
    "            )\n",
    "            fig_fleet.update_layout(\n",
    "                font=dict(size=14),\n",
    "                showlegend=True,\n",
    "                height=500\n",
    "            )\n",
    "            fig_fleet.show()\n",
    "        \n",
    "        # 2. Resource Utilization Heatmap\n",
    "        if all(m in df.columns for m in ['cpu_user_pct', 'mem_used_pct', 'disk_usage_pct']):\n",
    "            # Sample data for heatmap (aggregate by hour and profile)\n",
    "            df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "            \n",
    "            if 'profile' in df.columns:\n",
    "                heatmap_data = df.groupby(['hour', 'profile'])['cpu_user_pct'].mean().reset_index()\n",
    "                heatmap_pivot = heatmap_data.pivot(index='profile', columns='hour', values='cpu_user_pct')\n",
    "                \n",
    "                fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "                    z=heatmap_pivot.values,\n",
    "                    x=heatmap_pivot.columns,\n",
    "                    y=heatmap_pivot.index,\n",
    "                    colorscale='RdYlGn_r',\n",
    "                    text=heatmap_pivot.values.round(1),\n",
    "                    texttemplate='%{text}%',\n",
    "                    textfont={\"size\": 10},\n",
    "                    colorbar=dict(title=\"CPU %\")\n",
    "                ))\n",
    "                \n",
    "                fig_heatmap.update_layout(\n",
    "                    title='CPU Utilization Patterns by Profile and Hour of Day',\n",
    "                    xaxis_title='Hour of Day',\n",
    "                    yaxis_title='Server Profile',\n",
    "                    font=dict(size=12),\n",
    "                    height=400\n",
    "                )\n",
    "                fig_heatmap.show()\n",
    "        \n",
    "        # 3. Time Series Overview\n",
    "        if 'timestamp' in df.columns and 'cpu_user_pct' in df.columns:\n",
    "            # Sample every Nth point for performance\n",
    "            sample_size = min(10000, len(df))\n",
    "            df_sample = df.sample(n=sample_size).sort_values('timestamp')\n",
    "            \n",
    "            fig_ts = go.Figure()\n",
    "            \n",
    "            if 'profile' in df.columns:\n",
    "                for profile in df['profile'].unique()[:5]:  # Limit to 5 profiles\n",
    "                    profile_data = df_sample[df_sample['profile'] == profile]\n",
    "                    fig_ts.add_trace(go.Scatter(\n",
    "                        x=profile_data['timestamp'],\n",
    "                        y=profile_data['cpu_user_pct'],\n",
    "                        mode='markers',\n",
    "                        name=profile,\n",
    "                        marker=dict(size=3, opacity=0.6)\n",
    "                    ))\n",
    "            \n",
    "            fig_ts.update_layout(\n",
    "                title='CPU Utilization Over Time by Profile',\n",
    "                xaxis_title='Time',\n",
    "                yaxis_title='CPU User %',\n",
    "                font=dict(size=12),\n",
    "                height=500,\n",
    "                hovermode='closest'\n",
    "            )\n",
    "            fig_ts.show()\n",
    "        \n",
    "        print()\n",
    "        print(\"âœ… Executive visualizations generated successfully\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # READINESS ASSESSMENT\n",
    "    # ========================================================================\n",
    "    print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "    \n",
    "    # Determine readiness\n",
    "    min_required_records = 1000\n",
    "    min_required_servers = 5\n",
    "    min_completeness = 95.0\n",
    "    \n",
    "    is_ready = (\n",
    "        len(df) >= min_required_records and\n",
    "        num_servers >= min_required_servers and\n",
    "        completeness >= min_completeness and\n",
    "        len(available_metrics) >= 10\n",
    "    )\n",
    "    \n",
    "    if is_ready:\n",
    "        print(\"â•‘\" + \" \" * 15 + \"âœ… DATASET READY FOR TRAINING\" + \" \" * 23 + \"â•‘\")\n",
    "        print(\"â•‘\" + \" \" * 15 + f\"{len(df):,} records | {num_servers} servers | {completeness:.1f}% complete\" + \" \" * (38 - len(f\"{len(df):,} records | {num_servers} servers | {completeness:.1f}% complete\")) + \"â•‘\")\n",
    "    else:\n",
    "        print(\"â•‘\" + \" \" * 12 + \"âš ï¸  DATASET MAY NEED MORE DATA\" + \" \" * 26 + \"â•‘\")\n",
    "        \n",
    "        if len(df) < min_required_records:\n",
    "            print(\"â•‘\" + \" \" * 15 + f\"âš ï¸  Only {len(df):,} records (recommend {min_required_records:,}+)\" + \" \" * (48 - len(f\"Only {len(df):,} records (recommend {min_required_records:,}+)\")) + \"â•‘\")\n",
    "        if num_servers < min_required_servers:\n",
    "            print(\"â•‘\" + \" \" * 15 + f\"âš ï¸  Only {num_servers} servers (recommend {min_required_servers}+)\" + \" \" * (48 - len(f\"Only {num_servers} servers (recommend {min_required_servers}+)\")) + \"â•‘\")\n",
    "        if completeness < min_completeness:\n",
    "            print(\"â•‘\" + \" \" * 15 + f\"âš ï¸  {completeness:.1f}% complete (recommend {min_completeness}%+)\" + \" \" * (48 - len(f\"{completeness:.1f}% complete (recommend {min_completeness}%+)\")) + \"â•‘\")\n",
    "    \n",
    "    print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_training",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Trains the Temporal Fusion Transformer with:\n",
    "- Profile-based transfer learning\n",
    "- GPU acceleration (if available)\n",
    "- Early stopping to prevent overfitting\n",
    "\n",
    "**Adjust parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Model Training\n",
      "----------------------------------------------------------------------\n",
      "âš™ï¸  Configuration:\n",
      "   Epochs: 1\n",
      "   Dataset: ./training/ (relative to NordIQ/)\n",
      "   Mode: STREAMING (memory-efficient)\n",
      "\n",
      "ğŸ“¦ Streaming Mode Details:\n",
      "   - Chunk size: 2 hours (config: streaming_chunk_hours)\n",
      "   - Memory usage: ~3-4 GB per chunk\n",
      "   - Full dataset seen each epoch (all chunks processed)\n",
      "\n",
      "â±ï¸  Estimated time: 0h 24m - 0h 36m\n",
      "   (Based on ~20-30 minutes per epoch on RTX 4090)\n",
      "\n",
      "ğŸš€ Starting training...\n",
      "\n",
      "[INFO] Working directory: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ\n",
      "[GPU] Detected: NVIDIA GeForce RTX 4090\n",
      "[GPU] Compute Capability: SM 8.9\n",
      "[GPU] Profile: RTX 4090\n",
      "[GPU] Consumer/Workstation GPU - Ada Lovelace architecture\n",
      "[GPU] Tensor Cores: Enabled (precision=medium)\n",
      "[GPU] cuDNN: benchmark=True, deterministic=False\n",
      "[GPU] Memory: 85% reserved\n",
      "[TRAIN] Starting STREAMING training mode (memory-efficient)...\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SYSTEM INFORMATION - Training Environment\n",
      "======================================================================\n",
      "Platform:        Windows 10\n",
      "Python:          3.10.16\n",
      "PyTorch:         2.0.1+cu118\n",
      "CUDA Available:  True\n",
      "CUDA Version:    11.8\n",
      "GPU:             NVIDIA GeForce RTX 4090\n",
      "GPU Memory:      22.5 GB\n",
      "GPU Utilization: 20%\n",
      "GPU Mem Used:    2143 MB / 23028 MB\n",
      "\n",
      "Key Dependencies:\n",
      "  Lightning:     2.0.2\n",
      "  PytorchFrcst:  1.0.0\n",
      "  Pandas:        2.2.2\n",
      "  NumPy:         1.26.4\n",
      "\n",
      "Training Configuration:\n",
      "  Batch Size:    128\n",
      "  Learning Rate: 0.01\n",
      "  Epochs:        1\n",
      "  Hidden Size:   128\n",
      "  Attention:     8 heads\n",
      "  Context:       288 timesteps\n",
      "  Horizon:       96 timesteps\n",
      "======================================================================\n",
      "\n",
      "[SEED] Random seed 42 (benchmark mode - faster)\n",
      "[STREAM] Found 193 time chunks (2 hours each)\n",
      "[STREAM] Will process 193 chunks per epoch\n",
      "[STREAM] Total epochs: 1\n",
      "[STREAM] Checkpoint every 5 chunks\n",
      "[STREAM] Memory mode: ~1 chunk in memory at a time\n",
      "======================================================================\n",
      "\n",
      "[INIT] Initializing model from first chunk...\n",
      "[CHUNK] Loaded 21,150 records from chunk 20251119_12\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (21150, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 470\n",
      "[INFO] Using encoder length: 156, prediction length: 47\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 376\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n",
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 19035\n",
      "[OK] Validation samples: 1305\n",
      "[OK] Model initialized with 1,542,740 parameters\n",
      "\n",
      "============================================================\n",
      "[EPOCH 1/1] Starting streaming epoch...\n",
      "============================================================\n",
      "\n",
      "[CHUNK 1/193] Loading 20251123_06...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251123_06\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fb12538aeb4c4b8da98a4af232d90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251123_06 done in 283.0s | Loss: 5.2412\n",
      "\n",
      "[CHUNK 2/193] Loading 20251130_22...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251130_22\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10b9d9ba0354cf7b1ef294943b6f150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251130_22 done in 284.9s | Loss: 6.2791\n",
      "\n",
      "[CHUNK 3/193] Loading 20251125_20...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251125_20\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b27abeda0f493b9bf0f6d8ebae3be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251125_20 done in 283.5s | Loss: 4.0565\n",
      "\n",
      "[CHUNK 4/193] Loading 20251201_12...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251201_12\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99245862fef3400c9d0a739870a4e373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251201_12 done in 281.9s | Loss: 5.1119\n",
      "\n",
      "[CHUNK 5/193] Loading 20251128_22...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251128_22\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7372ca36fba42b287bbfd210e902d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251128_22 done in 283.5s | Loss: 4.1792\n",
      "[CHECKPOINT] Saved at epoch 1, chunk 5 -> models\\streaming_checkpoint.pt\n",
      "\n",
      "[CHUNK 6/193] Loading 20251202_04...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251202_04\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01164149888c481ebafabbb9a9281f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251202_04 done in 283.0s | Loss: 3.3673\n",
      "\n",
      "[CHUNK 7/193] Loading 20251120_18...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251120_18\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7925016cb1bd4f92a8ad169c6d1270ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251120_18 done in 284.0s | Loss: 5.9711\n",
      "\n",
      "[CHUNK 8/193] Loading 20251121_00...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251121_00\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7521c56c01478289afb59108cc807d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251121_00 done in 282.4s | Loss: 3.8209\n",
      "\n",
      "[CHUNK 9/193] Loading 20251120_20...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251120_20\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ffbdf9e4364c5eab8ee89da987d687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cb9f74ab624914976bce2224ca916d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251120_20 done in 282.5s | Loss: 4.0872\n",
      "\n",
      "[CHUNK 10/193] Loading 20251120_06...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251120_06\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eed30ec957d483fa78ea117e639b3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e67317efa7544faa0f7370d2b40fe12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251120_06 done in 284.1s | Loss: 4.1746\n",
      "[CHECKPOINT] Saved at epoch 1, chunk 10 -> models\\streaming_checkpoint.pt\n",
      "\n",
      "[CHUNK 11/193] Loading 20251126_00...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251126_00\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a9872048c149a4bbf1804fd491c192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251126_00 done in 288.5s | Loss: 3.6707\n",
      "\n",
      "[CHUNK 12/193] Loading 20251128_20...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251128_20\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697a8f0ec89140a7938b6559db552799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251128_20 done in 306.9s | Loss: 4.3030\n",
      "\n",
      "[CHUNK 13/193] Loading 20251125_00...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251125_00\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f58389cda8478ebf8c728d32d4c5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251125_00 done in 293.4s | Loss: 3.5666\n",
      "\n",
      "[CHUNK 14/193] Loading 20251124_22...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251124_22\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febe4e9bdd6e4591ab2c9f7171f1954b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251124_22 done in 292.4s | Loss: 4.5153\n",
      "\n",
      "[CHUNK 15/193] Loading 20251202_08...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251202_08\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbf61e3ae474eaf9807aae9da8aed58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251202_08 done in 294.4s | Loss: 5.4711\n",
      "[CHECKPOINT] Saved at epoch 1, chunk 15 -> models\\streaming_checkpoint.pt\n",
      "\n",
      "[CHUNK 16/193] Loading 20251122_00...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251122_00\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb9ead2b3f144a29b928c9b2378a694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251122_00 done in 290.8s | Loss: 3.7279\n",
      "\n",
      "[CHUNK 17/193] Loading 20251201_16...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251201_16\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6961fbc7eb8f488e8fad17ca3a3f9155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNK] 20251201_16 done in 300.2s | Loss: 6.8074\n",
      "\n",
      "[CHUNK 18/193] Loading 20251125_06...\n",
      "[CHUNK] Loaded 64,800 records from chunk 20251125_06\n",
      "[PREP] Preparing data for TFT training...\n",
      "[INFO] Original columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes']\n",
      "[INFO] Using server column: server_name\n",
      "[OK] Encoded 45 server_names using hash-based encoding\n",
      "   Sample mappings: {'ppcon01': '285039', 'ppcon02': '849036', 'ppdb001': '215733'}\n",
      "[INFO] Available NordIQ Metrics Framework metrics: 14/14\n",
      "[OK] Data prepared: (64800, 26)\n",
      "[INFO] Final columns: ['timestamp', 'server_name', 'profile', 'status', 'problem_child', 'cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct', 'mem_used_pct', 'swap_used_pct', 'disk_usage_pct', 'net_in_mb_s', 'net_out_mb_s', 'back_close_wait', 'front_close_wait', 'load_average', 'uptime_days', 'notes', 'time_idx', 'hour', 'day_of_week', 'month', 'is_weekend', 'server_id']\n",
      "[INFO] Creating TimeSeriesDataSets...\n",
      "[INFO] Min series length: 1440\n",
      "[INFO] Using encoder length: 288, prediction length: 96\n",
      "[INFO] Validation split: 20.0% | Training cutoff: 1152\n",
      "[TRANSFER] Profile feature enabled - model will learn per-profile patterns\n",
      "[TRANSFER] Profiles detected: ['conductor_mgmt', 'data_ingest', 'database', 'generic', 'ml_compute', 'risk_analytics', 'web_api']\n",
      "[INFO] Single-target mode: cpu_user_pct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 85    \n",
      "3  | prescalers                         | ModuleDict                      | 2.8 K \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 78.4 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 517 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 131 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 37.1 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 903   \n",
      "----------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.171     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRANSFER] Model configured with profile-based transfer learning\n",
      "[TRANSFER] New servers will predict based on their profile patterns\n",
      "[INFO] Model configured to accept unknown servers and statuses\n",
      "[OK] Training samples: 56160\n",
      "[OK] Validation samples: 12825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5586ce79bac4695bc418226f0c44191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Train TFT Model\n",
    "# Expected time: 10 epochs=3-5h | 20 epochs=6-10h\n",
    "# STREAMING MODE: ~10x less memory usage for large datasets\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to Python path (works from either root or NordIQ directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    # Notebook is in NordIQ folder\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - ADJUST THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "TRAINING_EPOCHS = 1       # Recommended: 10-20 epochs\n",
    "\n",
    "# STREAMING MODE: Use for large datasets (30+ days, 90+ servers)\n",
    "# - Loads time chunks one at a time instead of full dataset\n",
    "# - Memory: ~2-4 GB instead of 130+ GB\n",
    "# - Chunk size configured in model_config.py (streaming_chunk_hours)\n",
    "USE_STREAMING_MODE = True  # Set to True for large datasets\n",
    "\n",
    "# CHUNK SIZE: Adjust in NordIQ/src/core/config/model_config.py\n",
    "# - 'streaming_chunk_hours': 2   (safest, ~2-4 GB memory)\n",
    "# - 'streaming_chunk_hours': 4   (faster, ~4-8 GB memory)\n",
    "# - 'streaming_chunk_hours': 8   (fastest, ~8-12 GB memory)\n",
    "# After changing, regenerate dataset to create new partitions!\n",
    "\n",
    "# IMPORTANT: Training must run from NordIQ directory for paths to work correctly\n",
    "original_dir = Path.cwd()\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f\"ğŸ¤– Model Training\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"âš™ï¸  Configuration:\")\n",
    "print(f\"   Epochs: {TRAINING_EPOCHS}\")\n",
    "print(f\"   Dataset: ./training/ (relative to NordIQ/)\")\n",
    "print(f\"   Mode: {'STREAMING (memory-efficient)' if USE_STREAMING_MODE else 'Standard (full dataset in memory)'}\")\n",
    "print()\n",
    "\n",
    "if USE_STREAMING_MODE:\n",
    "    # Show current chunk config\n",
    "    try:\n",
    "        from core.config.model_config import MODEL_CONFIG\n",
    "        chunk_hours = MODEL_CONFIG.get('streaming_chunk_hours', 2)\n",
    "    except:\n",
    "        chunk_hours = 2\n",
    "    \n",
    "    print(\"ğŸ“¦ Streaming Mode Details:\")\n",
    "    print(f\"   - Chunk size: {chunk_hours} hours (config: streaming_chunk_hours)\")\n",
    "    print(f\"   - Memory usage: ~{chunk_hours * 1.5:.0f}-{chunk_hours * 2:.0f} GB per chunk\")\n",
    "    print(\"   - Full dataset seen each epoch (all chunks processed)\")\n",
    "    print()\n",
    "\n",
    "# Estimate training time\n",
    "est_mins_low = TRAINING_EPOCHS * 20\n",
    "est_mins_high = TRAINING_EPOCHS * 30\n",
    "if USE_STREAMING_MODE:\n",
    "    est_mins_low = int(est_mins_low * 1.2)  # Slightly slower\n",
    "    est_mins_high = int(est_mins_high * 1.2)\n",
    "print(f\"â±ï¸  Estimated time: {est_mins_low//60}h {est_mins_low%60}m - {est_mins_high//60}h {est_mins_high%60}m\")\n",
    "print(f\"   (Based on ~20-30 minutes per epoch on RTX 4090)\")\n",
    "print()\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "print()\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "# Import and run trainer\n",
    "from training.tft_trainer import train_model\n",
    "\n",
    "try:\n",
    "    # CRITICAL: Change to NordIQ directory before training\n",
    "    os.chdir(nordiq_root)\n",
    "    print(f\"[INFO] Working directory: {Path.cwd()}\")\n",
    "    \n",
    "    model_path = train_model(\n",
    "        dataset_path='./training/',\n",
    "        epochs=TRAINING_EPOCHS,\n",
    "        per_server=False,\n",
    "        streaming=USE_STREAMING_MODE\n",
    "    )\n",
    "    \n",
    "    if model_path:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“ Model saved: {model_path}\")\n",
    "        print()\n",
    "        print(\"ğŸ¯ Transfer Learning Enabled:\")\n",
    "        print(\"   âœ… Model learned patterns for each server profile\")\n",
    "        print(\"   âœ… New servers get strong predictions from day 1\")\n",
    "        print(\"   âœ… No retraining needed when adding servers of known types\")\n",
    "        print()\n",
    "        print(\"ğŸ’¡ Next Steps:\")\n",
    "        print(\"   1. Start system: start_all.bat (Windows) or ./start_all.sh (Linux/Mac)\")\n",
    "        print(\"   2. Open dashboard: http://localhost:8050\")\n",
    "        print(\"   3. API endpoint: http://localhost:8000\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Training failed - check logs above\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    os.chdir(original_dir)\n",
    "    print(f\"\\n[INFO] Restored working directory: {Path.cwd()}\")\n",
    "\n",
    "_elapsed = time.time() - _start\n",
    "_hours = int(_elapsed // 3600)\n",
    "_mins = int((_elapsed % 3600) // 60)\n",
    "_secs = int(_elapsed % 60)\n",
    "print(f\"\\nâ±ï¸  Execution time: {_hours}h {_mins}m {_secs}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Training Complete!\n",
    "\n",
    "### What you've built:\n",
    "\n",
    "âœ… **Profile-Based Transfer Learning**\n",
    "- Model learned patterns for 7 server profiles\n",
    "- New servers get accurate predictions immediately\n",
    "- No retraining needed for known server types\n",
    "\n",
    "âœ… **Production-Ready System**\n",
    "- 8-hour forecast horizon (96 steps)\n",
    "- Quantile uncertainty estimates (p10, p50, p90)\n",
    "- 14 LINBORG-compatible metrics\n",
    "- Safetensors model format\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Launch the System:\n",
    "\n",
    "**Windows:**\n",
    "```bash\n",
    "cd NordIQ\n",
    "start_all.bat\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "cd NordIQ\n",
    "./start_all.sh\n",
    "```\n",
    "\n",
    "**Manual start (development):**\n",
    "```bash\n",
    "# Terminal 1 - Inference daemon\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python src/daemons/tft_inference_daemon.py --port 8000\n",
    "\n",
    "# Terminal 2 - Metrics generator\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python src/daemons/metrics_generator_daemon.py --stream --servers 20\n",
    "\n",
    "# Terminal 3 - Dashboard\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python dash_app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Access Points:\n",
    "\n",
    "- **Dashboard:** http://localhost:8050\n",
    "- **Inference API:** http://localhost:8000\n",
    "- **Metrics Generator API:** http://localhost:8001\n",
    "- **Health Check:** http://localhost:8000/health\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Documentation:\n",
    "\n",
    "- **[NordIQ/README.md](NordIQ/README.md)** - Complete system overview\n",
    "- **[NordIQ/Docs/SERVER_PROFILES.md](NordIQ/Docs/SERVER_PROFILES.md)** - 7 server profiles explained\n",
    "- **[NordIQ/Docs/GETTING_STARTED.md](NordIQ/Docs/GETTING_STARTED.md)** - Setup and configuration\n",
    "- **[Docs/ARCHITECTURE_GUIDE.md](Docs/ARCHITECTURE_GUIDE.md)** - System architecture and data contract\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Incremental Training:\n",
    "\n",
    "To add more training epochs later (recommended for continuous learning):\n",
    "\n",
    "```bash\n",
    "cd NordIQ\n",
    "python src/training/tft_trainer.py --epochs 5 --incremental\n",
    "```\n",
    "\n",
    "The system will add epochs to your existing model without starting over!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Your predictive monitoring system is ready!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
