{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸš€ TFT Training Quick Start\n",
    "\n",
    "## Simplified workflow for dataset creation and model training\n",
    "\n",
    "This notebook does two things:\n",
    "1. **Generate training dataset** - Create realistic server metrics data\n",
    "2. **Train TFT model** - Train the Temporal Fusion Transformer\n",
    "\n",
    "The dashboard and inference daemon handle everything else!\n",
    "\n",
    "---\n",
    "\n",
    "**â±ï¸ Estimated Times:**\n",
    "- Dataset generation (24h): ~30-60 seconds\n",
    "- Dataset generation (720h): ~5-10 minutes\n",
    "- Model training (10 epochs): ~3-5 hours on RTX 4090\n",
    "\n",
    "**ğŸ¯ After Training:**\n",
    "- Start system: `start_all.bat` (Windows) or `./start_all.sh` (Linux/Mac)\n",
    "- Dashboard: http://localhost:8050\n",
    "- API: http://localhost:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ TFT Training System\n",
      "======================================================================\n",
      "âœ… Python path configured\n",
      "ğŸ“ NordIQ source: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ\\src\n",
      "ğŸ“ NordIQ root: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ\n",
      "\n",
      "ğŸ”§ Configuration:\n",
      "   Training directory: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ/training/\n",
      "   Models directory: D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ/models/\n",
      "   Prediction horizon: 96 steps (8 hours)\n",
      "   Context length: 288 steps (24 hours)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to Python path (works from either root or NordIQ directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    # Notebook is in NordIQ folder\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "print(\"ğŸ¯ TFT Training System\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… Python path configured\")\n",
    "print(f\"ğŸ“ NordIQ source: {nordiq_src}\")\n",
    "print(f\"ğŸ“ NordIQ root: {nordiq_root}\")\n",
    "print(\"\\nğŸ”§ Configuration:\")\n",
    "print(f\"   Training directory: {nordiq_root}/training/\")\n",
    "print(f\"   Models directory: {nordiq_root}/models/\")\n",
    "print(\"   Prediction horizon: 96 steps (8 hours)\")\n",
    "print(\"   Context length: 288 steps (24 hours)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grw8o6ndz6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## System Health Check\n",
    "\n",
    "Verify your environment is ready for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ntc378ebr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                    SYSTEM HEALTH CHECK                             â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "â”Œâ”€ Python Environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Python Version:     3.10.16                                       â”‚\n",
      "â”‚ Platform:           Windows 10                                  â”‚\n",
      "â”‚ Architecture:       AMD64                                         â”‚\n",
      "â”‚ Working Directory:  D:\\Vibe_Projects\\MonitoringPrediction         â”‚\n",
      "â”‚ NordIQ Root:        D:\\Vibe_Projects\\MonitoringPrediction\\NordIQ  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€ GPU Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ âœ… GPU Detected:     NVIDIA GeForce RTX 4090                      â”‚\n",
      "â”‚    CUDA Version:     11.8                                          â”‚\n",
      "â”‚    Memory:           22.5 GB                                          â”‚\n",
      "â”‚    PyTorch CUDA:     Enabled                                        â”‚\n",
      "â”‚    Utilization:      2%                                           â”‚\n",
      "â”‚    Memory Used:      2048 MB / 23028 MB                            â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€ Critical Dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ âœ… torch (2.0.1+cu118)                                            â”‚\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Comprehensive System Check\n",
    "# Verify GPU, Python environment, dependencies, and system readiness\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "# Setup paths (same as Cell 1)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "print(\"â•‘\" + \" \" * 20 + \"SYSTEM HEALTH CHECK\" + \" \" * 29 + \"â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PYTHON ENVIRONMENT\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ Python Environment \" + \"â”€\" * 47 + \"â”\")\n",
    "print(f\"â”‚ Python Version:     {platform.python_version():<46}â”‚\")\n",
    "print(f\"â”‚ Platform:           {platform.system()} {platform.release():<36}â”‚\")\n",
    "print(f\"â”‚ Architecture:       {platform.machine():<46}â”‚\")\n",
    "\n",
    "# Working directory - handle long paths gracefully\n",
    "cwd = str(Path.cwd())\n",
    "if len(cwd) <= 45:\n",
    "    print(f\"â”‚ Working Directory:  {cwd:<46}â”‚\")\n",
    "else:\n",
    "    # Split long paths across multiple lines\n",
    "    print(f\"â”‚ Working Directory:                                          â”‚\")\n",
    "    # Show path in chunks of 60 characters\n",
    "    chunk_size = 60\n",
    "    for i in range(0, len(cwd), chunk_size):\n",
    "        chunk = cwd[i:i+chunk_size]\n",
    "        print(f\"â”‚   {chunk:<64}â”‚\")\n",
    "\n",
    "# Show NordIQ root detection\n",
    "nordiq_root_str = str(nordiq_root)\n",
    "if len(nordiq_root_str) <= 45:\n",
    "    print(f\"â”‚ NordIQ Root:        {nordiq_root_str:<46}â”‚\")\n",
    "else:\n",
    "    print(f\"â”‚ NordIQ Root:                                                â”‚\")\n",
    "    for i in range(0, len(nordiq_root_str), chunk_size):\n",
    "        chunk = nordiq_root_str[i:i+chunk_size]\n",
    "        print(f\"â”‚   {chunk:<64}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. GPU AVAILABILITY & PYTORCH CUDA CHECK\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ GPU Status \" + \"â”€\" * 54 + \"â”\")\n",
    "\n",
    "gpu_available = False\n",
    "gpu_name = \"Not available\"\n",
    "gpu_memory = 0\n",
    "cuda_version = \"N/A\"\n",
    "torch_cuda_enabled = False\n",
    "pytorch_installed = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    pytorch_installed = True\n",
    "    torch_cuda_enabled = torch.cuda.is_available()\n",
    "    gpu_available = torch_cuda_enabled\n",
    "    \n",
    "    if torch_cuda_enabled:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        cuda_version = torch.version.cuda\n",
    "        \n",
    "        print(f\"â”‚ âœ… GPU Detected:     {gpu_name[:45]:<45}â”‚\")\n",
    "        print(f\"â”‚    CUDA Version:     {cuda_version:<46}â”‚\")\n",
    "        print(f\"â”‚    Memory:           {gpu_memory:.1f} GB{' ' * 42}â”‚\")\n",
    "        print(f\"â”‚    PyTorch CUDA:     Enabled{' ' * 40}â”‚\")\n",
    "        \n",
    "        # GPU utilization\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n",
    "                                   '--format=csv,noheader,nounits'], \n",
    "                                  capture_output=True, text=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                gpu_util, mem_used, mem_total = result.stdout.strip().split(',')\n",
    "                print(f\"â”‚    Utilization:      {gpu_util.strip()}%{' ' * 43}â”‚\")\n",
    "                print(f\"â”‚    Memory Used:      {mem_used.strip()} MB / {mem_total.strip()} MB{' ' * 28}â”‚\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    else:\n",
    "        # PyTorch installed but CUDA not available\n",
    "        print(\"â”‚ âš ï¸  PyTorch installed but CUDA not enabled{' ' * 24}â”‚\")\n",
    "        print(f\"â”‚    PyTorch Version:  {torch.__version__:<46}â”‚\")\n",
    "        print(f\"â”‚    CUDA Built:       {torch.version.cuda if torch.version.cuda else 'No (CPU-only)':<46}â”‚\")\n",
    "        print(f\"â”‚    CUDA Available:   {torch_cuda_enabled}{' ' * 41}â”‚\")\n",
    "        print(\"â”‚{' ' * 68}â”‚\")\n",
    "        print(\"â”‚ âš ï¸  Training will use CPU (20-40x slower){' ' * 25}â”‚\")\n",
    "        print(\"â”‚    Expected time:    10 epochs â‰ˆ 20-40 hours{' ' * 21}â”‚\")\n",
    "        print(\"â”‚{' ' * 68}â”‚\")\n",
    "        print(\"â”‚ ğŸ’¡ To enable GPU:{' ' * 50}â”‚\")\n",
    "        print(\"â”‚    1. Verify NVIDIA GPU is present (nvidia-smi){' ' * 19}â”‚\")\n",
    "        print(\"â”‚    2. Install CUDA Toolkit (nvidia.com/cuda){' ' * 22}â”‚\")\n",
    "        print(\"â”‚    3. Reinstall PyTorch with CUDA:{' ' * 34}â”‚\")\n",
    "        print(\"â”‚       pip uninstall torch{' ' * 42}â”‚\")\n",
    "        print(\"â”‚       pip install torch --index-url{' ' * 30}â”‚\")\n",
    "        print(\"â”‚         https://download.pytorch.org/whl/cu121{' ' * 20}â”‚\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"â”‚ âŒ PyTorch not installed{' ' * 42}â”‚\")\n",
    "    print(\"â”‚{' ' * 68}â”‚\")\n",
    "    print(\"â”‚ Install with GPU support:{' ' * 42}â”‚\")\n",
    "    print(\"â”‚   pip install torch --index-url{' ' * 34}â”‚\")\n",
    "    print(\"â”‚     https://download.pytorch.org/whl/cu121{' ' * 24}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CRITICAL DEPENDENCIES\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ Critical Dependencies \" + \"â”€\" * 43 + \"â”\")\n",
    "\n",
    "dependencies = {\n",
    "    'torch': 'PyTorch (Deep Learning)',\n",
    "    'lightning': 'PyTorch Lightning (Training)',\n",
    "    'pandas': 'Pandas (Data Processing)',\n",
    "    'numpy': 'NumPy (Numerical Computing)',\n",
    "    'pytorch_forecasting': 'PyTorch Forecasting (TFT Model)',\n",
    "    'fastapi': 'FastAPI (Inference API)',\n",
    "    'plotly': 'Plotly (Dashboard)',\n",
    "    'dash': 'Dash (Dashboard Framework)'\n",
    "}\n",
    "\n",
    "missing_deps = []\n",
    "installed_deps = []\n",
    "\n",
    "for package, description in dependencies.items():\n",
    "    spec = importlib.util.find_spec(package)\n",
    "    if spec is not None:\n",
    "        try:\n",
    "            module = importlib.import_module(package)\n",
    "            version = getattr(module, '__version__', 'unknown')\n",
    "            status = \"âœ…\"\n",
    "            installed_deps.append(package)\n",
    "            pkg_display = f\"{package} ({version})\"\n",
    "        except:\n",
    "            status = \"âš ï¸\"\n",
    "            pkg_display = package\n",
    "    else:\n",
    "        status = \"âŒ\"\n",
    "        missing_deps.append(package)\n",
    "        pkg_display = package\n",
    "    \n",
    "    print(f\"â”‚ {status} {pkg_display:<63}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DIRECTORY STRUCTURE\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ Directory Structure \" + \"â”€\" * 46 + \"â”\")\n",
    "\n",
    "required_dirs = {\n",
    "    'training': nordiq_root / 'training',\n",
    "    'models': nordiq_root / 'models',\n",
    "    'checkpoints': nordiq_root / 'checkpoints',\n",
    "    'logs': nordiq_root / 'logs'\n",
    "}\n",
    "\n",
    "for name, path in required_dirs.items():\n",
    "    exists = path.exists()\n",
    "    status = \"âœ…\" if exists else \"âš ï¸\"\n",
    "    existence = \"exists\" if exists else \"will be created\"\n",
    "    print(f\"â”‚ {status} {name + '/':20} {existence:<44}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. EXISTING MODELS CHECK\n",
    "# ============================================================================\n",
    "print(\"â”Œâ”€ Existing Models \" + \"â”€\" * 50 + \"â”\")\n",
    "\n",
    "models_dir = nordiq_root / 'models'\n",
    "if models_dir.exists():\n",
    "    model_dirs = sorted(models_dir.glob('tft_model_*'), reverse=True)\n",
    "    \n",
    "    if model_dirs:\n",
    "        print(f\"â”‚ Found {len(model_dirs)} trained model(s):{' ' * 40}â”‚\")\n",
    "        for i, model_dir in enumerate(model_dirs[:3], 1):  # Show last 3\n",
    "            model_name = model_dir.name\n",
    "            model_size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file()) / (1024**2)\n",
    "            print(f\"â”‚   {i}. {model_name:<40} ({model_size:>6.1f} MB) â”‚\")\n",
    "        if len(model_dirs) > 3:\n",
    "            print(f\"â”‚   ... and {len(model_dirs) - 3} more{' ' * 44}â”‚\")\n",
    "    else:\n",
    "        print(\"â”‚ No trained models found - ready for first training{' ' * 16}â”‚\")\n",
    "else:\n",
    "    print(\"â”‚ Models directory will be created on first training{' ' * 16}â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OVERALL READINESS\n",
    "# ============================================================================\n",
    "print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "\n",
    "all_critical_deps = all(dep in installed_deps for dep in ['torch', 'lightning', 'pandas', 'pytorch_forecasting'])\n",
    "\n",
    "if all_critical_deps and torch_cuda_enabled:\n",
    "    print(\"â•‘\" + \" \" * 15 + \"âœ… SYSTEM READY FOR TRAINING\" + \" \" * 24 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \" * 15 + f\"Estimated: 10 epochs â‰ˆ 3-5 hours on {gpu_name[:20]}\" + \" \" * (12 - len(gpu_name[:20])) + \"â•‘\")\n",
    "elif all_critical_deps and pytorch_installed and not torch_cuda_enabled:\n",
    "    print(\"â•‘\" + \" \" * 10 + \"âš ï¸  PYTORCH INSTALLED WITHOUT CUDA SUPPORT\" + \" \" * 16 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \" * 15 + \"Training will be 20-40x slower on CPU\" + \" \" * 15 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \" * 15 + \"Estimated: 10 epochs â‰ˆ 20-40 hours\" + \" \" * 19 + \"â•‘\")\n",
    "else:\n",
    "    print(\"â•‘\" + \" \" * 12 + \"âŒ MISSING DEPENDENCIES - INSTALL FIRST\" + \" \" * 17 + \"â•‘\")\n",
    "    if missing_deps:\n",
    "        print(\"â•‘\" + \" \" * 15 + f\"Missing: {', '.join(missing_deps)}\" + \" \" * (53 - len(', '.join(missing_deps))) + \"â•‘\")\n",
    "\n",
    "print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "print()\n",
    "\n",
    "# Clean up\n",
    "if not all_critical_deps:\n",
    "    print(\"âš ï¸  Install missing packages:\")\n",
    "    print(\"   pip install torch lightning pandas pytorch-forecasting fastapi plotly dash\")\n",
    "    print()\n",
    "elif pytorch_installed and not torch_cuda_enabled:\n",
    "    print(\"ğŸ’¡ GPU training available but PyTorch lacks CUDA support\")\n",
    "    print(\"   Reinstall PyTorch with CUDA:\")\n",
    "    print(\"   pip uninstall torch\")\n",
    "    print(\"   pip install torch --index-url https://download.pytorch.org/whl/cu121\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_production",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Generation\n",
    "\n",
    "Creates realistic server metrics with:\n",
    "- 7 server profiles (ML, DB, Web, Conductor, ETL, Risk, Generic)\n",
    "- Financial market hours patterns\n",
    "- 14 LINBORG-compatible metrics\n",
    "\n",
    "**Adjust parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Generate Training Dataset\n",
    "# Expected time: 24h=30-60s | 720h=15-20min (optimized with parallelization)\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Add src/ to Python path (works from either root or NordIQ directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    # Notebook is in NordIQ folder\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - SIMPLE TWO-PARAMETER SETUP\n",
    "# ============================================\n",
    "\n",
    "TRAINING_HOURS = 168        # Duration: 24 (1 day), 168 (1 week), 720 (30 days - production)\n",
    "TOTAL_SERVERS = 90         # Fleet size: 20 (demo), 90 (default), 400 (production)\n",
    "\n",
    "# Servers are AUTO-DISTRIBUTED across 7 profiles:\n",
    "#   - Web/API:       28% (user-facing services)\n",
    "#   - ML Compute:    22% (training workloads)\n",
    "#   - Database:      17% (critical infrastructure)\n",
    "#   - Data Ingest:   11% (ETL pipelines)\n",
    "#   - Risk Analytics: 9% (EOD calculations)\n",
    "#   - Generic:        7% (utility, max 10)\n",
    "#   - Conductor:      6% (orchestration)\n",
    "\n",
    "TRAINING_DIR = str(nordiq_root / 'training')\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f\"ğŸ¢ Dataset Generation\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"âš™ï¸  Configuration:\")\n",
    "print(f\"   Duration: {TRAINING_HOURS} hours ({TRAINING_HOURS/24:.1f} days)\")\n",
    "print(f\"   Fleet size: {TOTAL_SERVERS} servers (auto-distributed across 7 profiles)\")\n",
    "print(f\"   Output: {TRAINING_DIR}\")\n",
    "\n",
    "# Show expected distribution\n",
    "print(f\"\\nğŸ“Š Expected Profile Distribution:\")\n",
    "dist = {\n",
    "    'Web/API': int(TOTAL_SERVERS * 0.28),\n",
    "    'ML Compute': int(TOTAL_SERVERS * 0.22),\n",
    "    'Database': int(TOTAL_SERVERS * 0.17),\n",
    "    'Data Ingest': int(TOTAL_SERVERS * 0.11),\n",
    "    'Risk Analytics': int(TOTAL_SERVERS * 0.09),\n",
    "    'Generic': min(int(TOTAL_SERVERS * 0.07), 10),\n",
    "    'Conductor': int(TOTAL_SERVERS * 0.06)\n",
    "}\n",
    "for profile, count in dist.items():\n",
    "    print(f\"   {profile:<15} ~{count:>3} servers\")\n",
    "\n",
    "# Estimate rows and time\n",
    "expected_timestamps = TRAINING_HOURS * 3600 // 5  # 5-second intervals\n",
    "expected_rows = expected_timestamps * TOTAL_SERVERS\n",
    "print(f\"\\nğŸ“ˆ Expected Output:\")\n",
    "print(f\"   ~{expected_rows:,} rows ({expected_timestamps:,} timestamps Ã— {TOTAL_SERVERS} servers)\")\n",
    "print(f\"   Parallelized generation: ~{TRAINING_HOURS // 24 * 2 + 1}-{TRAINING_HOURS // 24 * 4 + 2} minutes\")\n",
    "print()\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "# Import and run generator\n",
    "from generators.metrics_generator import main as generate_metrics\n",
    "\n",
    "# Set up command-line arguments - SIMPLE: just --servers and --hours\n",
    "old_argv = sys.argv\n",
    "sys.argv = [\n",
    "    'metrics_generator.py',\n",
    "    '--hours', str(TRAINING_HOURS),\n",
    "    '--servers', str(TOTAL_SERVERS),  # Auto-distributes across profiles!\n",
    "    '--out_dir', TRAINING_DIR,\n",
    "    '--format', 'parquet'\n",
    "]\n",
    "\n",
    "try:\n",
    "    generate_metrics()\n",
    "    print(\"\\nâœ… Dataset generation complete!\")\n",
    "    success = True\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Generation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    success = False\n",
    "finally:\n",
    "    sys.argv = old_argv\n",
    "\n",
    "_elapsed = time.time() - _start\n",
    "_mins = int(_elapsed // 60)\n",
    "_secs = int(_elapsed % 60)\n",
    "print(f\"\\nâ±ï¸  Execution time: {_mins}m {_secs}s\")\n",
    "\n",
    "if success:\n",
    "    # Show what was created\n",
    "    training_path = Path(TRAINING_DIR)\n",
    "    parquet_files = list(training_path.glob(\"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        latest = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "        df = pd.read_parquet(latest)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Dataset Summary:\")\n",
    "        print(f\"   File: {latest.name}\")\n",
    "        print(f\"   Size: {latest.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"   Records: {len(df):,}\")\n",
    "        print(f\"   Servers: {df['server_name'].nunique()}\")\n",
    "        \n",
    "        # Show actual profile distribution\n",
    "        if 'profile' in df.columns:\n",
    "            profile_counts = df.groupby('profile')['server_name'].nunique()\n",
    "            print(f\"\\n   Profile Distribution:\")\n",
    "            for profile, count in profile_counts.sort_values(ascending=False).items():\n",
    "                print(f\"     {profile:<20} {count:>3} servers\")\n",
    "        \n",
    "        print(f\"\\n   Time span: {(df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "        print(f\"\\nğŸ¯ Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m50na52xq5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Explorer\n",
    "\n",
    "Executive-level dataset analysis and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6ny7cku4y",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset Explorer - Executive Presentation View\n",
    "# Professional analysis with visualizations suitable for C-suite presentations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_root / 'src') not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_root / 'src'))\n",
    "\n",
    "# Plotting imports\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"âš ï¸  Plotly not available - visualizations disabled\")\n",
    "    print(\"   Install: pip install plotly\")\n",
    "\n",
    "# Find the most recent dataset\n",
    "training_dir = nordiq_root / 'training'\n",
    "parquet_files = list(training_dir.glob(\"*.parquet\"))\n",
    "\n",
    "if not parquet_files:\n",
    "    print(\"âŒ No dataset found. Please run the Dataset Generation cell first.\")\n",
    "else:\n",
    "    latest_file = max(parquet_files, key=lambda p: p.stat().st_mtime)\n",
    "    \n",
    "    print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "    print(\"â•‘\" + \" \" * 18 + \"DATASET ANALYSIS REPORT\" + \" \" * 27 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \" * 15 + \"ArgusAI Predictive Monitoring\" + \" \" * 24 + \"â•‘\")\n",
    "    print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "    print()\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"ğŸ“‚ Loading dataset: {latest_file.name}\")\n",
    "    df = pd.read_parquet(latest_file)\n",
    "    print(f\"âœ… Loaded {len(df):,} records\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXECUTIVE SUMMARY\n",
    "    # ========================================================================\n",
    "    print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "    print(\"â•‘\" + \" \" * 22 + \"EXECUTIVE SUMMARY\" + \" \" * 29 + \"â•‘\")\n",
    "    print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "    print()\n",
    "    \n",
    "    file_size_mb = latest_file.stat().st_size / (1024 * 1024)\n",
    "    time_span = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600\n",
    "    num_servers = df['server_name'].nunique()\n",
    "    num_profiles = df['profile'].nunique() if 'profile' in df.columns else 0\n",
    "    records_per_hour = len(df) / time_span if time_span > 0 else 0\n",
    "    \n",
    "    print(f\"â”Œâ”€ Dataset Metrics \" + \"â”€\" * 50 + \"â”\")\n",
    "    print(f\"â”‚ Total Records:          {len(df):>12,} samples{' ' * 24}â”‚\")\n",
    "    print(f\"â”‚ File Size:              {file_size_mb:>12.1f} MB{' ' * 27}â”‚\")\n",
    "    print(f\"â”‚ Time Span:              {time_span:>12.1f} hours ({time_span/24:.1f} days){' ' * 13}â”‚\")\n",
    "    print(f\"â”‚ Sampling Rate:          {records_per_hour:>12.1f} records/hour{' ' * 16}â”‚\")\n",
    "    print(f\"â”‚ Date Range:             {df['timestamp'].min().strftime('%Y-%m-%d %H:%M'):<33}â”‚\")\n",
    "    print(f\"â”‚                    to   {df['timestamp'].max().strftime('%Y-%m-%d %H:%M'):<33}â”‚\")\n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FLEET COMPOSITION\n",
    "    # ========================================================================\n",
    "    print(f\"â”Œâ”€ Fleet Composition \" + \"â”€\" * 48 + \"â”\")\n",
    "    print(f\"â”‚ Total Servers:          {num_servers:>12} servers{' ' * 25}â”‚\")\n",
    "    \n",
    "    if 'profile' in df.columns:\n",
    "        print(f\"â”‚ Server Profiles:        {num_profiles:>12} types{' ' * 27}â”‚\")\n",
    "        print(f\"â”‚{' ' * 68}â”‚\")\n",
    "        \n",
    "        profile_counts = df.groupby('profile')['server_name'].nunique().sort_values(ascending=False)\n",
    "        for profile, count in profile_counts.items():\n",
    "            pct = (count / num_servers) * 100\n",
    "            bar_length = int(pct / 2)  # Scale to 50 chars max\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n",
    "            print(f\"â”‚  {profile[:20]:<20} {count:>3} ({pct:>5.1f}%) â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # METRICS COVERAGE\n",
    "    # ========================================================================\n",
    "    print(f\"â”Œâ”€ Metrics Coverage \" + \"â”€\" * 49 + \"â”\")\n",
    "    \n",
    "    from core.nordiq_metrics import NORDIQ_METRICS\n",
    "    \n",
    "    available_metrics = [m for m in NORDIQ_METRICS if m in df.columns]\n",
    "    coverage_pct = (len(available_metrics) / len(NORDIQ_METRICS)) * 100\n",
    "    \n",
    "    print(f\"â”‚ LINBORG Metrics:        {len(available_metrics):>12} / {len(NORDIQ_METRICS)} ({coverage_pct:.0f}%){' ' * 20}â”‚\")\n",
    "    print(f\"â”‚{' ' * 68}â”‚\")\n",
    "    \n",
    "    # Group metrics by category\n",
    "    metric_categories = {\n",
    "        'CPU': ['cpu_user_pct', 'cpu_sys_pct', 'cpu_iowait_pct', 'cpu_idle_pct', 'java_cpu_pct'],\n",
    "        'Memory': ['mem_used_pct', 'swap_used_pct'],\n",
    "        'Disk': ['disk_usage_pct'],\n",
    "        'Network': ['net_in_mb_s', 'net_out_mb_s'],\n",
    "        'Connections': ['back_close_wait', 'front_close_wait'],\n",
    "        'System': ['load_average', 'uptime_days']\n",
    "    }\n",
    "    \n",
    "    for category, metrics in metric_categories.items():\n",
    "        category_available = [m for m in metrics if m in df.columns]\n",
    "        cat_pct = (len(category_available) / len(metrics)) * 100\n",
    "        status = \"âœ…\" if cat_pct == 100 else \"âš ï¸\" if cat_pct > 0 else \"âŒ\"\n",
    "        print(f\"â”‚  {status} {category:<15} {len(category_available):>2}/{len(metrics)} metrics ({cat_pct:>5.1f}%){' ' * 25}â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DATA QUALITY METRICS\n",
    "    # ========================================================================\n",
    "    print(f\"â”Œâ”€ Data Quality \" + \"â”€\" * 53 + \"â”\")\n",
    "    \n",
    "    total_cells = len(df) * len(available_metrics)\n",
    "    missing_cells = df[available_metrics].isna().sum().sum()\n",
    "    completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
    "    \n",
    "    print(f\"â”‚ Completeness:           {completeness:>12.2f}%{' ' * 28}â”‚\")\n",
    "    print(f\"â”‚ Missing Values:         {missing_cells:>12,} cells{' ' * 24}â”‚\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated(subset=['timestamp', 'server_name']).sum()\n",
    "    duplicate_pct = (duplicates / len(df)) * 100\n",
    "    print(f\"â”‚ Duplicate Records:      {duplicates:>12,} ({duplicate_pct:.2f}%){' ' * 20}â”‚\")\n",
    "    \n",
    "    # Temporal consistency\n",
    "    if 'timestamp' in df.columns:\n",
    "        df_sorted = df.sort_values(['server_name', 'timestamp'])\n",
    "        time_diffs = df_sorted.groupby('server_name')['timestamp'].diff()\n",
    "        median_interval = time_diffs.median().total_seconds() / 60\n",
    "        print(f\"â”‚ Sampling Interval:      {median_interval:>12.1f} minutes (median){' ' * 14}â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STATISTICAL SUMMARY\n",
    "    # ========================================================================\n",
    "    print(f\"â”Œâ”€ Key Metrics Statistics \" + \"â”€\" * 43 + \"â”\")\n",
    "    print(f\"â”‚ {'Metric':<20} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10} â”‚\")\n",
    "    print(f\"â”‚ {'-'*20} {'-'*10} {'-'*10} {'-'*10} {'-'*10} â”‚\")\n",
    "    \n",
    "    key_metrics = ['cpu_user_pct', 'mem_used_pct', 'disk_usage_pct', 'load_average']\n",
    "    for metric in key_metrics:\n",
    "        if metric in df.columns:\n",
    "            stats = df[metric].describe()\n",
    "            print(f\"â”‚ {metric:<20} {stats['mean']:>10.2f} {stats['std']:>10.2f} {stats['min']:>10.2f} {stats['max']:>10.2f} â”‚\")\n",
    "    \n",
    "    print(\"â””\" + \"â”€\" * 68 + \"â”˜\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VISUALIZATIONS (Executive Charts)\n",
    "    # ========================================================================\n",
    "    if PLOTLY_AVAILABLE:\n",
    "        print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "        print(\"â•‘\" + \" \" * 20 + \"EXECUTIVE VISUALIZATIONS\" + \" \" * 24 + \"â•‘\")\n",
    "        print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "        print()\n",
    "        \n",
    "        # 1. Fleet Distribution by Profile\n",
    "        if 'profile' in df.columns:\n",
    "            fig_fleet = px.pie(\n",
    "                profile_counts.reset_index(), \n",
    "                values='server_name', \n",
    "                names='profile',\n",
    "                title='Fleet Distribution by Server Profile',\n",
    "                color_discrete_sequence=px.colors.qualitative.Set3\n",
    "            )\n",
    "            fig_fleet.update_layout(\n",
    "                font=dict(size=14),\n",
    "                showlegend=True,\n",
    "                height=500\n",
    "            )\n",
    "            fig_fleet.show()\n",
    "        \n",
    "        # 2. Resource Utilization Heatmap\n",
    "        if all(m in df.columns for m in ['cpu_user_pct', 'mem_used_pct', 'disk_usage_pct']):\n",
    "            # Sample data for heatmap (aggregate by hour and profile)\n",
    "            df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "            \n",
    "            if 'profile' in df.columns:\n",
    "                heatmap_data = df.groupby(['hour', 'profile'])['cpu_user_pct'].mean().reset_index()\n",
    "                heatmap_pivot = heatmap_data.pivot(index='profile', columns='hour', values='cpu_user_pct')\n",
    "                \n",
    "                fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "                    z=heatmap_pivot.values,\n",
    "                    x=heatmap_pivot.columns,\n",
    "                    y=heatmap_pivot.index,\n",
    "                    colorscale='RdYlGn_r',\n",
    "                    text=heatmap_pivot.values.round(1),\n",
    "                    texttemplate='%{text}%',\n",
    "                    textfont={\"size\": 10},\n",
    "                    colorbar=dict(title=\"CPU %\")\n",
    "                ))\n",
    "                \n",
    "                fig_heatmap.update_layout(\n",
    "                    title='CPU Utilization Patterns by Profile and Hour of Day',\n",
    "                    xaxis_title='Hour of Day',\n",
    "                    yaxis_title='Server Profile',\n",
    "                    font=dict(size=12),\n",
    "                    height=400\n",
    "                )\n",
    "                fig_heatmap.show()\n",
    "        \n",
    "        # 3. Time Series Overview\n",
    "        if 'timestamp' in df.columns and 'cpu_user_pct' in df.columns:\n",
    "            # Sample every Nth point for performance\n",
    "            sample_size = min(10000, len(df))\n",
    "            df_sample = df.sample(n=sample_size).sort_values('timestamp')\n",
    "            \n",
    "            fig_ts = go.Figure()\n",
    "            \n",
    "            if 'profile' in df.columns:\n",
    "                for profile in df['profile'].unique()[:5]:  # Limit to 5 profiles\n",
    "                    profile_data = df_sample[df_sample['profile'] == profile]\n",
    "                    fig_ts.add_trace(go.Scatter(\n",
    "                        x=profile_data['timestamp'],\n",
    "                        y=profile_data['cpu_user_pct'],\n",
    "                        mode='markers',\n",
    "                        name=profile,\n",
    "                        marker=dict(size=3, opacity=0.6)\n",
    "                    ))\n",
    "            \n",
    "            fig_ts.update_layout(\n",
    "                title='CPU Utilization Over Time by Profile',\n",
    "                xaxis_title='Time',\n",
    "                yaxis_title='CPU User %',\n",
    "                font=dict(size=12),\n",
    "                height=500,\n",
    "                hovermode='closest'\n",
    "            )\n",
    "            fig_ts.show()\n",
    "        \n",
    "        print()\n",
    "        print(\"âœ… Executive visualizations generated successfully\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # READINESS ASSESSMENT\n",
    "    # ========================================================================\n",
    "    print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "    \n",
    "    # Determine readiness\n",
    "    min_required_records = 1000\n",
    "    min_required_servers = 5\n",
    "    min_completeness = 95.0\n",
    "    \n",
    "    is_ready = (\n",
    "        len(df) >= min_required_records and\n",
    "        num_servers >= min_required_servers and\n",
    "        completeness >= min_completeness and\n",
    "        len(available_metrics) >= 10\n",
    "    )\n",
    "    \n",
    "    if is_ready:\n",
    "        print(\"â•‘\" + \" \" * 15 + \"âœ… DATASET READY FOR TRAINING\" + \" \" * 23 + \"â•‘\")\n",
    "        print(\"â•‘\" + \" \" * 15 + f\"{len(df):,} records | {num_servers} servers | {completeness:.1f}% complete\" + \" \" * (38 - len(f\"{len(df):,} records | {num_servers} servers | {completeness:.1f}% complete\")) + \"â•‘\")\n",
    "    else:\n",
    "        print(\"â•‘\" + \" \" * 12 + \"âš ï¸  DATASET MAY NEED MORE DATA\" + \" \" * 26 + \"â•‘\")\n",
    "        \n",
    "        if len(df) < min_required_records:\n",
    "            print(\"â•‘\" + \" \" * 15 + f\"âš ï¸  Only {len(df):,} records (recommend {min_required_records:,}+)\" + \" \" * (48 - len(f\"Only {len(df):,} records (recommend {min_required_records:,}+)\")) + \"â•‘\")\n",
    "        if num_servers < min_required_servers:\n",
    "            print(\"â•‘\" + \" \" * 15 + f\"âš ï¸  Only {num_servers} servers (recommend {min_required_servers}+)\" + \" \" * (48 - len(f\"Only {num_servers} servers (recommend {min_required_servers}+)\")) + \"â•‘\")\n",
    "        if completeness < min_completeness:\n",
    "            print(\"â•‘\" + \" \" * 15 + f\"âš ï¸  {completeness:.1f}% complete (recommend {min_completeness}%+)\" + \" \" * (48 - len(f\"{completeness:.1f}% complete (recommend {min_completeness}%+)\")) + \"â•‘\")\n",
    "    \n",
    "    print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_training",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Training\n",
    "\n",
    "Trains the Temporal Fusion Transformer with:\n",
    "- Profile-based transfer learning\n",
    "- GPU acceleration (if available)\n",
    "- Early stopping to prevent overfitting\n",
    "\n",
    "**Adjust parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Train TFT Model\n",
    "# Expected time: 10 epochs=3-5h | 20 epochs=6-10h\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src/ to Python path (works from either root or NordIQ directory)\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'NordIQ':\n",
    "    # Notebook is in NordIQ folder\n",
    "    nordiq_src = (current_dir / 'src').absolute()\n",
    "    nordiq_root = current_dir\n",
    "else:\n",
    "    # Notebook is in root folder\n",
    "    nordiq_src = (current_dir / 'NordIQ' / 'src').absolute()\n",
    "    nordiq_root = current_dir / 'NordIQ'\n",
    "\n",
    "if str(nordiq_src) not in sys.path:\n",
    "    sys.path.insert(0, str(nordiq_src))\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION - ADJUST THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "TRAINING_EPOCHS = 1       # Recommended: 10-20 epochs\n",
    "\n",
    "# IMPORTANT: Training must run from NordIQ directory for paths to work correctly\n",
    "# Save current directory so we can restore it after training\n",
    "original_dir = Path.cwd()\n",
    "\n",
    "# ============================================\n",
    "\n",
    "print(f\"ğŸ¤– Model Training\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"âš™ï¸  Configuration:\")\n",
    "print(f\"   Epochs: {TRAINING_EPOCHS}\")\n",
    "print(f\"   Dataset: ./training/ (relative to NordIQ/)\")\n",
    "print(f\"   Mode: Fleet-wide with profile-based transfer learning\")\n",
    "print()\n",
    "\n",
    "# Estimate training time\n",
    "est_mins_low = TRAINING_EPOCHS * 20\n",
    "est_mins_high = TRAINING_EPOCHS * 30\n",
    "print(f\"â±ï¸  Estimated time: {est_mins_low//60}h {est_mins_low%60}m - {est_mins_high//60}h {est_mins_high%60}m\")\n",
    "print(f\"   (Based on ~20-30 minutes per epoch on RTX 4090)\")\n",
    "print()\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "print()\n",
    "\n",
    "_start = time.time()\n",
    "\n",
    "# Import and run trainer\n",
    "from training.tft_trainer import train_model\n",
    "\n",
    "try:\n",
    "    # CRITICAL: Change to NordIQ directory before training\n",
    "    # This ensures all relative paths (./training/, ./models/, etc.) resolve correctly\n",
    "    os.chdir(nordiq_root)\n",
    "    print(f\"[INFO] Working directory: {Path.cwd()}\")\n",
    "    \n",
    "    model_path = train_model(\n",
    "        dataset_path='./training/',  # Relative to NordIQ/\n",
    "        epochs=TRAINING_EPOCHS,\n",
    "        per_server=False  # Fleet-wide training with profiles\n",
    "    )\n",
    "    \n",
    "    if model_path:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“ Model saved: {model_path}\")\n",
    "        print()\n",
    "        print(\"ğŸ¯ Transfer Learning Enabled:\")\n",
    "        print(\"   âœ… Model learned patterns for each server profile\")\n",
    "        print(\"   âœ… New servers get strong predictions from day 1\")\n",
    "        print(\"   âœ… No retraining needed when adding servers of known types\")\n",
    "        print()\n",
    "        print(\"ğŸ’¡ Next Steps:\")\n",
    "        print(\"   1. Start system: start_all.bat (Windows) or ./start_all.sh (Linux/Mac)\")\n",
    "        print(\"   2. Open dashboard: http://localhost:8050\")\n",
    "        print(\"   3. API endpoint: http://localhost:8000\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Training failed - check logs above\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Restore original working directory\n",
    "    os.chdir(original_dir)\n",
    "    print(f\"\\n[INFO] Restored working directory: {Path.cwd()}\")\n",
    "\n",
    "_elapsed = time.time() - _start\n",
    "_hours = int(_elapsed // 3600)\n",
    "_mins = int((_elapsed % 3600) // 60)\n",
    "_secs = int(_elapsed % 60)\n",
    "print(f\"\\nâ±ï¸  Execution time: {_hours}h {_mins}m {_secs}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Training Complete!\n",
    "\n",
    "### What you've built:\n",
    "\n",
    "âœ… **Profile-Based Transfer Learning**\n",
    "- Model learned patterns for 7 server profiles\n",
    "- New servers get accurate predictions immediately\n",
    "- No retraining needed for known server types\n",
    "\n",
    "âœ… **Production-Ready System**\n",
    "- 8-hour forecast horizon (96 steps)\n",
    "- Quantile uncertainty estimates (p10, p50, p90)\n",
    "- 14 LINBORG-compatible metrics\n",
    "- Safetensors model format\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Launch the System:\n",
    "\n",
    "**Windows:**\n",
    "```bash\n",
    "cd NordIQ\n",
    "start_all.bat\n",
    "```\n",
    "\n",
    "**Linux/Mac:**\n",
    "```bash\n",
    "cd NordIQ\n",
    "./start_all.sh\n",
    "```\n",
    "\n",
    "**Manual start (development):**\n",
    "```bash\n",
    "# Terminal 1 - Inference daemon\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python src/daemons/tft_inference_daemon.py --port 8000\n",
    "\n",
    "# Terminal 2 - Metrics generator\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python src/daemons/metrics_generator_daemon.py --stream --servers 20\n",
    "\n",
    "# Terminal 3 - Dashboard\n",
    "cd NordIQ\n",
    "conda activate py310\n",
    "python dash_app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Access Points:\n",
    "\n",
    "- **Dashboard:** http://localhost:8050\n",
    "- **Inference API:** http://localhost:8000\n",
    "- **Metrics Generator API:** http://localhost:8001\n",
    "- **Health Check:** http://localhost:8000/health\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Documentation:\n",
    "\n",
    "- **[NordIQ/README.md](NordIQ/README.md)** - Complete system overview\n",
    "- **[NordIQ/Docs/SERVER_PROFILES.md](NordIQ/Docs/SERVER_PROFILES.md)** - 7 server profiles explained\n",
    "- **[NordIQ/Docs/GETTING_STARTED.md](NordIQ/Docs/GETTING_STARTED.md)** - Setup and configuration\n",
    "- **[Docs/ARCHITECTURE_GUIDE.md](Docs/ARCHITECTURE_GUIDE.md)** - System architecture and data contract\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Incremental Training:\n",
    "\n",
    "To add more training epochs later (recommended for continuous learning):\n",
    "\n",
    "```bash\n",
    "cd NordIQ\n",
    "python src/training/tft_trainer.py --epochs 5 --incremental\n",
    "```\n",
    "\n",
    "The system will add epochs to your existing model without starting over!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Your predictive monitoring system is ready!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
