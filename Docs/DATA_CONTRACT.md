# DATA CONTRACT - Single Source of Truth

**Version:** 2.0.0 (NordIQ Metrics Framework Metrics)
**Created:** 2025-10-11
**Updated:** 2025-10-14
**Status:** ‚ö†Ô∏è AUTHORITATIVE - All code must conform to this contract

---

## üéØ Purpose

This document defines the **immutable data contract** for the TFT Monitoring Prediction System. ALL components (data generation, training, inference) MUST conform to this specification. Any deviation will cause model loading failures and pipeline breaks.

**DO NOT modify schemas without updating this document first.**

---

## üìä Source Data Reference

### Production Monitoring System (AIMLP OSDS 2.0)
- **Source:** `Docs/aimlp_server_metrics.csv`
- **Description:** `Docs/metrics decription.txt`
- **Server Pattern:** `ppvra00a00XX` (production servers)

---

## üîë Core Schema Definition

### Required Columns (All Stages) - NordIQ Metrics Framework Metrics v2.0

**Core Identification (3 columns):**

| Column | Type | Description | Range/Values |
|--------|------|-------------|--------------|
| `timestamp` | datetime | ISO8601 timestamp | Any valid datetime |
| `server_name` | string | Unique hostname | e.g., `ppml0001`, `ppdb001` |
| `state` | string | Operational state | See State Contract below |

**NordIQ Metrics Framework CPU Metrics (5 columns):**

| Column | Type | Description | Range/Values |
|--------|------|-------------|--------------|
| `cpu_user_pct` | float | User space CPU | 0.0 - 100.0 |
| `cpu_sys_pct` | float | System/kernel CPU | 0.0 - 100.0 |
| `cpu_iowait_pct` | float | **I/O wait (CRITICAL)** | 0.0 - 100.0 |
| `cpu_idle_pct` | float | Idle CPU (% Used = 100 - idle) | 0.0 - 100.0 |
| `java_cpu_pct` | float | Java/Spark CPU usage | 0.0 - 100.0 |

**NordIQ Metrics Framework Memory Metrics (2 columns):**

| Column | Type | Description | Range/Values |
|--------|------|-------------|--------------|
| `mem_used_pct` | float | Memory utilization | 0.0 - 100.0 |
| `swap_used_pct` | float | Swap usage (thrashing indicator) | 0.0 - 100.0 |

**NordIQ Metrics Framework Disk Metrics (1 column):**

| Column | Type | Description | Range/Values |
|--------|------|-------------|--------------|
| `disk_usage_pct` | float | Disk space usage | 0.0 - 100.0 |

**NordIQ Metrics Framework Network Metrics (2 columns):**

| Column | Type | Description | Range/Values |
|--------|------|-------------|--------------|
| `net_in_mb_s` | float | Network ingress (MB/s) | 0.0+ |
| `net_out_mb_s` | float | Network egress (MB/s) | 0.0+ |

**NordIQ Metrics Framework Connection Metrics (2 columns):**

| Column | Type | Description | Range/Values |
|--------|------|-------------|--------------|
| `back_close_wait` | int | TCP backend connections | 0+ |
| `front_close_wait` | int | TCP frontend connections | 0+ |

**NordIQ Metrics Framework System Metrics (2 columns):**

| Column | Type | Description | Range/Values |
|--------|------|-------------|--------------|
| `load_average` | float | System load average | 0.0+ |
| `uptime_days` | int | Days since reboot | 0-365 |

**Total:** 3 core + 14 NordIQ Metrics Framework metrics = **17 required columns**

### DEPRECATED Columns (DO NOT USE):

‚ùå `cpu_pct` - Replaced by cpu_user_pct, cpu_sys_pct, cpu_iowait_pct, cpu_idle_pct, java_cpu_pct
‚ùå `mem_pct` - Replaced by mem_used_pct, swap_used_pct
‚ùå `disk_io_mb_s` - Replaced by net_in_mb_s, net_out_mb_s
‚ùå `latency_ms` - Replaced by load_average

### State Contract (IMMUTABLE)

**These 8 values MUST match across all pipeline stages:**

```python
VALID_STATES = [
    'critical_issue',  # Severe problems requiring immediate attention
    'healthy',         # Normal operational state
    'heavy_load',      # High utilization but stable
    'idle',           # Low activity baseline
    'maintenance',     # Scheduled maintenance mode
    'morning_spike',   # Peak usage periods (time-based)
    'offline',        # Server unavailable/unreachable
    'recovery'        # Post-incident recovery phase
]
```

**State Determination Logic:**
```python
# Based on source metrics
if anomaly_score > 0.7 or cpu_pct > 95 or mem_pct > 95:
    state = 'critical_issue'
elif cpu_pct > 80 or mem_pct > 80:
    state = 'heavy_load'
elif cpu_pct < 5 and mem_pct < 10:
    state = 'idle'
elif is_business_hours and (9 <= hour <= 11):
    state = 'morning_spike'
# ... (see full logic in metrics_generator.py)
```

---

## üèóÔ∏è Server Name Encoding Strategy

### Problem Statement
Current approach uses sequential integers (1, 2, 3...) which breaks when servers are added/removed in production.

### Solution: Hash-Based Encoding with Mapping Table

#### Encoding (Training/Generation)
```python
import hashlib

def encode_server_name(server_name: str) -> str:
    """
    Create deterministic hash-based encoding for server names.

    Args:
        server_name: Original hostname (e.g., 'ppvra00a0018')

    Returns:
        Consistent numeric string ID (e.g., '12345')
    """
    # Use first 8 chars of SHA256 hash as numeric ID
    hash_obj = hashlib.sha256(server_name.encode('utf-8'))
    hash_int = int(hash_obj.hexdigest()[:8], 16)
    return str(hash_int % 1_000_000)  # Keep it reasonable for TFT

def create_server_mapping(server_names: list) -> dict:
    """
    Create bidirectional mapping for encoding/decoding.

    Returns:
        {
            'name_to_id': {'ppvra00a0018': '123456', ...},
            'id_to_name': {'123456': 'ppvra00a0018', ...}
        }
    """
    name_to_id = {name: encode_server_name(name) for name in server_names}
    id_to_name = {v: k for k, v in name_to_id.items()}
    return {'name_to_id': name_to_id, 'id_to_name': id_to_name}
```

#### Decoding (Inference)
```python
def decode_server_name(server_id: str, mapping: dict) -> str:
    """
    Decode server ID back to original name.

    Args:
        server_id: Encoded server ID
        mapping: Server mapping dict from create_server_mapping()

    Returns:
        Original server name or 'UNKNOWN_{id}' if not found
    """
    return mapping['id_to_name'].get(server_id, f'UNKNOWN_{server_id}')
```

#### Mapping Persistence
```python
# Save mapping during training
import json

mapping = create_server_mapping(df['server_name'].unique())
with open(f'{model_dir}/server_mapping.json', 'w') as f:
    json.dump(mapping, f, indent=2)

# Load mapping during inference
with open(f'{model_dir}/server_mapping.json', 'r') as f:
    server_mapping = json.load(f)
```

**Benefits:**
- ‚úÖ Deterministic: Same server name ‚Üí same ID every time
- ‚úÖ Stable: Adding/removing servers doesn't affect existing IDs
- ‚úÖ Reversible: Can decode predictions back to server names
- ‚úÖ Production-ready: Handles dynamic server fleets

---

## üìê Schema Transformations

### 1. Source ‚Üí Training Data

**File:** `metrics_generator.py`

```python
# Source (AIMLP CSV)
Host Name       ‚Üí server_name
%CPU User+Sys   ‚Üí cpu_pct
%Mem Used       ‚Üí mem_pct
Disk Usage      ‚Üí disk_io_mb_s (derived)
Load Aver       ‚Üí latency_ms (proxy)
(derived)       ‚Üí state (using state determination logic)

# Additional training features
timestamp       ‚Üí hour, day_of_week, month, is_weekend (temporal)
server_name     ‚Üí server_id (encoded via hash)
```

**Output Format:** Parquet (required)
**Output Location:** `training/server_metrics.parquet`

### 2. Training Data ‚Üí Model

**File:** `tft_trainer.py`

```python
# TimeSeriesDataSet configuration
time_idx: Sequential integer (0, 1, 2, ...)
target: 'cpu_pct' (primary target)
group_ids: ['server_id']  # Encoded server names

# Time-varying unknown (to be predicted)
- cpu_pct
- mem_pct
- disk_io_mb_s
- latency_ms

# Time-varying known (known in advance)
- hour
- day_of_week
- month
- is_weekend
- is_business_hours

# Categorical
- state (8 values - see State Contract)
- server_id (hash-encoded)

# Encoders (CRITICAL)
categorical_encoders = {
    'server_id': NaNLabelEncoder(add_nan=True),  # Allow unknown servers
    'state': NaNLabelEncoder(add_nan=True)       # Allow unknown states
}
```

### 3. Model ‚Üí Predictions

**File:** `tft_inference.py`

```python
# Input: Same schema as training
# Output: Predictions with decoded server names

{
    'server_name': 'ppvra00a0018',  # Decoded from server_id
    'prediction_time': '2025-10-11T07:30:00',
    'predictions': {
        '30min': {'cpu_pct': 65.2, 'mem_pct': 72.1, ...},
        '1hr': {...},
        '8hr': {...}
    },
    'quantiles': {
        'p10': {...},  # Lower bound
        'p50': {...},  # Median
        'p90': {...}   # Upper bound
    }
}
```

---

## üîí Validation Contract

### Data Validation (All Stages)

**Required validations before training/inference:**

```python
def validate_schema(df: pd.DataFrame) -> tuple[bool, list[str]]:
    """
    Validate DataFrame against data contract.

    Returns:
        (is_valid, list_of_errors)
    """
    errors = []

    # Required columns
    required_cols = ['timestamp', 'server_name', 'cpu_pct', 'mem_pct',
                     'disk_io_mb_s', 'latency_ms', 'state']
    missing = set(required_cols) - set(df.columns)
    if missing:
        errors.append(f"Missing columns: {missing}")

    # State values
    VALID_STATES = ['critical_issue', 'healthy', 'heavy_load', 'idle',
                    'maintenance', 'morning_spike', 'offline', 'recovery']
    invalid_states = set(df['state'].unique()) - set(VALID_STATES)
    if invalid_states:
        errors.append(f"Invalid states found: {invalid_states}")

    # Numeric ranges
    if (df['cpu_pct'] < 0).any() or (df['cpu_pct'] > 100).any():
        errors.append("cpu_pct out of range [0, 100]")

    if (df['mem_pct'] < 0).any() or (df['mem_pct'] > 100).any():
        errors.append("mem_pct out of range [0, 100]")

    # Timestamp format
    if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
        errors.append("timestamp must be datetime type")

    return (len(errors) == 0, errors)
```

### Model Loading Validation

**Before loading model weights:**

```python
def validate_model_compatibility(model_dir: Path, data_df: pd.DataFrame) -> bool:
    """
    Verify model can load data without dimension mismatches.

    Checks:
    - State values count matches
    - Server mapping exists
    - Feature columns match
    """
    # Load training info
    with open(model_dir / 'training_info.json') as f:
        training_info = json.load(f)

    # Validate state count
    trained_states = training_info.get('unique_states', [])
    current_states = data_df['state'].unique().tolist()

    if set(trained_states) != set(VALID_STATES):
        print(f"[ERROR] Model trained with {len(trained_states)} states, "
              f"contract requires {len(VALID_STATES)}")
        return False

    # Validate server mapping exists
    if not (model_dir / 'server_mapping.json').exists():
        print("[ERROR] server_mapping.json not found in model directory")
        return False

    return True
```

---

## üì¶ File Structure Contract

### Training Data Output
```
training/
‚îú‚îÄ‚îÄ server_metrics.parquet        # Main training data
‚îú‚îÄ‚îÄ metrics_metadata.json         # Generation metadata
‚îî‚îÄ‚îÄ server_mapping.json           # Server name mappings
```

### Model Output
```
models/tft_model_YYYYMMDD_HHMMSS/
‚îú‚îÄ‚îÄ model.safetensors             # Model weights
‚îú‚îÄ‚îÄ config.json                   # Model architecture config
‚îú‚îÄ‚îÄ training_info.json            # Training metadata
‚îú‚îÄ‚îÄ server_mapping.json           # Server name mappings (REQUIRED)
‚îî‚îÄ‚îÄ data_contract_version.txt     # Contract version used
```

### Metadata Schemas

#### metrics_metadata.json
```json
{
  "generated_at": "ISO8601",
  "total_samples": 432000,
  "time_span_hours": 24,
  "servers_count": 25,
  "unique_states": ["critical_issue", "healthy", ...],
  "state_counts": {"healthy": 279598, ...},
  "data_contract_version": "1.0.0"
}
```

#### training_info.json
```json
{
  "trained_at": "ISO8601",
  "epochs": 20,
  "total_samples": 432000,
  "num_servers": 25,
  "unique_states": ["critical_issue", "healthy", ...],
  "state_encoder_size": 8,
  "server_encoder_size": 25,
  "data_contract_version": "1.0.0"
}
```

---

## üö® Breaking Changes Protocol

### If You Must Change The Contract:

1. **Update this document FIRST**
2. **Increment version number** (e.g., 1.0.0 ‚Üí 2.0.0)
3. **Create migration guide** in `Docs/MIGRATIONS.md`
4. **Update all three stages:**
   - `metrics_generator.py`
   - `tft_trainer.py`
   - `tft_inference.py`
5. **Regenerate training data**
6. **Retrain all models**
7. **Run full validation suite**
8. **Update all documentation**

### Version Compatibility Matrix

| Contract | Generator | Trainer | Inference | Status |
|----------|-----------|---------|-----------|--------|
| 1.0.0    | ‚úÖ        | ‚úÖ      | ‚úÖ        | Current |

---

## üõ†Ô∏è Implementation Checklist

### Phase 1: Implement Server Encoding (CURRENT)
- [ ] Create `server_encoder.py` utility module
- [ ] Update `metrics_generator.py` to encode server names
- [ ] Save `server_mapping.json` during generation
- [ ] Update `tft_trainer.py` to use encoded server_id
- [ ] Save `server_mapping.json` with model
- [ ] Update `tft_inference.py` to decode server names

### Phase 2: Add Validation
- [ ] Create `data_validator.py` module
- [ ] Add validation to data generation
- [ ] Add validation before training
- [ ] Add validation before inference
- [ ] Add contract version tracking

### Phase 3: Regenerate & Retrain
- [ ] Regenerate training data with new encoder
- [ ] Validate training data against contract
- [ ] Retrain model with validated data
- [ ] Test full pipeline end-to-end
- [ ] Update all documentation

---

## üìñ Reference Materials

- **Source Data:** [Docs/aimlp_server_metrics.csv](aimlp_server_metrics.csv)
- **Field Descriptions:** [Docs/metrics decription.txt](metrics decription.txt)
- **Current Training Data:** `training/server_metrics.parquet`
- **Metadata:** `training/metrics_metadata.json`

---

## üîç Quick Reference

### State Values (Copy-Paste)
```python
VALID_STATES = [
    'critical_issue',
    'healthy',
    'heavy_load',
    'idle',
    'maintenance',
    'morning_spike',
    'offline',
    'recovery'
]
```

### Required Columns (Copy-Paste) - NordIQ Metrics Framework v2.0
```python
REQUIRED_COLUMNS = [
    'timestamp',
    'server_name',
    'state',
    # CPU metrics (5)
    'cpu_user_pct',
    'cpu_sys_pct',
    'cpu_iowait_pct',  # CRITICAL - "system troubleshooting 101"
    'cpu_idle_pct',
    'java_cpu_pct',
    # Memory metrics (2)
    'mem_used_pct',
    'swap_used_pct',
    # Disk metrics (1)
    'disk_usage_pct',
    # Network metrics (2)
    'net_in_mb_s',
    'net_out_mb_s',
    # Connection metrics (2)
    'back_close_wait',
    'front_close_wait',
    # System metrics (2)
    'load_average',
    'uptime_days'
]
```

---

**Contract Version:** 2.0.0 (NordIQ Metrics Framework Metrics)
**Last Updated:** 2025-10-14
**Maintained By:** Project Team
**Review Frequency:** Before any schema changes

‚ö†Ô∏è **THIS IS THE SOURCE OF TRUTH - ALL CODE MUST CONFORM TO THIS CONTRACT**

**Breaking Change from v1.0.0:** All old metrics (cpu_pct, mem_pct, disk_io_mb_s, latency_ms) replaced with 14 NordIQ Metrics Framework production metrics.
