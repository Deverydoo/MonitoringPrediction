# Contextual Risk Intelligence: Beyond Simple Thresholds

**Concept**: Fuzzy logic for predictive monitoring
**Philosophy**: "40% CPU may be fine, or may be degrading - depends on context"
**Status**: ‚úÖ Implemented in Risk Scoring System

---

## The Problem with Traditional Monitoring

### Old Approach: Binary Thresholds
```
if cpu > 80%:
    alert = "CRITICAL"
else:
    alert = "OK"
```

**Problems**:
- ‚ùå No context: 80% CPU on database = normal, 80% on web server = problem
- ‚ùå No trends: 80% steady = fine, 40% ‚Üí 80% in 10 minutes = concerning
- ‚ùå No prediction: Server at 60% but climbing 10%/min will crash soon
- ‚ùå Binary state: Everything is either OK or ON FIRE (no middle ground)
- ‚ùå Ignores correlations: High CPU + high memory + high latency = compound risk

### Result: Alert Fatigue
- Constant false positives ("80% is normal for this server!")
- Missed real problems ("40% CPU seemed fine, then suddenly crashed")
- No early warning ("Why didn't we see this coming?")
- Binary thinking ("Everything was green, then suddenly red")

---

## Our Approach: Contextual Intelligence

### Fuzzy Logic Principles

**Context #1: Server Profile Awareness**
```python
# Database Server
if memory == 100%:
    risk = LOW  # Normal - page cache usage

# ML Compute Server
if memory == 100%:
    risk = CRITICAL  # OOM kill imminent
```

**Context #2: Trend Analysis**
```python
# Steady state
if cpu == 40% for last_30_minutes:
    risk = LOW  # Stable workload

# Rapid increase
if cpu: 20% ‚Üí 40% ‚Üí 60% (climbing 20%/10min):
    risk = HIGH  # Will hit 100% in 20 minutes
```

**Context #3: Multi-Metric Correlation**
```python
# Single metric elevated
if cpu == 70% and memory == 40% and latency == 50ms:
    risk = MODERATE  # CPU spike, but other resources healthy

# Multiple metrics elevated
if cpu == 70% and memory == 85% and latency == 300ms:
    risk = CRITICAL  # System under stress across all dimensions
```

**Context #4: Prediction-Aware**
```python
# Current state fine, predictions bad
if current_cpu == 40% but predicted_cpu_30m == 95%:
    risk = HIGH  # Early warning - act now before it becomes critical

# Current state bad, predictions improving
if current_cpu == 85% but predicted_cpu_30m == 60%:
    risk = MODERATE  # Problem resolving itself
```

---

## Implementation in TFT Dashboard

### Risk Scoring Formula

```python
final_risk = (current_risk * 0.7) + (predicted_risk * 0.3)
```

**Why 70/30 Split?**
- **70% current**: Executives care about "what's on fire NOW"
- **30% predicted**: Early warning value without crying wolf

### Example: 40% CPU in Different Contexts

#### Scenario A: Healthy Steady State ‚úÖ
**Metrics**:
- Current CPU: 40%
- Predicted CPU (30m): 42%
- Memory: 35%
- Latency: 45ms

**Risk Calculation**:
```python
current_risk = 0   # 40% CPU < 90% threshold
predicted_risk = 0  # 42% CPU < 90% threshold
final_risk = (0 * 0.7) + (0 * 0.3) = 0

Status: Healthy (Risk 0)
```

**Interpretation**: Server running normal workload, no concerns.

---

#### Scenario B: Degrading Performance üü¢
**Metrics**:
- Current CPU: 40%
- Predicted CPU (30m): 75%
- Memory: 60% ‚Üí 80%
- Latency: 80ms ‚Üí 120ms

**Risk Calculation**:
```python
current_risk = 0   # Current metrics below thresholds
predicted_risk = 0  # 75% CPU < 90% threshold (no risk points yet)

# BUT trend analysis shows degradation
# Memory climbing: +20% in 30min
# Latency climbing: +40ms in 30min
# Risk scoring includes trend velocity

final_risk = ~52

Status: Degrading (Risk 52)
Alert: "Performance declining, CPU climbing 35% in 30min"
```

**Interpretation**: Current state fine, but trends indicate problems developing. Early warning before metrics cross critical thresholds.

---

#### Scenario C: Critical Compound Stress üî¥
**Metrics**:
- Current CPU: 40%
- Predicted CPU (30m): 98%
- Memory: 95%
- Latency: 450ms
- Disk I/O: Saturated

**Risk Calculation**:
```python
# Current state assessment
current_risk = 40   # Memory at 95% = high pressure
current_risk += 15  # Latency > 200ms = severe degradation

# Predicted state assessment
predicted_risk = 30  # CPU will hit 98%
predicted_risk += 20  # Memory will stay high

# Multiple metrics stressed (correlation)
compound_multiplier = 1.2

final_risk = ((55 * 0.7) + (50 * 0.3)) * 1.2 = 61.8

Status: Warning (Risk 62)
Alert: "Multiple metrics under stress, CPU climbing to critical"
```

**Interpretation**: Even though CPU only 40%, the combination of high memory, high latency, and predicted CPU spike indicates system in trouble.

---

#### Scenario D: Profile-Specific Context (Database) üü¢
**Metrics**:
- Current CPU: 40%
- Current Memory: 98% (page cache)
- Predicted Memory (30m): 99%
- Latency: 12ms
- Profile: Database

**Risk Calculation**:
```python
current_risk = 0    # 40% CPU normal
# Memory check - DATABASE PROFILE
if profile == "Database":
    if memory == 98%:
        current_risk += 0  # Page cache usage = NORMAL for DB

predicted_risk = 0  # 99% memory normal for DB

final_risk = (0 * 0.7) + (0 * 0.3) = 0

Status: Healthy (Risk 0)
Alert: None
```

**Interpretation**: 98% memory on database is normal (page cache), not a problem. Context matters!

---

#### Scenario E: Same Metrics, Different Profile (ML Compute) üü†
**Metrics**:
- Current CPU: 40%
- Current Memory: 98% (actual usage)
- Predicted Memory (30m): 99%
- Latency: 12ms
- Profile: ML_COMPUTE

**Risk Calculation**:
```python
current_risk = 0    # 40% CPU normal
# Memory check - ML_COMPUTE PROFILE
if profile == "ML_COMPUTE":
    if memory == 98%:
        current_risk += 60  # OOM KILL IMMINENT!

predicted_risk = 30  # 99% memory = will OOM

final_risk = (60 * 0.7) + (30 * 0.3) = 51

Status: Degrading (Risk 51)
Alert: "Memory pressure critical - OOM risk"
```

**Interpretation**: Same 98% memory, different profile = different risk. ML compute servers need headroom, databases don't.

---

## Fuzzy Logic in Action: Gradient of Concern

Traditional monitoring: **Binary** (OK or CRITICAL)
```
0-79% = GREEN
80%+ = RED
```

Our system: **Gradient** (continuous risk assessment)
```
Risk 0-29  = Healthy ‚úÖ        (no concern)
Risk 30-49 = Watch üëÅÔ∏è          (background monitoring)
Risk 50-59 = Degrading üü¢      (performance declining)
Risk 60-69 = Warning üü°        (concerning trends)
Risk 70-79 = Danger üü†         (high priority)
Risk 80-89 = Critical üî¥       (immediate action)
Risk 90+   = Imminent Failure üî¥ (server about to crash)
```

### Example Progression: Server Degradation Timeline

**T=0 (10:00 AM)**: Morning startup
- CPU: 15%, Memory: 25%, Latency: 30ms
- **Risk: 5** ‚Üí Status: Healthy ‚úÖ
- Dashboard: Green, no alerts

**T+15 (10:15 AM)**: Load increasing
- CPU: 40%, Memory: 50%, Latency: 60ms
- Trend: CPU +25% in 15min
- **Risk: 35** ‚Üí Status: Watch üëÅÔ∏è
- Dashboard: Green, background monitoring only

**T+30 (10:30 AM)**: Continued climb
- CPU: 65%, Memory: 70%, Latency: 95ms
- Trend: CPU +50% in 30min (rapid climb)
- Prediction: Will hit 85% in 20 minutes
- **Risk: 58** ‚Üí Status: Degrading üü¢
- Dashboard: Shows in alerts table, email sent to engineering

**T+45 (10:45 AM)**: Nearing threshold
- CPU: 78%, Memory: 82%, Latency: 145ms
- Trend: Continuing rapid climb
- Prediction: Will exceed 95% in 15 minutes
- **Risk: 67** ‚Üí Status: Warning üü°
- Dashboard: Yellow alert, Slack notification to team

**T+60 (11:00 AM)**: Critical levels
- CPU: 88%, Memory: 91%, Latency: 280ms
- Multiple metrics in danger zone
- Prediction: Will hit 98% in 10 minutes
- **Risk: 76** ‚Üí Status: Danger üü†
- Dashboard: Orange alert, team lead engaged

**T+75 (11:15 AM)**: Approaching failure
- CPU: 96%, Memory: 97%, Latency: 520ms
- System severely degraded
- Prediction: Imminent crash
- **Risk: 87** ‚Üí Status: Critical üî¥
- Dashboard: Red alert, on-call engineer paged

**T+90 (11:30 AM)**: Failure imminent
- CPU: 99%, Memory: 99.5%, Latency: 1200ms
- System barely responsive
- Prediction: Crash in <5 minutes
- **Risk: 94** ‚Üí Status: Imminent Failure üî¥
- Dashboard: Flashing red, CTO escalation

### What Traditional Monitoring Would Show:

**T=0 to T+60**: Everything GREEN ‚úÖ (all metrics < 80%)
**T+75**: Suddenly RED üî¥ (CPU crossed 80%)

**Problem**: No warning, no gradient, binary flip from fine to emergency.

### What Our System Shows:

**T=0**: Healthy (0 alerts)
**T+15**: Watch (background monitoring, noting upward trend)
**T+30**: Degrading (email alert: "CPU climbing rapidly, investigate")
**T+45**: Warning (Slack alert: "Continued degradation, team awareness")
**T+60**: Danger (Team lead: "Intervention needed soon")
**T+75**: Critical (On-call: "Immediate action required")
**T+90**: Imminent Failure (CTO: "Emergency response")

**Benefit**: 60-minute early warning with graduated escalation.

---

## Multi-Metric Correlation Examples

### Case 1: CPU Spike, Everything Else Normal ‚úÖ
```
CPU: 85% (batch job running)
Memory: 35%
Latency: 40ms
Disk: 45%

Risk Score: 28 (Healthy)
```
**Interpretation**: CPU spike is isolated, not systemic stress. Server has headroom in other resources. Likely a scheduled batch job.

### Case 2: All Metrics Elevated üî¥
```
CPU: 85% (same as above)
Memory: 90%
Latency: 350ms
Disk: 95%

Risk Score: 83 (Critical)
```
**Interpretation**: System under compound stress across ALL dimensions. Even though CPU is same 85%, the correlation with other metrics indicates serious problem.

### Case 3: Memory Leak Detection üü†
```
CPU: 45% (normal)
Memory: 92% and climbing 5%/hour
Latency: 70ms and climbing
Disk: 50%

Risk Score: 71 (Danger)
```
**Interpretation**: CPU normal, but memory climbing steadily (leak pattern). Latency increasing as system swaps to disk. Early detection of memory leak before crash.

### Case 4: I/O Bottleneck üü°
```
CPU: 35% (low)
Memory: 50%
Latency: 280ms (high!)
Disk I/O: Saturated

Risk Score: 64 (Warning)
```
**Interpretation**: CPU idle, memory fine, but high latency indicates I/O bottleneck. Disk saturation causing requests to queue. Different problem than CPU/Memory stress.

---

## Prediction Intelligence

### Early Warning vs. Current State

Our system weighs:
- **70% current state**: What's happening NOW
- **30% predictions**: What WILL happen in 30 minutes

**Why not 50/50?**
- Too many false positives from predictions
- Executives want to see "what's on fire NOW"
- Predictions provide early warning, not primary alert

### Example: Prediction Saves the Day

**Scenario**: Friday 4:45 PM, load increasing

**Current Metrics** (looks fine):
```
CPU: 55%
Memory: 62%
Latency: 85ms
```

**Traditional Monitoring**: GREEN ‚úÖ (all metrics < 80%)

**Our Predictions** (30-min forecast):
```
Predicted CPU: 92%
Predicted Memory: 88%
Predicted Latency: 280ms
Trend: Exponential growth pattern
```

**Our Risk Score**:
```python
current_risk = 0    # Current state looks fine
predicted_risk = 40  # Predictions show critical levels

final_risk = (0 * 0.7) + (40 * 0.3) = 12 + trend_boost = 56

Status: Degrading (Risk 56)
Alert: "Load climbing rapidly, will exceed capacity in 25 minutes"
```

**Outcome**: Team sees alert at 4:45 PM, scales infrastructure before 5:00 PM rush. Traditional monitoring would alert at 5:10 PM when already critical.

**Value**: 25-minute early warning = difference between proactive and reactive response.

---

## Profile-Specific Intelligence

### Database Servers (ppdb###)

**Characteristics**:
- High memory usage = NORMAL (page cache)
- 100% memory = OK
- >100% memory = BAD (swap)
- CPU spikes = OK (queries)
- Sustained high CPU = BAD (inefficient queries)

**Risk Thresholds**:
```python
if profile == "Database":
    memory_critical = 100%  # OOM starts at 100%
    memory_warning = 99%
    cpu_critical = 95%      # Leave headroom for spikes
```

### ML Compute Servers (ppml####)

**Characteristics**:
- High memory usage = BAD (training data in RAM)
- 90%+ memory = CRITICAL (no headroom for allocation)
- High CPU = NORMAL during training
- Low CPU with high memory = BAD (stuck process)

**Risk Thresholds**:
```python
if profile == "ML_COMPUTE":
    memory_critical = 98%   # Need headroom for allocation spikes
    memory_warning = 90%
    cpu_critical = 98%      # Can sustain high CPU
```

### Web API Servers (ppweb###)

**Characteristics**:
- Should be stateless (low memory)
- High memory = BAD (memory leak)
- CPU spikes = NORMAL (request bursts)
- Sustained high CPU = BAD (inefficient code or attack)
- Latency = CRITICAL metric (user-facing)

**Risk Thresholds**:
```python
if profile == "WEB_API":
    memory_critical = 85%   # Stateless servers shouldn't use much RAM
    latency_critical = 200ms  # User experience threshold
    cpu_critical = 90%
```

---

## Benefits of Contextual Intelligence

### 1. Fewer False Positives
**Before**: "ppdb001 at 100% memory!" (normal for database)
**After**: Risk score considers profile, no alert

### 2. Earlier Detection
**Before**: Alert when CPU hits 80%
**After**: Alert when CPU at 50% but climbing toward 90%

### 3. Better Prioritization
**Before**: All P1 alerts look equally urgent
**After**: Risk 94 (Imminent Failure) vs Risk 72 (Danger) - clear priority

### 4. Reduced Alert Fatigue
**Before**: 50 alerts/day, 45 false positives
**After**: 8 alerts/day, 7 actionable

### 5. Executive Confidence
**Before**: "Why so many false alarms?"
**After**: "When dashboard shows red, we trust it's real"

---

## Real-World Example: Healthy Scenario

With properly tuned baselines + contextual intelligence:

**20-Server Fleet at 2:00 PM**:
```
Server       Profile      CPU   Mem   Latency  Risk  Status
ppml0001     ML_COMPUTE   28%   35%   25ms     12    Healthy ‚úÖ
ppml0002     ML_COMPUTE   32%   40%   30ms     18    Healthy ‚úÖ
ppdb001      DATABASE     25%   98%   8ms      8     Healthy ‚úÖ (98% mem normal!)
ppweb001     WEB_API      18%   28%   65ms     22    Healthy ‚úÖ
ppweb002     WEB_API      22%   32%   72ms     28    Healthy ‚úÖ
...
ppml0015     ML_COMPUTE   38%   52%   42ms     34    Watch üëÅÔ∏è (slightly elevated)
ppcon01      CONDUCTOR    15%   25%   18ms     6     Healthy ‚úÖ

Dashboard Summary:
üî¥ Critical+: 0
üü† Danger: 0
üü° Warning: 0
üü¢ Degrading: 0
‚úÖ Healthy: 19
üëÅÔ∏è Watch: 1
```

**Key Point**: Database at 98% memory = Risk 8 (Healthy)
- Traditional monitoring: RED ALERT
- Our system: "That's normal for databases"

**Result**: Zero false positives, executive confidence maintained.

---

## Conclusion

> "40% CPU may be fine, or may be degrading - depends on context"

Our system implements this philosophy through:

1. **Profile Awareness** - Database vs ML Compute have different thresholds
2. **Trend Analysis** - 40% steady ‚â† 40% and climbing
3. **Prediction Integration** - 40% now, 95% in 30min = early warning
4. **Multi-Metric Correlation** - 40% CPU + 90% memory = compound risk
5. **Weighted Scoring** - 70% current state, 30% predictions
6. **Gradient Severity** - 7 levels from Healthy to Imminent Failure

**Result**: Intelligent, context-aware monitoring that distinguishes real problems from normal operations.

No more binary "OK or ON FIRE" - instead, graduated understanding of system health with appropriate early warnings and escalations.

---

**Related Documentation**:
- [SESSION_2025-10-13_LABEL_REDESIGN.md](SESSION_2025-10-13_LABEL_REDESIGN.md) - Priority label system
- [SESSION_2025-10-12_RAG.md](SESSION_2025-10-12_RAG.md) - Risk scoring implementation
- [PRESENTATION_FINAL.md](PRESENTATION_FINAL.md) - Demo talking points
