# Automated Retraining System

**Version:** 1.0.0
**Status:** ‚úÖ Production Ready
**Phase:** 1 - Production Essentials

---

## üéØ Overview

The Automated Retraining System monitors model drift in real-time and intelligently triggers retraining when predictions become stale, ensuring always-accurate forecasts without manual intervention.

### Key Features

‚úÖ **Drift Detection** - 4 real-time metrics with 88% SLA alignment
‚úÖ **Intelligent Scheduling** - Trains during quiet periods only
‚úÖ **Safeguards** - Prevents over-training with time/frequency limits
‚úÖ **Zero Configuration** - Automatic data buffering from inference daemon
‚úÖ **Incremental Training** - Adds epochs without overwriting base model

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            AUTOMATED RETRAINING PIPELINE                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Inference Daemon (Port 8000)
    ‚Üì Receives metrics every 5s
DataBuffer (data_buffer/)
    ‚Üì Accumulates in daily parquet files
    ‚Üì
DriftMonitor (drift_monitor.py)
    ‚Üì Calculates 4 drift metrics every 5 min
    ‚Üì
RetrainingDecisionEngine
    ‚Üì Checks: Drift? Quiet Period? Safeguards?
    ‚Üì
    ‚Üì YES ‚Üí Trigger Retraining
    ‚Üì
Incremental Training (tft_trainer.py --incremental)
    ‚Üì Adds 5 epochs using last 30 days of data
    ‚Üì
New Model Checkpoint
    ‚Üì
Inference Daemon (auto-reloads)
```

---

## üìä Drift Metrics (4 Metrics)

### 1. Prediction Error Rate (PER) - 40% Weight
**What:** Mean Absolute Percentage Error between predictions and actuals
**Threshold:** 10% - triggers retraining if exceeded
**Why Important:** Direct measure of prediction accuracy

### 2. Distribution Shift Score (DSS) - 30% Weight
**What:** Kolmogorov-Smirnov test comparing current vs baseline distributions
**Threshold:** 20% - triggers retraining if exceeded
**Why Important:** Detects when input data patterns fundamentally change

### 3. Feature Drift Score (FDS) - 20% Weight
**What:** Z-score distance of current feature means from baseline
**Threshold:** 15% - triggers retraining if exceeded
**Why Important:** Catches gradual feature distribution shifts

### 4. Anomaly Rate - 10% Weight
**What:** Percentage of data points >3 standard deviations from baseline
**Threshold:** 5% - triggers retraining if exceeded
**Why Important:** Detects unusual patterns not captured by other metrics

### Combined Drift Score

```
Combined Score = (PER √ó 0.40) + (DSS √ó 0.30) + (FDS √ó 0.20) + (Anomaly √ó 0.10)
```

Retraining recommended if **ANY** individual metric exceeds threshold OR combined score > 0.12

---

## üõ°Ô∏è Safeguards

Prevents over-training while ensuring freshness:

| Safeguard | Limit | Purpose |
|-----------|-------|---------|
| **Min Time Between** | 6 hours | Prevent rapid successive trainings |
| **Max Time Without** | 30 days | Force retrain even if drift low |
| **Max Per Week** | 3 trainings | Limit computational cost |
| **Quiet Period Only** | CPU < 60%, MEM < 70% | Minimize production impact |

---

## üöÄ Quick Start

### Option 1: Automated (Recommended)

```bash
# Start inference daemon (with automatic data buffering)
python tft_inference_daemon.py

# Start adaptive retraining daemon (in separate terminal)
python adaptive_retraining_daemon.py --interval 300
```

The retraining daemon will:
1. Check drift every 5 minutes
2. Trigger retraining when conditions met
3. Run continuously in background

### Option 2: Manual Testing

```bash
# Run single drift check (testing)
python adaptive_retraining_daemon.py --once

# Check drift monitor directly
python drift_monitor.py

# Check data buffer stats
python -c "from data_buffer import DataBuffer; b = DataBuffer(); print(b.get_stats())"
```

---

## üìã Component Reference

### drift_monitor.py (467 lines)

**Purpose:** Real-time drift detection and metric calculation

**Key Methods:**
```python
monitor = DriftMonitor(window_size=1000)

# Update with new data
monitor.update(predictions, actuals, features)

# Calculate metrics
metrics = monitor.calculate_drift_metrics()
# Returns: {'per': 0.08, 'dss': 0.15, 'fds': 0.12, 'anomaly_rate': 0.03,
#           'combined_score': 0.10, 'needs_retraining': False}

# Generate report
print(monitor.generate_report())

# Save metrics
monitor.save_metrics()  # ‚Üí drift_metrics.json
```

**Thresholds (Configurable):**
```python
monitor.thresholds = {
    'per_threshold': 0.10,      # 10%
    'dss_threshold': 0.20,      # 20%
    'fds_threshold': 0.15,      # 15%
    'anomaly_threshold': 0.05   # 5%
}
```

---

### data_buffer.py (340 lines)

**Purpose:** Accumulate metrics in daily parquet files for retraining

**Key Methods:**
```python
buffer = DataBuffer(buffer_dir='./data_buffer', retention_days=60)

# Append incoming metrics (called by inference daemon)
buffer.append(records)

# Flush to disk (automatic every 10k records)
buffer.flush()

# Get training window
df = buffer.get_training_window(days=30, include_today=False)

# Export for training
buffer.export_training_data('training_data.parquet', days=30)

# Stats
stats = buffer.get_stats()
# Returns: {'file_count': 15, 'total_records': 432000,
#           'date_range': {'start': '2025-10-03', 'end': '2025-10-17'},
#           'disk_usage_mb': 125.4}
```

**Storage Structure:**
```
data_buffer/
‚îú‚îÄ‚îÄ metrics_2025-10-17.parquet  (today - being written)
‚îú‚îÄ‚îÄ metrics_2025-10-16.parquet  (22.5 MB, 86,400 records)
‚îú‚îÄ‚îÄ metrics_2025-10-15.parquet
...
‚îî‚îÄ‚îÄ metrics_2025-09-18.parquet  (60 days ago - will be deleted)
```

**Automatic Cleanup:** Files older than 60 days deleted daily

---

### adaptive_retraining_daemon.py (400 lines)

**Purpose:** Main automation daemon - orchestrates drift monitoring and retraining

**Key Components:**

**1. RetrainingDecisionEngine**
```python
engine = RetrainingDecisionEngine(
    min_hours_between_training=6,
    max_days_without_training=30,
    max_trainings_per_week=3
)

# Decide if should retrain
decision = engine.should_retrain(drift_metrics, is_quiet_period=True)
# Returns: {'should_retrain': True/False, 'reason': '...', 'safeguards': {...}}

# Record training event
engine.record_training()
```

**2. AdaptiveRetrainingDaemon**
```python
daemon = AdaptiveRetrainingDaemon(
    daemon_url='http://localhost:8000',
    check_interval=300,  # 5 minutes
    data_buffer_dir='./data_buffer',
    training_script='tft_trainer.py'
)

# Run continuously
daemon.run()

# Or single check (testing)
daemon.run_check_cycle()
```

**Command Line:**
```bash
# Run daemon with custom settings
python adaptive_retraining_daemon.py \
  --daemon-url http://localhost:8000 \
  --interval 300 \
  --data-buffer-dir ./data_buffer

# Test mode (single check)
python adaptive_retraining_daemon.py --once
```

---

## üîÑ Retraining Workflow

### Step-by-Step Process

**1. Continuous Monitoring (Every 5 minutes)**
```
DriftMonitor checks last 1000 predictions
Calculates PER, DSS, FDS, Anomaly Rate
Combined score = weighted average
```

**2. Decision Check**
```
IF any metric > threshold:
  IF quiet period (CPU < 60%, MEM < 70%):
    IF safeguards pass (time limits, frequency):
      ‚Üí TRIGGER RETRAINING
    ELSE:
      ‚Üí WAIT (safeguard violated)
  ELSE:
    ‚Üí WAIT (not quiet period)
ELSE:
  ‚Üí SKIP (model healthy)
```

**3. Retraining Execution**
```bash
# Export last 30 days from buffer
buffer.export_training_data('training_data_incremental.parquet', days=30)

# Run incremental training (adds 5 epochs)
python tft_trainer.py \
  --data training_data_incremental.parquet \
  --incremental \
  --epochs 5
```

**4. Model Update**
```
New checkpoint saved ‚Üí checkpoints/last.ckpt
Inference daemon auto-reloads on next prediction
State recorded (last_training_time, training_history)
```

**5. Resume Monitoring**
```
Continue drift monitoring with new baseline
Wait minimum 6 hours before next training
```

---

## üìà Monitoring & Troubleshooting

### Check Drift Status

```bash
# View latest drift metrics
cat drift_metrics.json | jq

# Example output:
{
  "current_metrics": {
    "per": 0.08,
    "dss": 0.15,
    "fds": 0.12,
    "anomaly_rate": 0.03,
    "combined_score": 0.10,
    "needs_retraining": false
  },
  "trends": {
    "per": "increasing",
    "dss": "stable",
    "fds": "decreasing"
  },
  "recommendation": "OK"
}
```

### Check Buffer Status

```bash
# Python
python -c "
from data_buffer import DataBuffer
b = DataBuffer()
stats = b.get_stats()
print(f'Files: {stats[\"file_count\"]}')
print(f'Records: {stats[\"total_records\"]:,}')
print(f'Disk: {stats[\"disk_usage_mb\"]} MB')
print(f'Range: {stats[\"date_range\"][\"start\"]} to {stats[\"date_range\"][\"end\"]}')
"
```

### Check Retraining State

```bash
# View retraining history
cat retraining_state.json | jq

# Example output:
{
  "last_training_time": "2025-10-17T14:23:15",
  "training_history": [
    "2025-10-17T14:23:15",
    "2025-10-14T03:12:45",
    "2025-10-10T22:45:33"
  ],
  "total_trainings": 3
}
```

### Common Issues

#### Issue: "Retraining not triggered despite high drift"

**Check:**
```bash
# Is it quiet period?
curl http://localhost:8000/predictions/current | jq '.predictions |
  [.[] | .current | .cpu_pct] | add / length'
# Should be < 60 for quiet period

# Check safeguards
cat retraining_state.json | jq '.last_training_time'
# Must be >6 hours ago
```

#### Issue: "Data buffer empty"

**Check:**
```bash
# Is inference daemon buffering?
ls -lh data_buffer/
# Should see metrics_YYYY-MM-DD.parquet files

# Check daemon logs
tail -f inference_daemon.log | grep buffer
```

#### Issue: "Training failed"

**Check:**
```bash
# Check training data
python -c "
import pandas as pd
df = pd.read_parquet('training_data_incremental.parquet')
print(f'Records: {len(df)}')
print(f'Servers: {df.server_name.nunique()}')
print(f'Date range: {df.timestamp.min()} to {df.timestamp.max()}')
"

# Check available disk space
df -h .
# Need >5GB for training
```

---

## üéõÔ∏è Configuration

### Drift Thresholds

Edit `drift_monitor.py`:
```python
self.thresholds = {
    'per_threshold': 0.10,      # Increase for less sensitive
    'dss_threshold': 0.20,
    'fds_threshold': 0.15,
    'anomaly_threshold': 0.05
}
```

### Metric Weights

Edit `drift_monitor.py`:
```python
self.weights = {
    'per_weight': 0.40,   # Prediction accuracy
    'dss_weight': 0.30,   # Distribution shift
    'fds_weight': 0.20,   # Feature drift
    'anomaly_weight': 0.10  # Anomalies
}
```

### Safeguards

Edit `adaptive_retraining_daemon.py`:
```python
engine = RetrainingDecisionEngine(
    min_hours_between_training=6,   # Minimum time between trainings
    max_days_without_training=30,   # Force retrain after N days
    max_trainings_per_week=3        # Weekly limit
)
```

### Quiet Period Detection

Edit `adaptive_retraining_daemon.py` ‚Üí `check_quiet_period()`:
```python
# Quiet period thresholds
is_quiet = avg_cpu < 60.0 and avg_mem < 70.0
```

---

## üìä Performance Impact

### Disk Usage

- **Per day:** ~1.5 MB per 20 servers (17,280 records/day at 5s intervals)
- **60 days:** ~90 MB total
- **Training export:** ~45 MB (30-day window)

### CPU Impact

- **Drift monitoring:** <1% CPU (runs every 5 min, takes ~100ms)
- **Data buffering:** <0.1% CPU (append-only, 10k batch writes)
- **Training:** 100% CPU for 10-30 minutes (depends on data size)

### Memory Usage

- **DriftMonitor:** ~10 MB (1000 record window)
- **DataBuffer:** ~2 MB (10k record buffer)
- **Training:** ~2-4 GB (depends on model size)

---

## üöÄ Production Deployment

### Systemd Service (Linux)

Create `/etc/systemd/system/adaptive-retraining.service`:
```ini
[Unit]
Description=Adaptive Retraining Daemon
After=network.target tft-inference-daemon.service
Requires=tft-inference-daemon.service

[Service]
Type=simple
User=tft
WorkingDirectory=/opt/MonitoringPrediction
Environment="PATH=/opt/conda/envs/py310/bin"
ExecStart=/opt/conda/envs/py310/bin/python adaptive_retraining_daemon.py --interval 300
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Enable and start:
```bash
sudo systemctl enable adaptive-retraining
sudo systemctl start adaptive-retraining
sudo systemctl status adaptive-retraining
```

### Docker Compose

Add to `docker-compose.yml`:
```yaml
services:
  adaptive-retraining:
    build: .
    command: python adaptive_retraining_daemon.py --interval 300
    depends_on:
      - inference-daemon
    volumes:
      - ./data_buffer:/app/data_buffer
      - ./models:/app/models
    environment:
      - TFT_API_KEY=${TFT_API_KEY}
    restart: unless-stopped
```

---

## üìö Related Documentation

- **[ADAPTIVE_RETRAINING_PLAN.md](ADAPTIVE_RETRAINING_PLAN.md)** - Original design document
- **[CONTINUOUS_LEARNING_PLAN.md](CONTINUOUS_LEARNING_PLAN.md)** - Continuous learning strategy
- **[MODEL_TRAINING_GUIDELINES.md](MODEL_TRAINING_GUIDELINES.md)** - Training best practices
- **[PRODUCTION_INTEGRATION_GUIDE.md](PRODUCTION_INTEGRATION_GUIDE.md)** - Production setup

---

## üéØ Next Steps

### Enhancements (Future)

1. **Dashboard Integration** - Real-time drift metrics visualization
2. **Unknown Prediction Tracking** - Track predictions for unknown servers
3. **Blue-Green Deployment** - A/B test new models before full rollout
4. **Prediction Validation** - Compare retrained model vs production before switching
5. **Alert Integration** - Slack/PagerDuty notifications when drift detected

### Monitoring Recommendations

1. Set up alerts for high drift scores (>0.15)
2. Monitor training failures (check logs daily)
3. Track training frequency (should be <3/week normally)
4. Monitor disk usage (data_buffer/ directory)
5. Review retraining_state.json weekly

---

**Last Updated:** 2025-10-17
**Status:** ‚úÖ Production Ready
**Version:** 1.0.0
