{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea56623b-5100-4e38-9275-e8e00f5a7c27",
   "metadata": {},
   "source": [
    "# DISTILLED MONITORING SYSTEM\n",
    "\n",
    "## Predictive monitoring with local caching and fallback support\n",
    "\n",
    "### üöÄ QUICK WORKFLOW:\n",
    "1. `setup()` - Initialize system and fallbacks\n",
    "2. `generate_datasets()` - Generate training data (resumable)\n",
    "3. `train()` - Train the distilled model\n",
    "4. `test()` - Test model inference\n",
    "5. `demo()` - Run monitoring demo\n",
    "\n",
    "### üìä MONITORING:\n",
    "- `status()` - Check system status\n",
    "- `show_progress()` - Check dataset generation progress\n",
    "\n",
    "### üîß RECOVERY:\n",
    "- `retry_failed()` - Retry failed generations\n",
    "- `reset_progress()` - Start fresh\n",
    "\n",
    "**Data Sources:** Splunk, Jira, Confluence, IBM Spectrum Conductor, VEMKD logs from Red Hat Linux\n",
    "\n",
    "**Fallback Order:** Remote API ‚Üí Ollama ‚Üí Local Model ‚Üí Static Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b1b6e-1fcb-4460-a06d-eaff33611c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the system\n",
    "from main_notebook import *\n",
    "from config import CONFIG\n",
    "\n",
    "CONFIG['model_name'] = \"bert-base-uncased\" # use local cached model instead of attempting to download. \n",
    "print(\"üöÄ Distilled Monitoring System\")\n",
    "print(\"üìä Ready for predictive monitoring with local caching\")\n",
    "print(f\"üìÅ Cache directory: {CONFIG['hf_cache_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d867a0-11d9-4f5f-a14e-f3735d0913c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup system with fallback chain\n",
    "print(\"üöÄ Setting up Distilled Monitoring System...\")\n",
    "print(\"This includes: directories, fallback systems, and progress tracking\")\n",
    "\n",
    "setup_success = setup()\n",
    "\n",
    "if setup_success:\n",
    "    print(\"\\n‚úÖ System setup complete!\")\n",
    "    print(\"\\nNext: generate_datasets() to create training data\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Setup failed. Check error messages above.\")\n",
    "    print(\"You may need to install dependencies or setup Ollama.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1e239-fae1-4074-b484-24c18086b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check system status\n",
    "status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404acda-847f-4513-a50a-d0860f2a0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check dataset generation progress (if any)\n",
    "print(\"üìä Current dataset generation progress:\")\n",
    "show_progress()\n",
    "\n",
    "print(\"\\nüí° TIPS:\")\n",
    "print(\"‚Ä¢ First run: Will show new session\")\n",
    "print(\"‚Ä¢ Resuming: Will show existing progress\")\n",
    "print(\"‚Ä¢ Use reset_progress() to start fresh\")\n",
    "print(\"‚Ä¢ Use retry_failed() to retry failed items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a9550-6e3f-4243-81c9-6be41311c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use with caution.\n",
    "# reset_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cebbd-93b8-4330-85b5-fb277bd6cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate training datasets with dynamic calculation\n",
    "print(\"üìä DATASET GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate dynamic targets based on YAML content\n",
    "from dataset_generator import OptimizedDatasetGenerator\n",
    "\n",
    "# Calculate more realistic targets\n",
    "temp_generator = DatasetGenerator()  # Changed from OptimizedDatasetGenerator\n",
    "targets, total_language_target = temp_generator._calculate_dynamic_targets()\n",
    "existing = temp_generator._analyze_existing_dataset()\n",
    "\n",
    "print(f\"üßÆ Realistic Target Calculation:\")\n",
    "print(f\"  Technical explanations: {targets.get('technical_explanation', 0)}\")\n",
    "print(f\"  Error interpretations: {targets.get('error_interpretation', 0)}\")\n",
    "print(f\"  Conversational samples: {targets.get('conversational_samples', 0)}\")\n",
    "print(f\"  Total language target: {total_language_target}\")\n",
    "print(f\"  Models per question: {CONFIG.get('models_per_question', 2)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"üìä Current Progress:\")\n",
    "total_existing = 0\n",
    "total_needed = 0\n",
    "\n",
    "for sample_type, target_count in targets.items():\n",
    "    existing_count = existing.get(sample_type, 0)\n",
    "    needed = max(0, target_count - existing_count)\n",
    "    total_existing += existing_count\n",
    "    total_needed += needed\n",
    "    status = \"‚úÖ\" if needed == 0 else \"üîÑ\"\n",
    "    print(f\"  {status} {sample_type}: {existing_count}/{target_count} (need {needed})\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"üìà Overall Progress: {total_existing}/{total_language_target} ({total_existing/total_language_target*100:.1f}%)\")\n",
    "print(f\"üéØ Remaining: {total_needed} language samples to generate\")\n",
    "print(f\"  Metrics target: {CONFIG['metrics_samples']}\")\n",
    "print(f\"  Models per question: {CONFIG.get('models_per_question', 2)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"üìä Current Progress:\")\n",
    "for sample_type, target_count in targets.items():\n",
    "    existing_count = existing.get(sample_type, 0)\n",
    "    needed = max(0, target_count - existing_count)\n",
    "    status = \"‚úÖ\" if needed == 0 else \"üîÑ\"\n",
    "    print(f\"  {status} {sample_type}: {existing_count}/{target_count} (need {needed})\")\n",
    "\n",
    "total_existing = sum(existing.values())\n",
    "total_needed = sum(max(0, targets[t] - existing.get(t, 0)) for t in targets)\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"üìà Overall Progress: {total_existing}/{total_language_target} ({total_existing/total_language_target*100:.1f}%)\")\n",
    "print(f\"üéØ Remaining: {total_needed} language samples to generate\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  IMPORTANT:\")\n",
    "print(\"‚Ä¢ Generation time based on actual YAML content\")\n",
    "print(\"‚Ä¢ Safe to interrupt with Ctrl+C (progress saved)\")\n",
    "print(\"‚Ä¢ Progress saved every 50 items\")\n",
    "print(\"‚Ä¢ Automatically resumes from last checkpoint\")\n",
    "print(\"‚ö†Ô∏è Requires Network access to Huggingface or manual download into the hf_cache directory.\")\n",
    "print(\"\")\n",
    "\n",
    "if total_needed > 0:\n",
    "    print(f\"üöÄ Starting generation of {total_needed} remaining samples...\")\n",
    "else:\n",
    "    print(\"‚úÖ All language samples already complete! Checking metrics...\")\n",
    "\n",
    "# Generate datasets using dynamic calculation\n",
    "try:\n",
    "    language_data, metrics_data = generate_datasets(\n",
    "        language_count=None,  # Use dynamic calculation\n",
    "        metrics_count=CONFIG['metrics_samples']\n",
    "    )\n",
    "    \n",
    "    if language_data is not None or metrics_data is not None:\n",
    "        print(\"\\n‚úÖ Dataset generation completed!\")\n",
    "        if language_data:\n",
    "            print(f\"Language samples: {len(language_data)} new samples generated\")\n",
    "        print(f\"Metrics samples: {len(metrics_data.get('training_samples', []))} total\")\n",
    "        print(\"\\nNext: train() to train the model\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Generation interrupted or no new samples needed\")\n",
    "        print(\"Run this cell again to continue if needed\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚è∏Ô∏è  Generation interrupted by user\")\n",
    "    print(\"Progress saved. Run this cell again to continue.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Generation error: {e}\")\n",
    "    print(\"Check error and use retry_failed() if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3050304-c6f2-479f-a410-12df7e78e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train the distilled model\n",
    "print(\"üèãÔ∏è TRAINING DISTILLED MODEL\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Environment: {detect_training_environment()}\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(\"\")\n",
    "print(\"‚ö†Ô∏è  Training can take way long depending on hardware\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    success = train()\n",
    "    if success:\n",
    "        print(\"\\n‚úÖ Training completed!\")\n",
    "        print(\"Next: test() to test the model\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Training failed - check if datasets exist\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    print(\"Check logs for detailed error information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76354c-63a1-41b5-b04b-21d3d1bf7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Test model inference\n",
    "print(\"üß™ TESTING MODEL INFERENCE\")\n",
    "print(\"=\"*40)\n",
    "print(\"Testing with scenarios:\")\n",
    "print(\"‚Ä¢ Normal operation\")\n",
    "print(\"‚Ä¢ CPU spike\")\n",
    "print(\"‚Ä¢ Memory pressure\")\n",
    "print(\"\")\n",
    "\n",
    "test_success = test()\n",
    "\n",
    "if test_success:\n",
    "    print(\"\\n‚úÖ Model testing successful!\")\n",
    "    print(\"Next: demo() to run monitoring demo\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Testing failed - ensure model is trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce7e32-b023-4000-9b43-2520c268904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Run monitoring demo\n",
    "print(\"üé≠ MONITORING DEMO\")\n",
    "print(\"=\"*30)\n",
    "print(\"Features:\")\n",
    "print(\"‚Ä¢ Real-time metric processing\")\n",
    "print(\"‚Ä¢ Anomaly detection\")\n",
    "print(\"‚Ä¢ Alert generation\")\n",
    "print(\"‚Ä¢ Recommendation engine\")\n",
    "print(\"‚Ä¢ Dashboard display\")\n",
    "print(\"\")\n",
    "\n",
    "# Customize demo duration\n",
    "DEMO_MINUTES = 3\n",
    "\n",
    "print(f\"Running {DEMO_MINUTES}-minute demo...\")\n",
    "print(\"Will inject anomalies to demonstrate detection\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    demo(minutes=DEMO_MINUTES)\n",
    "    print(\"\\n‚úÖ Demo completed!\")\n",
    "    print(\"Check exported metrics history for results\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è  Demo stopped by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Demo error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10feba3f-2a33-43c4-9b07-c828ee8d94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Final system status\n",
    "print(\"üìã FINAL SYSTEM STATUS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "status()\n",
    "\n",
    "print(\"\\nüéâ SYSTEM COMPLETE!\")\n",
    "print(\"=\"*30)\n",
    "print(\"Your distilled monitoring system includes:\")\n",
    "print(\"  ‚úÖ Multi-model fallback system\")\n",
    "print(\"  ‚úÖ Local caching for portability\")\n",
    "print(\"  ‚úÖ Progress tracking and resume\")\n",
    "print(\"  ‚úÖ Trained monitoring model\")\n",
    "print(\"  ‚úÖ Real-time anomaly detection\")\n",
    "print(\"  ‚úÖ Actionable recommendations\")\n",
    "print(\"\")\n",
    "print(\"üîß NEXT STEPS:\")\n",
    "print(\"  ‚Ä¢ Integrate with your actual data sources\")\n",
    "print(\"  ‚Ä¢ Customize thresholds and alerts\")\n",
    "print(\"  ‚Ä¢ Set up continuous monitoring\")\n",
    "print(\"  ‚Ä¢ Implement feedback loops for learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c6d67-30e1-4c99-b74c-e85253161dfc",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Troubleshooting & Recovery\n",
    "\n",
    "### Common Commands:\n",
    "- `status()` - Complete system status\n",
    "- `show_progress()` - Dataset generation progress\n",
    "- `retry_failed()` - Retry failed generation items\n",
    "- `reset_progress()` - Start dataset generation fresh\n",
    "\n",
    "### Common Issues:\n",
    "- **Generation interrupted:** Just run the generation cell again\n",
    "- **Failed generations:** Use `retry_failed()`\n",
    "- **Want to start over:** Use `reset_progress()`\n",
    "- **Memory issues:** Reduce `CONFIG['batch_size']`\n",
    "- **No models available:** Check Ollama is running\n",
    "\n",
    "### Configuration:\n",
    "Modify `CONFIG` in `config.py` or use:\n",
    "```python\n",
    "CONFIG['language_samples'] = 2000  # Increase dataset size\n",
    "CONFIG['batch_size'] = 8           # Reduce for less memory\n",
    "CONFIG['epochs'] = 5               # More training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244f206b-6117-4326-abb8-e447ce002553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:config:üìã Batch discovered 19 Ollama models\n",
      "INFO:config:üìÅ Efficiently discovered 4 local models\n",
      "INFO:config:üìã Discovered 22 total models\n",
      "INFO:config:üéØ Built rotation pool: 18 models\n",
      "INFO:config:   ollama: 15 models\n",
      "INFO:config:   local: 2 models\n",
      "INFO:config:   static: 1 models\n",
      "INFO:config:‚úÖ Enhanced model chain initialized\n",
      "INFO:config:   Total models: 22\n",
      "INFO:config:   Rotation pool: 18\n",
      "INFO:distilled_model_trainer:üéÆ CUDA GPU: NVIDIA GeForce RTX 4090\n",
      "INFO:distilled_model_trainer:üöÄ CUDA optimizations enabled\n",
      "WARNING:distilled_model_trainer:‚ö†Ô∏è Latest model incomplete: models\\distilled_monitoring_20250723_180627\n",
      "INFO:distilled_model_trainer:üÜï No valid checkpoint found, starting fresh\n",
      "INFO:distilled_model_trainer:üèãÔ∏è Starting model training...\n",
      "INFO:distilled_model_trainer:üìÅ Loading from local: pretrained\\bert-base-uncased\n",
      "INFO:distilled_model_trainer:‚úÖ Loaded local model: bert-base-uncased\n",
      "INFO:distilled_model_trainer:üöÄ Model compilation enabled\n",
      "INFO:distilled_model_trainer:üìä Discovering and loading datasets...\n",
      "INFO:distilled_model_trainer:üìÅ Discovered 4 dataset files\n",
      "INFO:distilled_model_trainer:  language: 2 files\n",
      "INFO:distilled_model_trainer:    - language_dataset.json (5.6MB)\n",
      "INFO:distilled_model_trainer:    - language_dataset-checkpoint.json (3.9MB)\n",
      "INFO:distilled_model_trainer:  metrics: 2 files\n",
      "INFO:distilled_model_trainer:    - metrics_dataset.json (32.9MB)\n",
      "INFO:distilled_model_trainer:    - metrics_dataset-checkpoint.json (7.2MB)\n",
      "INFO:distilled_model_trainer:‚úÖ Loaded language_dataset.json\n",
      "INFO:distilled_model_trainer:‚úÖ Extracted 2397 samples from language_dataset.json\n",
      "INFO:distilled_model_trainer:‚úÖ Loaded language_dataset-checkpoint.json\n",
      "INFO:distilled_model_trainer:‚úÖ Extracted 2399 samples from language_dataset-checkpoint.json\n",
      "INFO:distilled_model_trainer:‚úÖ Loaded metrics_dataset.json\n",
      "INFO:distilled_model_trainer:‚úÖ Extracted 50000 samples from metrics_dataset.json\n",
      "INFO:distilled_model_trainer:‚úÖ Loaded metrics_dataset-checkpoint.json\n",
      "INFO:distilled_model_trainer:‚úÖ Extracted 10000 samples from metrics_dataset-checkpoint.json\n",
      "INFO:distilled_model_trainer:üßπ Cleaned: 64710/64796 samples retained\n",
      "INFO:distilled_model_trainer:üéØ Final dataset: 64710 samples\n",
      "INFO:distilled_model_trainer:   Labels: 2 unique\n",
      "INFO:distilled_model_trainer:   Anomalies: 12156 anomaly samples\n",
      "INFO:distilled_model_trainer:üî§ Tokenizing training data...\n",
      "INFO:distilled_model_trainer:‚úÖ Training dataset ready: 64710 samples\n",
      "INFO:distilled_model_trainer:üìö Epoch 1/1\n",
      "INFO:distilled_model_trainer:  Step 0: Loss=29395314.0000 (0.0% complete)\n",
      "INFO:distilled_model_trainer:  Step 50: Loss=31270694.0000 (1.6% complete)\n",
      "INFO:distilled_model_trainer:  Step 100: Loss=27863362.0000 (3.1% complete)\n",
      "INFO:distilled_model_trainer:  Step 150: Loss=21387500.0000 (4.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 200: Loss=30612222.0000 (6.2% complete)\n",
      "INFO:distilled_model_trainer:  Step 250: Loss=30966534.0000 (7.8% complete)\n",
      "INFO:distilled_model_trainer:  Step 300: Loss=40777772.0000 (9.3% complete)\n",
      "INFO:distilled_model_trainer:  Step 350: Loss=30423178.0000 (10.8% complete)\n",
      "INFO:distilled_model_trainer:  Step 400: Loss=21342286.0000 (12.4% complete)\n",
      "INFO:distilled_model_trainer:  Step 450: Loss=31210154.0000 (13.9% complete)\n",
      "INFO:distilled_model_trainer:  Step 500: Loss=25747462.0000 (15.5% complete)\n",
      "INFO:distilled_model_trainer:  Step 550: Loss=34591832.0000 (17.0% complete)\n",
      "INFO:distilled_model_trainer:  Step 600: Loss=28204214.0000 (18.6% complete)\n",
      "INFO:distilled_model_trainer:  Step 650: Loss=31143848.0000 (20.1% complete)\n",
      "INFO:distilled_model_trainer:  Step 700: Loss=25959806.0000 (21.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 750: Loss=24207692.0000 (23.2% complete)\n",
      "INFO:distilled_model_trainer:  Step 800: Loss=27615760.0000 (24.8% complete)\n",
      "INFO:distilled_model_trainer:  Step 850: Loss=32169278.0000 (26.3% complete)\n",
      "INFO:distilled_model_trainer:  Step 900: Loss=25121154.0000 (27.8% complete)\n",
      "INFO:distilled_model_trainer:  Step 950: Loss=39884528.0000 (29.4% complete)\n",
      "INFO:distilled_model_trainer:  Step 1000: Loss=32693054.0000 (30.9% complete)\n",
      "INFO:distilled_model_trainer:  Step 1050: Loss=26105588.0000 (32.5% complete)\n",
      "INFO:distilled_model_trainer:  Step 1100: Loss=33086814.0000 (34.0% complete)\n",
      "INFO:distilled_model_trainer:  Step 1150: Loss=29242826.0000 (35.6% complete)\n",
      "INFO:distilled_model_trainer:  Step 1200: Loss=27662752.0000 (37.1% complete)\n",
      "INFO:distilled_model_trainer:  Step 1250: Loss=33243322.0000 (38.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 1300: Loss=25596380.0000 (40.2% complete)\n",
      "INFO:distilled_model_trainer:  Step 1350: Loss=32129174.0000 (41.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 1400: Loss=32626456.0000 (43.3% complete)\n",
      "INFO:distilled_model_trainer:  Step 1450: Loss=35660268.0000 (44.8% complete)\n",
      "INFO:distilled_model_trainer:  Step 1500: Loss=32683352.0000 (46.4% complete)\n",
      "INFO:distilled_model_trainer:  Step 1550: Loss=32813310.0000 (47.9% complete)\n",
      "INFO:distilled_model_trainer:  Step 1600: Loss=30719660.0000 (49.5% complete)\n",
      "INFO:distilled_model_trainer:  Step 1650: Loss=26061086.0000 (51.0% complete)\n",
      "INFO:distilled_model_trainer:  Step 1700: Loss=35206604.0000 (52.6% complete)\n",
      "INFO:distilled_model_trainer:  Step 1750: Loss=24782992.0000 (54.1% complete)\n",
      "INFO:distilled_model_trainer:  Step 1800: Loss=32379254.0000 (55.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 1850: Loss=36379368.0000 (57.2% complete)\n",
      "INFO:distilled_model_trainer:  Step 1900: Loss=26768326.0000 (58.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 1950: Loss=27204934.0000 (60.3% complete)\n",
      "INFO:distilled_model_trainer:  Step 2000: Loss=35803200.0000 (61.8% complete)\n",
      "INFO:distilled_model_trainer:  Step 2050: Loss=25262718.0000 (63.4% complete)\n",
      "INFO:distilled_model_trainer:  Step 2100: Loss=30140660.0000 (64.9% complete)\n",
      "INFO:distilled_model_trainer:  Step 2150: Loss=26261744.0000 (66.5% complete)\n",
      "INFO:distilled_model_trainer:  Step 2200: Loss=38443416.0000 (68.0% complete)\n",
      "INFO:distilled_model_trainer:  Step 2250: Loss=22239856.0000 (69.6% complete)\n",
      "INFO:distilled_model_trainer:  Step 2300: Loss=30055460.0000 (71.1% complete)\n",
      "INFO:distilled_model_trainer:  Step 2350: Loss=28273350.0000 (72.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 2400: Loss=22157662.0000 (74.2% complete)\n",
      "INFO:distilled_model_trainer:  Step 2450: Loss=25135042.0000 (75.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 2500: Loss=29811064.0000 (77.3% complete)\n",
      "INFO:distilled_model_trainer:  Step 2550: Loss=33520976.0000 (78.8% complete)\n",
      "INFO:distilled_model_trainer:  Step 2600: Loss=24733784.0000 (80.4% complete)\n",
      "INFO:distilled_model_trainer:  Step 2650: Loss=35015504.0000 (81.9% complete)\n",
      "INFO:distilled_model_trainer:  Step 2700: Loss=30391054.0000 (83.5% complete)\n",
      "INFO:distilled_model_trainer:  Step 2750: Loss=27094432.0000 (85.0% complete)\n",
      "INFO:distilled_model_trainer:  Step 2800: Loss=35537668.0000 (86.6% complete)\n",
      "INFO:distilled_model_trainer:  Step 2850: Loss=30722462.0000 (88.1% complete)\n",
      "INFO:distilled_model_trainer:  Step 2900: Loss=35124000.0000 (89.6% complete)\n",
      "INFO:distilled_model_trainer:  Step 2950: Loss=22955436.0000 (91.2% complete)\n",
      "INFO:distilled_model_trainer:  Step 3000: Loss=37827460.0000 (92.7% complete)\n",
      "INFO:distilled_model_trainer:  Step 3050: Loss=35813212.0000 (94.3% complete)\n",
      "INFO:distilled_model_trainer:  Step 3100: Loss=29363576.0000 (95.8% complete)\n",
      "INFO:distilled_model_trainer:  Step 3150: Loss=27419074.0000 (97.4% complete)\n",
      "INFO:distilled_model_trainer:  Step 3200: Loss=24072642.0000 (98.9% complete)\n",
      "INFO:distilled_model_trainer:‚úÖ Epoch 1 completed: Avg Loss=29515632.9033\n",
      "INFO:distilled_model_trainer:üíæ Model saved to: models\\distilled_monitoring_20250723_183003\n",
      "INFO:distilled_model_trainer:üéâ Training completed in 0:12:22.853125\n",
      "INFO:distilled_model_trainer:   Best loss: 29515632.9033\n",
      "INFO:distilled_model_trainer:   Final model: models\\distilled_monitoring_20250723_183003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRITON_DISABLE_LINE_INFO'] = '1'\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "from transformers import AutoTokenizer\n",
    "from config import CONFIG\n",
    "from distilled_model_trainer import DistilledModelTrainer\n",
    "\n",
    "trainer = DistilledModelTrainer(CONFIG, resume_training=True)\n",
    "trainer.train()  # Updates latest model or creates new if none found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24ea69-3d60-470f-9b47-054243e86d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the moved model will work for training\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "def test_training_compatibility():\n",
    "    \"\"\"Test if the moved model is ready for training.\"\"\"\n",
    "    model_path = \"./pretrained/bert-base-uncased/\"\n",
    "    \n",
    "    print(\"üß™ TESTING TRAINING COMPATIBILITY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Load exactly as the training code will\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        print(\"Loading config...\")\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            config=config,\n",
    "            local_files_only=True,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ All components loaded successfully!\")\n",
    "        \n",
    "        # Test training-specific functionality\n",
    "        print(\"\\nTesting training features...\")\n",
    "        \n",
    "        # Check if model can be put in training mode\n",
    "        model.train()\n",
    "        print(\"‚úÖ Model can enter training mode\")\n",
    "        \n",
    "        # Test gradient computation\n",
    "        model.eval()\n",
    "        test_input = tokenizer(\"test\", return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        \n",
    "        # Enable gradients\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        output = model(**test_input)\n",
    "        loss = output.last_hidden_state.mean()  # Dummy loss\n",
    "        loss.backward()\n",
    "        \n",
    "        print(\"‚úÖ Gradient computation works\")\n",
    "        \n",
    "        # Check model size\n",
    "        param_count = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"‚úÖ Model parameters: {param_count:,} ({param_count/1e6:.1f}M)\")\n",
    "        \n",
    "        # Check config details\n",
    "        print(f\"‚úÖ Hidden size: {config.hidden_size}\")\n",
    "        print(f\"‚úÖ Vocab size: {config.vocab_size}\")\n",
    "        \n",
    "        print(f\"\\nüéâ MODEL IS READY FOR TRAINING!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training compatibility test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "success = test_training_compatibility()\n",
    "\n",
    "if success:\n",
    "    print(f\"\\n‚úÖ Your moved model in ./pretrained/bert-base-uncased/ is ready!\")\n",
    "    print(f\"The distilled_model_trainer.py should now work without internet.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå There may still be issues with the moved model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77385761-1e88-4d50-817f-061c6d70236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovery and troubleshooting commands\n",
    "print(\"üîß RECOVERY COMMANDS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"\\nüìä Progress Management:\")\n",
    "print(\"show_progress()    # Check current progress\")\n",
    "print(\"retry_failed()     # Retry failed items\")\n",
    "print(\"reset_progress()   # Start completely fresh\")\n",
    "\n",
    "print(\"\\nüîç Diagnostics:\")\n",
    "print(\"status()           # Complete system status\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  Current Configuration:\")\n",
    "print(f\"Language samples: {CONFIG['language_samples']}\")\n",
    "print(f\"Metrics samples: {CONFIG['metrics_samples']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Cache dir: {CONFIG['hf_cache_dir']}\")\n",
    "\n",
    "print(\"\\nüí° To modify configuration:\")\n",
    "print(\"CONFIG['language_samples'] = 2000\")\n",
    "print(\"CONFIG['batch_size'] = 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43291b78-5fe7-49ba-8cf9-bbff09eb1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run individual recovery commands\n",
    "# Uncomment as needed:\n",
    "\n",
    "# show_progress()      # Check progress\n",
    "# retry_failed()       # Retry failed items\n",
    "# reset_progress()     # Start fresh (WARNING: deletes progress)\n",
    "\n",
    "print(\"Uncomment commands above as needed for recovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f4317-f378-454f-8ad8-efba65620e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb702cdf-261c-4504-a54b-6b5b5506169a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbc05c-969c-475d-93e1-2abb8fe8ced3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
