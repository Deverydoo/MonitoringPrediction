{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea56623b-5100-4e38-9275-e8e00f5a7c27",
   "metadata": {},
   "source": [
    "# DISTILLED MONITORING SYSTEM\n",
    "\n",
    "## Predictive monitoring with local caching and fallback support\n",
    "\n",
    "### 🚀 QUICK WORKFLOW:\n",
    "1. `setup()` - Initialize system and fallbacks\n",
    "2. `generate_datasets()` - Generate training data (resumable)\n",
    "3. `train()` - Train the distilled model\n",
    "4. `test()` - Test model inference\n",
    "5. `demo()` - Run monitoring demo\n",
    "\n",
    "### 📊 MONITORING:\n",
    "- `status()` - Check system status\n",
    "- `show_progress()` - Check dataset generation progress\n",
    "\n",
    "### 🔧 RECOVERY:\n",
    "- `retry_failed()` - Retry failed generations\n",
    "- `reset_progress()` - Start fresh\n",
    "\n",
    "**Data Sources:** Splunk, Jira, Confluence, IBM Spectrum Conductor, VEMKD logs from Red Hat Linux\n",
    "\n",
    "**Fallback Order:** Remote API → Ollama → Local Model → Static Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6b1b6e-1fcb-4460-a06d-eaff33611c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:config:📋 Batch discovered 19 Ollama models\n",
      "INFO:config:📁 Efficiently discovered 4 local models\n",
      "INFO:config:📋 Discovered 22 total models\n",
      "INFO:config:🎯 Built rotation pool: 18 models\n",
      "INFO:config:   ollama: 15 models\n",
      "INFO:config:   local: 2 models\n",
      "INFO:config:   static: 1 models\n",
      "INFO:config:✅ Enhanced model chain initialized\n",
      "INFO:config:   Total models: 22\n",
      "INFO:config:   Rotation pool: 18\n",
      "INFO:distilled_model_trainer:🗂️ Enhanced logging initialized - local log: logs\\training_20250724_174024.log\n",
      "INFO:common_utils:✅ Loaded language_dataset.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Distilled Monitoring System\n",
      "📊 Predictive monitoring with dynamic discovery\n",
      "\n",
      "Type quick_start_guide() for usage instructions\n",
      "Type status() to check system status\n",
      "🚀 Distilled Monitoring System\n",
      "📊 Ready for predictive monitoring with local caching\n",
      "📁 Cache directory: ./hf_cache/\n"
     ]
    }
   ],
   "source": [
    "# Import the system\n",
    "from main_notebook import *\n",
    "from config import CONFIG\n",
    "\n",
    "CONFIG['model_name'] = \"bert-base-uncased\" # use local cached model instead of attempting to download. \n",
    "print(\"🚀 Distilled Monitoring System\")\n",
    "print(\"📊 Ready for predictive monitoring with local caching\")\n",
    "print(f\"📁 Cache directory: {CONFIG['hf_cache_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d867a0-11d9-4f5f-a14e-f3735d0913c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Setting up Distilled Monitoring System...\n",
      "This includes: directories, fallback systems, and progress tracking\n",
      "🚀 Setting up distilled monitoring system...\n",
      "\n",
      "==================================================\n",
      "FALLBACK SYSTEM SETUP\n",
      "==================================================\n",
      "Setting up comprehensive fallback system...\n",
      "\n",
      "1. Checking Remote LLM...\n",
      "   ❌ Remote LLM not configured\n",
      "\n",
      "2. Checking Ollama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:config:📝 Static responses created: 20 responses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Ollama available with 19 models\n",
      "      • qwen2.5-coder:latest\n",
      "      • falcon:latest\n",
      "      • qwen2.5vl:latest\n",
      "      • tinyllama:latest\n",
      "      • phi4:latest\n",
      "      • ... and 14 more\n",
      "\n",
      "3. Checking Local Models...\n",
      "      • microsoft_DialoGPT-medium\n",
      "      • bert-base-uncased\n",
      "   ✅ 2 local models available\n",
      "\n",
      "4. Setting up Static Fallback...\n",
      "   ✅ Static fallback responses created\n",
      "\n",
      "✅ Fallback system ready with 3 methods: Ollama, Local Models, Static Fallback\n",
      "Testing fallback system...\n",
      "\n",
      "Running test queries...\n",
      "\n",
      "Test 1: Explain what high CPU usage indicates in system mo...\n",
      "✅ Success: 849 chars, 1 model(s)\n",
      "   Sample: High CPU usage in system monitoring indicates that the central processing unit (...\n",
      "\n",
      "Test 2: What causes java.lang.OutOfMemoryError in applicat...\n",
      "✅ Success: 744 chars, 1 model(s)\n",
      "   Sample: The `java.lang.OutOfMemoryError` is a common error that occurs when an applicati...\n",
      "\n",
      "Test 3: How do you troubleshoot network connectivity issue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataset_generator:🔍 Discovering YAML files in data_config\n",
      "INFO:dataset_generator:  ✅ conversation_prompts.yaml: 499 items (conversation_patterns)\n",
      "INFO:dataset_generator:  ✅ english_patterns.yaml: 200 items (error_scenarios)\n",
      "INFO:dataset_generator:  ✅ error_patterns.yaml: 33 items (error_scenarios)\n",
      "INFO:dataset_generator:  ✅ metrics_patterns.yaml: 107 items (error_scenarios)\n",
      "INFO:dataset_generator:  ✅ personality_types.yaml: 40 items (personality_types)\n",
      "INFO:dataset_generator:  ✅ project_management_terms.yaml: 625 items (technical_terms)\n",
      "INFO:dataset_generator:  ✅ question_styles.yaml: 54 items (question_styles)\n",
      "INFO:dataset_generator:  ✅ response_templates.yaml: 48 items (general_knowledge)\n",
      "INFO:dataset_generator:  ✅ technical_terms.yaml: 1271 items (technical_terms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success: 629 chars, 1 model(s)\n",
      "   Sample: To troubleshoot network connectivity issues on Linux, you can follow these steps...\n",
      "\n",
      "📊 TEST RESULTS: 3/3 successful\n",
      "✅ Fallback system ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataset_generator:📊 Total YAML configs loaded: 9\n",
      "INFO:dataset_generator:📊 Resuming session session_20250718_113730\n",
      "INFO:dataset_generator:🔧 Extracted technical terms: 1352 terms across 15 categories\n",
      "INFO:dataset_generator:✅ Enhanced generator initialized\n",
      "INFO:dataset_generator:   YAML configs: 9\n",
      "INFO:dataset_generator:   Technical terms: 15\n",
      "INFO:dataset_generator:   Conversation styles: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Setup complete!\n",
      "\n",
      "✅ System setup complete!\n",
      "\n",
      "Next: generate_datasets() to create training data\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup system with fallback chain\n",
    "print(\"🚀 Setting up Distilled Monitoring System...\")\n",
    "print(\"This includes: directories, fallback systems, and progress tracking\")\n",
    "\n",
    "setup_success = setup()\n",
    "\n",
    "if setup_success:\n",
    "    print(\"\\n✅ System setup complete!\")\n",
    "    print(\"\\nNext: generate_datasets() to create training data\")\n",
    "else:\n",
    "    print(\"\\n❌ Setup failed. Check error messages above.\")\n",
    "    print(\"You may need to install dependencies or setup Ollama.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d1e239-fae1-4074-b484-24c18086b822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:common_utils:✅ Loaded language_dataset.json\n",
      "INFO:dataset_generator:🎯 Dynamic targets calculated:\n",
      "INFO:dataset_generator:   Base multiplier: 1.5\n",
      "INFO:dataset_generator:   Models per question: 1\n",
      "INFO:dataset_generator:   Total target: 2347\n",
      "INFO:common_utils:✅ Loaded language_dataset.json\n",
      "INFO:common_utils:✅ Loaded language_dataset.json\n",
      "INFO:config:🎮 CUDA GPU: NVIDIA GeForce RTX 4090 (22GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SYSTEM STATUS\n",
      "==================================================\n",
      "Setup: ✅\n",
      "Datasets: ❌\n",
      "Model: ✅\n",
      "Fallbacks: ✅\n",
      "\n",
      "======================================================================\n",
      "ENHANCED DATASET GENERATION PROGRESS\n",
      "======================================================================\n",
      "Session: session_20250718_113730\n",
      "Models per question: 1\n",
      "Save frequency: every 50 samples\n",
      "\n",
      "✅ technical_explanation:\n",
      "    Progress: 2540/2028 (125.2%)\n",
      "    Base: 2028 × 1 models = 2028 total\n",
      "    Remaining: 0\n",
      "\n",
      "🔄 conversation_example:\n",
      "    Progress: 0/165 (0.0%)\n",
      "    Base: 165 × 1 models = 165 total\n",
      "    Remaining: 165\n",
      "\n",
      "🔄 error_troubleshooting:\n",
      "    Progress: 0/69 (0.0%)\n",
      "    Base: 69 × 1 models = 69 total\n",
      "    Remaining: 69\n",
      "\n",
      "🔄 english_pattern:\n",
      "    Progress: 0/19 (0.0%)\n",
      "    Base: 19 × 1 models = 19 total\n",
      "    Remaining: 19\n",
      "\n",
      "🔄 personality_response:\n",
      "    Progress: 0/36 (0.0%)\n",
      "    Base: 36 × 1 models = 36 total\n",
      "    Remaining: 36\n",
      "\n",
      "🔄 question_style_variation:\n",
      "    Progress: 0/30 (0.0%)\n",
      "    Base: 30 × 1 models = 30 total\n",
      "    Remaining: 30\n",
      "\n",
      "📈 OVERALL: 2540/2347 (108.2%)\n",
      "🎯 Remaining: 319 samples\n",
      "💾 Saves completed: 0\n",
      "======================================================================\n",
      "\n",
      "Files:\n",
      "  Language Dataset: ✅\n",
      "    Samples: 3314\n",
      "  Metrics Dataset: ❌\n",
      "  Trained Model: ✅\n",
      "\n",
      "Environment: cuda\n",
      "Time: 2025-07-24 17:40:40\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 2. Check system status\n",
    "status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404acda-847f-4513-a50a-d0860f2a0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check dataset generation progress (if any)\n",
    "print(\"📊 Current dataset generation progress:\")\n",
    "show_progress()\n",
    "\n",
    "print(\"\\n💡 TIPS:\")\n",
    "print(\"• First run: Will show new session\")\n",
    "print(\"• Resuming: Will show existing progress\")\n",
    "print(\"• Use reset_progress() to start fresh\")\n",
    "print(\"• Use retry_failed() to retry failed items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a9550-6e3f-4243-81c9-6be41311c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use with caution.\n",
    "# reset_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42cebbd-93b8-4330-85b5-fb277bd6cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataset_generator:🗣️ Starting complete dataset generation\n",
      "INFO:dataset_generator:🎯 Dynamic targets calculated:\n",
      "INFO:dataset_generator:   Base multiplier: 1.5\n",
      "INFO:dataset_generator:   Models per question: 1\n",
      "INFO:dataset_generator:   Total target: 2347\n",
      "INFO:dataset_generator:🗣️ Starting rich language generation: 2347 target\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 DATASET GENERATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:common_utils:✅ Loaded language_dataset.json\n",
      "INFO:dataset_generator:✅ Target already met: 3314/2347\n",
      "INFO:dataset_generator:📊 Starting metrics generation: 10000 target\n",
      "WARNING:common_utils:Dataset file not found: training\\metrics_dataset.json\n",
      "INFO:dataset_generator:📊 Need to generate: 10000 metrics samples\n",
      "INFO:common_utils:💾 Saved metrics_dataset.json\n",
      "INFO:dataset_generator:✅ Metrics generation complete: 10000 total\n",
      "INFO:common_utils:✅ Loaded language_dataset.json\n",
      "INFO:common_utils:✅ Loaded metrics_dataset.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset generation completed successfully!\n",
      "   Language dataset: 0\n",
      "   Metrics dataset: Updated\n",
      "✅ Dataset generation completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Generate the datasets. \n",
    "generate_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3050304-c6f2-479f-a410-12df7e78e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train the distilled model\n",
    "print(\"🏋️ TRAINING DISTILLED MODEL\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Environment: {detect_training_environment()}\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(\"\")\n",
    "print(\"⚠️  Training can take way long depending on hardware\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    success = train()\n",
    "    if success:\n",
    "        print(\"\\n✅ Training completed!\")\n",
    "        print(\"Next: test() to test the model\")\n",
    "    else:\n",
    "        print(\"\\n❌ Training failed - check if datasets exist\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training error: {e}\")\n",
    "    print(\"Check logs for detailed error information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76354c-63a1-41b5-b04b-21d3d1bf7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Test model inference\n",
    "print(\"🧪 TESTING MODEL INFERENCE\")\n",
    "print(\"=\"*40)\n",
    "print(\"Testing with scenarios:\")\n",
    "print(\"• Normal operation\")\n",
    "print(\"• CPU spike\")\n",
    "print(\"• Memory pressure\")\n",
    "print(\"\")\n",
    "\n",
    "test_success = test()\n",
    "\n",
    "if test_success:\n",
    "    print(\"\\n✅ Model testing successful!\")\n",
    "    print(\"Next: demo() to run monitoring demo\")\n",
    "else:\n",
    "    print(\"\\n❌ Testing failed - ensure model is trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce7e32-b023-4000-9b43-2520c268904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Run monitoring demo\n",
    "print(\"🎭 MONITORING DEMO\")\n",
    "print(\"=\"*30)\n",
    "print(\"Features:\")\n",
    "print(\"• Real-time metric processing\")\n",
    "print(\"• Anomaly detection\")\n",
    "print(\"• Alert generation\")\n",
    "print(\"• Recommendation engine\")\n",
    "print(\"• Dashboard display\")\n",
    "print(\"\")\n",
    "\n",
    "# Customize demo duration\n",
    "DEMO_MINUTES = 3\n",
    "\n",
    "print(f\"Running {DEMO_MINUTES}-minute demo...\")\n",
    "print(\"Will inject anomalies to demonstrate detection\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    demo(minutes=DEMO_MINUTES)\n",
    "    print(\"\\n✅ Demo completed!\")\n",
    "    print(\"Check exported metrics history for results\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⏹️  Demo stopped by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Demo error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10feba3f-2a33-43c4-9b07-c828ee8d94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Final system status\n",
    "print(\"📋 FINAL SYSTEM STATUS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "status()\n",
    "\n",
    "print(\"\\n🎉 SYSTEM COMPLETE!\")\n",
    "print(\"=\"*30)\n",
    "print(\"Your distilled monitoring system includes:\")\n",
    "print(\"  ✅ Multi-model fallback system\")\n",
    "print(\"  ✅ Local caching for portability\")\n",
    "print(\"  ✅ Progress tracking and resume\")\n",
    "print(\"  ✅ Trained monitoring model\")\n",
    "print(\"  ✅ Real-time anomaly detection\")\n",
    "print(\"  ✅ Actionable recommendations\")\n",
    "print(\"\")\n",
    "print(\"🔧 NEXT STEPS:\")\n",
    "print(\"  • Integrate with your actual data sources\")\n",
    "print(\"  • Customize thresholds and alerts\")\n",
    "print(\"  • Set up continuous monitoring\")\n",
    "print(\"  • Implement feedback loops for learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c6d67-30e1-4c99-b74c-e85253161dfc",
   "metadata": {},
   "source": [
    "## 🛠️ Troubleshooting & Recovery\n",
    "\n",
    "### Common Commands:\n",
    "- `status()` - Complete system status\n",
    "- `show_progress()` - Dataset generation progress\n",
    "- `retry_failed()` - Retry failed generation items\n",
    "- `reset_progress()` - Start dataset generation fresh\n",
    "\n",
    "### Common Issues:\n",
    "- **Generation interrupted:** Just run the generation cell again\n",
    "- **Failed generations:** Use `retry_failed()`\n",
    "- **Want to start over:** Use `reset_progress()`\n",
    "- **Memory issues:** Reduce `CONFIG['batch_size']`\n",
    "- **No models available:** Check Ollama is running\n",
    "\n",
    "### Configuration:\n",
    "Modify `CONFIG` in `config.py` or use:\n",
    "```python\n",
    "CONFIG['language_samples'] = 2000  # Increase dataset size\n",
    "CONFIG['batch_size'] = 8           # Reduce for less memory\n",
    "CONFIG['epochs'] = 5               # More training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f206b-6117-4326-abb8-e447ce002553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRITON_DISABLE_LINE_INFO'] = '1'\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "from transformers import AutoTokenizer\n",
    "from config import CONFIG\n",
    "from distilled_model_trainer import DistilledModelTrainer\n",
    "\n",
    "trainer = DistilledModelTrainer(CONFIG, resume_training=True)\n",
    "trainer.train()  # Updates latest model or creates new if none found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24ea69-3d60-470f-9b47-054243e86d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the moved model will work for training\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "def test_training_compatibility():\n",
    "    \"\"\"Test if the moved model is ready for training.\"\"\"\n",
    "    model_path = \"./pretrained/bert-base-uncased/\"\n",
    "    \n",
    "    print(\"🧪 TESTING TRAINING COMPATIBILITY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Load exactly as the training code will\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        print(\"Loading config...\")\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        print(\"Loading model...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path,\n",
    "            config=config,\n",
    "            local_files_only=True,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        print(\"✅ All components loaded successfully!\")\n",
    "        \n",
    "        # Test training-specific functionality\n",
    "        print(\"\\nTesting training features...\")\n",
    "        \n",
    "        # Check if model can be put in training mode\n",
    "        model.train()\n",
    "        print(\"✅ Model can enter training mode\")\n",
    "        \n",
    "        # Test gradient computation\n",
    "        model.eval()\n",
    "        test_input = tokenizer(\"test\", return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        \n",
    "        # Enable gradients\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        output = model(**test_input)\n",
    "        loss = output.last_hidden_state.mean()  # Dummy loss\n",
    "        loss.backward()\n",
    "        \n",
    "        print(\"✅ Gradient computation works\")\n",
    "        \n",
    "        # Check model size\n",
    "        param_count = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"✅ Model parameters: {param_count:,} ({param_count/1e6:.1f}M)\")\n",
    "        \n",
    "        # Check config details\n",
    "        print(f\"✅ Hidden size: {config.hidden_size}\")\n",
    "        print(f\"✅ Vocab size: {config.vocab_size}\")\n",
    "        \n",
    "        print(f\"\\n🎉 MODEL IS READY FOR TRAINING!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training compatibility test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "success = test_training_compatibility()\n",
    "\n",
    "if success:\n",
    "    print(f\"\\n✅ Your moved model in ./pretrained/bert-base-uncased/ is ready!\")\n",
    "    print(f\"The distilled_model_trainer.py should now work without internet.\")\n",
    "else:\n",
    "    print(f\"\\n❌ There may still be issues with the moved model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77385761-1e88-4d50-817f-061c6d70236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovery and troubleshooting commands\n",
    "print(\"🔧 RECOVERY COMMANDS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"\\n📊 Progress Management:\")\n",
    "print(\"show_progress()    # Check current progress\")\n",
    "print(\"retry_failed()     # Retry failed items\")\n",
    "print(\"reset_progress()   # Start completely fresh\")\n",
    "\n",
    "print(\"\\n🔍 Diagnostics:\")\n",
    "print(\"status()           # Complete system status\")\n",
    "\n",
    "print(\"\\n⚙️  Current Configuration:\")\n",
    "print(f\"Language samples: {CONFIG['language_samples']}\")\n",
    "print(f\"Metrics samples: {CONFIG['metrics_samples']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Cache dir: {CONFIG['hf_cache_dir']}\")\n",
    "\n",
    "print(\"\\n💡 To modify configuration:\")\n",
    "print(\"CONFIG['language_samples'] = 2000\")\n",
    "print(\"CONFIG['batch_size'] = 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43291b78-5fe7-49ba-8cf9-bbff09eb1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run individual recovery commands\n",
    "# Uncomment as needed:\n",
    "\n",
    "# show_progress()      # Check progress\n",
    "# retry_failed()       # Retry failed items\n",
    "# reset_progress()     # Start fresh (WARNING: deletes progress)\n",
    "\n",
    "print(\"Uncomment commands above as needed for recovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f4317-f378-454f-8ad8-efba65620e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb702cdf-261c-4504-a54b-6b5b5506169a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbc05c-969c-475d-93e1-2abb8fe8ced3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
