# NordIQ AI Systems

## Predictive Infrastructure Monitoring

> **Nordic precision, AI intelligence**
>
> Predict server incidents 8 hours in advance with 88% accuracy
> Production-ready AI for infrastructure monitoring and proactive incident prevention

[![Version](https://img.shields.io/badge/version-1.1.0-blue.svg)](VERSION)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: BSL 1.1](https://img.shields.io/badge/License-BSL%201.1-yellow.svg)](LICENSE)

**Version 1.1.0** - NordIQ AI branding release | [Changelog](CHANGELOG.md)

**Copyright © 2025 NordIQ AI, LLC. All rights reserved.**
Developed by Craig Giannelli

![Dashboard Preview](preview.webp)

---

## 🎯 What This Does

This system uses **Temporal Fusion Transformers (TFT)** to predict server incidents before they happen. It monitors your infrastructure in real-time and alerts you to problems **hours before** they become critical.

**Key Features:**
- 🔮 **8-hour advance warning** of critical incidents
- 📊 **88% prediction accuracy** on server failures
- 🚀 **Real-time monitoring** via REST API + WebSocket
- 🎨 **Interactive web dashboard** built with Streamlit
- 🧠 **Transfer learning** - new servers get accurate predictions immediately
- ⚡ **GPU-accelerated** inference with RTX optimization
- 🔄 **Automatic retraining** pipeline for fleet changes

---

## 🚀 Quick Start

### One-Command Startup

Navigate to the **NordIQ/** application directory and run:

```bash
# Windows
cd NordIQ
start_all.bat

# Linux/Mac
cd NordIQ
./start_all.sh
```

**That's it!** The system will automatically:
- ✅ Generate/verify API keys
- ✅ Start inference daemon (port 8000)
- ✅ Start metrics generator (demo data)
- ✅ Launch web dashboard (port 8501)

**Dashboard URL:** http://localhost:8501
**API URL:** http://localhost:8000

> **Note:** All application files are now in the `NordIQ/` directory for clean deployment. See [NordIQ/README.md](NordIQ/README.md) for detailed deployment guide.

---

## 💡 Why This Exists

**The Problem:**
- Server outages cost $50K-$100K+ per incident
- Most monitoring is **reactive** - alerts fire when it's already too late
- Emergency fixes happen at 3 AM with customer impact

**The Solution:**
- Predict incidents **8 hours ahead** with TFT deep learning
- Fix problems during business hours with **planned maintenance**
- Avoid SLA penalties, lost revenue, and emergency overtime

**One avoided outage pays for this entire system.**

---

## 📊 The Numbers

| Metric | Value |
|--------|-------|
| **Prediction Horizon** | 8 hours (96 timesteps) |
| **Accuracy** | 88% on critical incidents |
| **Context Window** | 24 hours (288 timesteps) |
| **Fleet Size** | 20-90 servers (scalable) |
| **Inference Speed** | <100ms per server (GPU) |
| **Model Size** | 88K parameters |
| **Training Time** | ~30 min on RTX 4090 |
| **Development Time** | 67.5 hours total |

---

## 🏗️ Architecture

### Development/Training Pipeline
```
┌─────────────────────────────────────────────────┐
│  NordIQ/src/generators/metrics_generator.py     │
│  Generates realistic server metrics             │
│  → NordIQ/data/training/*.parquet               │
└─────────────────┬───────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────┐
│  NordIQ/src/training/tft_trainer.py             │
│  Trains Temporal Fusion Transformer             │
│  → NordIQ/models/tft_model_*/                   │
└─────────────────────────────────────────────────┘
```

### Production Runtime Architecture (Microservices)
```
┌───────────────────────────────────────────────────────────┐
│  MongoDB / Elasticsearch                                  │
│  Production metrics from Linborg monitoring               │
└─────────────────┬─────────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────┐
│  NordIQ/src/core/adapters/*_adapter.py          │
│  Fetches metrics every 5s, forwards to daemon   │
│  ↓ HTTP POST /feed                              │
└─────────────────┬───────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────┐
│  NordIQ/src/daemons/tft_inference_daemon.py     │
│  Production inference server                    │
│  Port 8000 - REST API + WebSocket               │
│  ↓ HTTP GET /predict                            │
└─────────────────┬───────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────┐
│  NordIQ/src/dashboard/tft_dashboard_web.py      │
│  Interactive Streamlit dashboard                │
│  → http://localhost:8501                        │
└─────────────────────────────────────────────────┘
```

**⚠️ CRITICAL:** Adapters run as **independent daemons** that actively PUSH data to the inference daemon. See **[Docs/ADAPTER_ARCHITECTURE.md](Docs/ADAPTER_ARCHITECTURE.md)** for complete details on the microservices architecture.

---

## 🎨 Dashboard Features

### 1. Fleet Overview
- Real-time fleet health status (20/20 servers monitored)
- Environment incident probability
- Active alerts and risk distribution

### 2. Server Heatmap
- Visual grid of all servers
- Color-coded by risk level (green/yellow/red)
- Grouped by server profile

### 3. Top Problem Servers
- Ranked by incident risk score
- TFT predictions for next 8 hours
- Specific failure modes (CPU, memory, disk)

### 4. Historical Trends
- Prediction confidence over time
- Metric evolution charts
- Pattern recognition insights

### 5. Interactive Demo Mode
- **Healthy → Degrading → Critical** scenarios
- Watch the model detect patterns in real-time
- Perfect for presentations and testing

---

## 🧠 The Secret Sauce: Profile-Based Transfer Learning

Most AI treats every server as unique. This system is smarter.

**7 Server Profiles:**
```python
ml_compute      # ML training nodes (high CPU/memory)
database        # Databases (disk I/O intensive)
web_api         # Web servers (network heavy)
conductor_mgmt  # Orchestration systems
data_ingest     # ETL pipelines
risk_analytics  # Risk calculation nodes
generic         # Catch-all for other workloads
```

**Why This Matters:**
- New server `ppml0099` comes online → Model sees `ppml` prefix
- Instantly applies all ML server patterns learned during training
- **Strong predictions from day 1** with zero retraining
- Reduces retraining frequency by **80%** (every 2 months vs every 2 weeks)

---

## 📦 Installation

### Prerequisites
```bash
# Python 3.10+
# CUDA 11.8+ (for GPU acceleration)
# 16GB+ RAM recommended
```

### Setup
```bash
# 1. Clone repository
git clone https://github.com/yourusername/MonitoringPrediction.git
cd MonitoringPrediction

# 2. Create conda environment
conda create -n py310 python=3.10
conda activate py310

# 3. Install dependencies
pip install -r requirements.txt

# 4. Verify GPU (optional but recommended)
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# 5. Navigate to application directory
cd NordIQ
```

---

## 📁 Project Structure

```
MonitoringPrediction/
├── NordIQ/                          # 🎯 Main Application (Deploy This!)
│   ├── start_all.bat/sh             # One-command startup
│   ├── stop_all.bat/sh              # Stop all services
│   ├── README.md                    # Deployment guide
│   │
│   ├── bin/                         # Utility scripts
│   │   ├── generate_api_key.py      # API key management
│   │   └── setup_api_key.*          # Setup helpers
│   │
│   ├── src/                         # Application source code
│   │   ├── daemons/                 # Background services
│   │   │   ├── tft_inference_daemon.py
│   │   │   ├── metrics_generator_daemon.py
│   │   │   └── adaptive_retraining_daemon.py
│   │   ├── dashboard/               # Web interface
│   │   │   ├── tft_dashboard_web.py
│   │   │   └── Dashboard/           # Modular components
│   │   ├── training/                # Model training
│   │   │   ├── main.py              # CLI interface
│   │   │   ├── tft_trainer.py       # Training engine
│   │   │   └── precompile.py        # Optimization
│   │   ├── core/                    # Shared libraries
│   │   │   ├── config/              # Configuration
│   │   │   ├── utils/               # Utilities
│   │   │   ├── adapters/            # Production adapters
│   │   │   ├── explainers/          # XAI components
│   │   │   └── *.py                 # Core modules
│   │   └── generators/              # Data generation
│   │       └── metrics_generator.py
│   │
│   ├── models/                      # Trained models
│   ├── data/                        # Runtime data
│   ├── logs/                        # Application logs
│   └── .streamlit/                  # Dashboard config
│
├── Docs/                            # Documentation
│   ├── RAG/                         # For AI assistants
│   └── *.md                         # User guides
├── BusinessPlanning/                # Confidential (gitignored)
├── tools/                           # Development tools
├── README.md                        # This file
├── CHANGELOG.md                     # Version history
├── VERSION                          # Current version (1.1.0)
└── LICENSE                          # BSL 1.1
```

**Key Points:**
- 🎯 **Deploy**: Copy the entire `NordIQ/` folder
- 📚 **Learn**: Read `Docs/` for guides and architecture
- 🔐 **Business**: `BusinessPlanning/` is gitignored (confidential)
- 🛠️ **Dev**: Root contains development/documentation files

---

## 🎓 Training & Configuration

### Option 1: Using CLI (Recommended)

Navigate to NordIQ directory first:
```bash
cd NordIQ
```

Then use the training CLI:
```bash
# Generate 30 days of realistic metrics (20 servers)
python src/training/main.py generate --servers 20 --hours 720

# Train model (20 epochs)
python src/training/main.py train --epochs 20

# Check status
python src/training/main.py status
```

### Option 2: Direct Commands

```bash
cd NordIQ

# Generate training data
python src/generators/metrics_generator.py --servers 20 --hours 720

# Train model
python src/training/tft_trainer.py --epochs 20

# Data saved to: NordIQ/data/training/*.parquet
# Model saved to: NordIQ/models/tft_model_*/
```

**Time:** ~30-60 seconds for data generation, ~30-40 minutes for training on RTX 4090

### Configuration

All configuration is in `NordIQ/src/core/config/`:
- `model_config.py` - Model hyperparameters
- `metrics_config.py` - Server profiles and baselines
- `api_config.py` - API and authentication settings

To customize, edit these files before training.

---

## 🔌 API Usage

### REST API

```bash
# Health check
curl http://localhost:8000/health

# Current predictions
curl http://localhost:8000/predictions/current

# Specific server prediction
curl http://localhost:8000/predict/ppml0001

# Active alerts
curl http://localhost:8000/alerts/active

# Fleet status
curl http://localhost:8000/status
```

### WebSocket (Real-time)

```javascript
const ws = new WebSocket('ws://localhost:8000/ws');

ws.onmessage = (event) => {
  const prediction = JSON.parse(event.data);
  console.log(`Server ${prediction.server_id}: ${prediction.risk_score}`);
};
```

---

## 🛠️ Project Structure

```
MonitoringPrediction/
├── 📄 _StartHere.ipynb              # Interactive notebook walkthrough
├── 🔧 config.py                     # System configuration
├── 📊 metrics_generator.py          # Training data generator
├── 🧠 tft_trainer.py                # Model training
├── ⚡ tft_inference.py              # Production inference daemon
├── 🎨 tft_dashboard_web.py          # Streamlit web dashboard
├── 🔐 data_validator.py             # Contract validation
├── 🔑 server_encoder.py             # Hash-based server encoding
├── 🎮 gpu_profiles.py               # GPU optimization profiles
├── 📁 training/                     # Training data directory
│   ├── server_metrics.parquet       # Generated metrics
│   └── server_mapping.json          # Server encoder mapping
├── 📁 models/                       # Trained models
│   └── tft_model_YYYYMMDD_HHMMSS/
│       ├── model.safetensors        # Model weights
│       ├── dataset_parameters.pkl   # Trained encoders (CRITICAL!)
│       ├── server_mapping.json      # Server encoder
│       ├── training_info.json       # Contract metadata
│       └── config.json              # Model architecture
└── 📁 Docs/                         # Comprehensive documentation
    ├── ESSENTIAL_RAG.md             # Complete system reference (1200 lines)
    ├── DATA_CONTRACT.md             # Schema specification
    ├── QUICK_START.md               # Fast onboarding
    ├── DASHBOARD_GUIDE.md           # Dashboard features
    ├── SERVER_PROFILES.md           # Transfer learning design
    └── PROJECT_CODEX.md             # Architecture deep dive
```

---

## 🔬 Technical Innovations

### 1. Hash-Based Server Encoding
**Problem:** Sequential IDs break when fleet changes
**Solution:** Deterministic SHA256-based encoding

```python
# Before (breaks easily)
ppml0001 → 0
ppml0002 → 1
# Add ppml0003? All IDs shift!

# After (stable)
ppml0001 → hash('ppml0001') → '285039'  # Always the same
ppml0002 → hash('ppml0002') → '215733'  # Deterministic
ppml0003 → hash('ppml0003') → '921211'  # No conflicts
```

### 2. Data Contract System
**Problem:** Schema mismatches break models
**Solution:** Single source of truth for all components

```python
# DATA_CONTRACT.md defines:
✅ Valid states: ['healthy', 'heavy_load', 'critical_issue', ...]
✅ Required features: cpu_percent, memory_percent, disk_percent, ...
✅ Encoding methods: hash-based server IDs, NaN handling
✅ Version tracking: v1.0.0 compatibility checks
```

### 3. Encoder Persistence
**Problem:** TFT encoders lost between training/inference
**Solution:** Save `dataset_parameters.pkl` with trained vocabularies

```python
# Training saves:
dataset_parameters.pkl → {
  'server_id': NaNLabelEncoder(vocabulary=['285039', '215733', ...]),
  'status': NaNLabelEncoder(vocabulary=['healthy', 'critical_issue', ...]),
  'profile': NaNLabelEncoder(vocabulary=['ml_compute', 'database', ...])
}

# Inference loads → All servers recognized!
```

---

## 📈 Performance

### Training Performance
| Dataset Size | Epochs | GPU | Time |
|--------------|--------|-----|------|
| 24 hours | 20 | RTX 4090 | ~8 min |
| 168 hours (1 week) | 20 | RTX 4090 | ~15 min |
| 720 hours (30 days) | 20 | RTX 4090 | ~30 min |

### Inference Performance
| Fleet Size | Batch | GPU | Latency |
|------------|-------|-----|---------|
| 20 servers | 1 | RTX 4090 | ~50ms |
| 90 servers | 1 | RTX 4090 | ~85ms |
| 20 servers | 20 | RTX 4090 | ~120ms |

### Data Loading (Parquet vs JSON)
| Format | 24h | 168h | 720h |
|--------|-----|------|------|
| **JSON** | 2.1s | 15.3s | 68.7s |
| **Parquet** | 0.12s | 0.45s | 1.8s |
| **Speedup** | **17.5x** | **34x** | **38x** |

---

## 🎯 Use Cases

### 1. Proactive Incident Prevention
- Predict memory exhaustion 8 hours ahead
- Schedule maintenance during business hours
- Avoid 3 AM emergency wake-up calls

### 2. Capacity Planning
- Identify servers approaching resource limits
- Forecast infrastructure needs
- Optimize server allocation

### 3. SLA Protection
- Get early warning before SLA violations
- Prevent customer-impacting outages
- Reduce penalty costs

### 4. Cost Optimization
- Rightsize over-provisioned servers
- Identify idle resources
- Reduce cloud spend

---

## 📚 Documentation

Comprehensive docs in `/Docs/`:

### Core Documentation
- **[ESSENTIAL_RAG.md](Docs/ESSENTIAL_RAG.md)** - Complete system reference (1200 lines)
- **[QUICK_START.md](Docs/QUICK_START.md)** - Get started in 30 seconds
- **[DATA_CONTRACT.md](Docs/DATA_CONTRACT.md)** - Schema specification (MUST READ)
- **[DASHBOARD_GUIDE.md](Docs/DASHBOARD_GUIDE.md)** - Dashboard features walkthrough
- **[SERVER_PROFILES.md](Docs/SERVER_PROFILES.md)** - Transfer learning design
- **[PROJECT_CODEX.md](Docs/PROJECT_CODEX.md)** - Deep architecture dive

### Production Integration
- **[ADAPTER_ARCHITECTURE.md](Docs/ADAPTER_ARCHITECTURE.md)** - ⚠️ CRITICAL: How adapters work (microservices)
- **[PRODUCTION_DATA_ADAPTERS.md](Docs/PRODUCTION_DATA_ADAPTERS.md)** - MongoDB/Elasticsearch integration
- **[adapters/README.md](adapters/README.md)** - Complete adapter guide (100+ pages)
- **[ADAPTIVE_RETRAINING_PLAN.md](Docs/ADAPTIVE_RETRAINING_PLAN.md)** - Drift detection & retraining
- **[PERFORMANCE_OPTIMIZATION.md](Docs/PERFORMANCE_OPTIMIZATION.md)** - Bytecode caching & optimization

### Architecture
- **[UNKNOWN_SERVER_HANDLING.md](Docs/UNKNOWN_SERVER_HANDLING.md)** - How new servers work

---

## 🤝 Contributing

Contributions welcome! Areas for improvement:

- [ ] Additional server profiles (Kubernetes, message queues, caches)
- [ ] Multi-datacenter support
- [ ] Automated retraining pipeline
- [ ] Action recommendation system
- [ ] Integration with alerting platforms (PagerDuty, Slack, Teams)
- [ ] Explainable AI features (SHAP values, attention visualization)

See [FUTURE_ROADMAP.md](Docs/FUTURE_ROADMAP.md) for planned features.

---

## 📝 License

MIT License - See [LICENSE](LICENSE) file for details

---

## 🙏 Acknowledgments

**Built with:**
- [PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/) - TFT implementation
- [Streamlit](https://streamlit.io/) - Web dashboard framework
- [PyTorch](https://pytorch.org/) - Deep learning framework
- [Pandas](https://pandas.pydata.org/) - Data manipulation
- [Plotly](https://plotly.com/) - Interactive visualizations

**Special Thanks:**
- **Claude Code** - AI-assisted development that made this possible in 67.5 hours
- Temporal Fusion Transformer paper: [arxiv.org/abs/1912.09363](https://arxiv.org/abs/1912.09363)

---

## 📞 Contact

**Questions? Issues? Feedback?**

- Open an issue on GitHub
- Check the [Docs/](Docs/) directory for detailed guides
- Review [ESSENTIAL_RAG.md](Docs/ESSENTIAL_RAG.md) for troubleshooting

---

## 🎤 The Story

This system was built in **67.5 hours** using AI-assisted development with Claude Code. What would have taken months of traditional development was accomplished in days through intelligent collaboration between human domain expertise and AI coding capabilities.

**Key Stats:**
- ⏱️ **67.5 hours** total development time
- 📊 **88% accuracy** on critical incident prediction
- 🚀 **8-hour advance warning** before failures
- 💰 **One prevented outage** pays for the entire system
- 🎯 **Production-ready** from day 1

**Read the full story:**
- [PRESENTATION_MASTER.md](PRESENTATION_MASTER.md) - Complete presentation script
- [TIME_TRACKING.md](Docs/TIME_TRACKING.md) - Detailed development timeline
- [THE_PROPHECY.md](Docs/THE_PROPHECY.md) - The vision and philosophy

---

**Built with 🧠 AI + ☕ Coffee + ⚡ Vibe Coding**

*"Use AI or get replaced by someone who will."* 🎯

---

**Ready to predict the future?** Start with the [Quick Start](#-quick-start) above! 🚀
